{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "from tqdm import trange\n",
    "\n",
    "# maze size\n",
    "N = 10\n",
    "\n",
    "# thetas deformations (range(1,M),range(1,M))\n",
    "M = 5\n",
    "\n",
    "# belief update iterations\n",
    "iters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_maze(dim):\n",
    "    maze = np.ones((dim*2+1, dim*2+1))\n",
    "    x, y = (0, 0)\n",
    "    maze[2*x+1, 2*y+1] = 0\n",
    "    stack = [(x, y)]\n",
    "    \n",
    "    while len(stack) > 0:\n",
    "        x, y = stack[-1]\n",
    "        directions = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
    "        random.shuffle(directions)\n",
    "\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if nx >= 0 and ny >= 0 and nx < dim and ny < dim and maze[2*nx+1, 2*ny+1] == 1:\n",
    "                maze[2*nx+1, 2*ny+1] = 0\n",
    "                maze[2*x+1+dx, 2*y+1+dy] = 0\n",
    "                stack.append((nx, ny))\n",
    "                break\n",
    "        else:\n",
    "            stack.pop()\n",
    "\n",
    "    # Create entrance and exit\n",
    "    # maze[1, 0] = 0\n",
    "    # maze[-2, -1] = 0\n",
    "\n",
    "    return maze\n",
    "\n",
    "def stretch_maze(maze, scale_x, scale_y):\n",
    "    original_height, original_width = maze.shape\n",
    "    # Calculate new dimensions\n",
    "    new_height = original_height * scale_y\n",
    "    new_width = original_width * scale_x\n",
    "    \n",
    "    # Create a new maze with stretched dimensions\n",
    "    stretched_maze = np.ones((new_height, new_width), dtype=int)\n",
    "\n",
    "    # Fill the new maze with values from the original maze\n",
    "    for i in range(original_height):\n",
    "        for j in range(original_width):\n",
    "            if maze[i, j] == 0:  # Path cell\n",
    "                # Fill the corresponding region in the stretched maze\n",
    "                stretched_maze[i*scale_y:(i+1)*scale_y, j*scale_x:(j+1)*scale_x] = 0\n",
    "\n",
    "    return stretched_maze\n",
    "\n",
    "def obs_fun(agent_observation,agent_orientation, theta,M):\n",
    "    \n",
    "    \"\"\"\"given the agent observation and the \\theta state, return the probability of observing o in f_\\theta(M)\"\"\"\n",
    "\n",
    "    num_obs = 0\n",
    "    num_ones = 0\n",
    "    belief_state = stretch_maze(M,theta[0],theta[1])    \n",
    "\n",
    "    for i in range(belief_state.shape[0]):\n",
    "        for j in range(belief_state.shape[1]):\n",
    "            if belief_state[i,j] == 1:\n",
    "                num_ones = num_ones + 1\n",
    "                continue\n",
    "\n",
    "            ind = [(i,j) + a for a in [np.array([0,-1]),\n",
    "                            np.array([-1,-1]),\n",
    "                            np.array([-1,0]),\n",
    "                            np.array([-1,+1]),\n",
    "                            np.array([0,+1]),\n",
    "                            np.array([+1,+1]),\n",
    "                            np.array([+1,0]),\n",
    "                            np.array([+1,-1])]]\n",
    "            \n",
    "            obsij = [belief_state[tuple(ind[i%8])] \n",
    "                                for i in range(2*agent_orientation, 2*agent_orientation+5)]\n",
    "            \n",
    "            if np.all(agent_observation == obsij):\n",
    "                num_obs = num_obs + 1\n",
    "    \n",
    "    return np.float64(num_obs/(belief_state.shape[0]*belief_state.shape[1]-num_ones))\n",
    "\n",
    "\n",
    "class MazeEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, maze, max_steps=100):\n",
    "        super(MazeEnv, self).__init__()\n",
    "        \n",
    "        self.original_maze = maze\n",
    "        self.original_maze_shape = maze.shape\n",
    "\n",
    "        self.t0 = np.random.randint(1, M)\n",
    "        self.t1 = np.random.randint(1, M)\n",
    "        \n",
    "        self.maze = self.stretch_maze(self.original_maze,self.t0,self.t1)\n",
    "        self.maze_shape = maze.shape\n",
    "\n",
    "        self.max_steps = max_steps\n",
    "                \n",
    "        self.action_space = spaces.Discrete(4)  # Four possible actions: up, down, left, right\n",
    "\n",
    "        self.observation_space = spaces.Dict(\n",
    "            dict(\n",
    "                full_state=spaces.Box(low=0, high=1, shape=self.maze.shape, dtype=np.int32),\n",
    "                agent_observations=spaces.Box(low=0, high=1, shape=(5,), dtype=np.int32),\n",
    "                agent_orientation=spaces.Box(low=0,high=3,shape=(1,),dtype=np.int32)\n",
    "            )\n",
    "            )\n",
    "        \n",
    "        # self.vt = {'0': (-1,0), '1': (0, +1), '2': (+1, 0), '3': (0.-1)}\n",
    "        self.vt = {'0': np.array([-1,0]),\n",
    "                   '1': np.array([0, +1]), \n",
    "                   '2': np.array([+1, 0]), \n",
    "                   '3': np.array([0.-1])}\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    \n",
    "    def reset(self, seed=42):\n",
    "\n",
    "        # set seed\n",
    "        # TODO\n",
    "        self.t0 = np.random.randint(1, M)\n",
    "        self.t1 = np.random.randint(1, M)\n",
    "        \n",
    "        self.maze = self.stretch_maze(self.original_maze,self.t0,self.t1)\n",
    "        self.maze_shape = maze.shape\n",
    "\n",
    "        self.agent_pos = self._set_random_pos()\n",
    "        self.agent_orientation = self._set_random_orientation()\n",
    "        self.goal_pos = self._set_random_pos()\n",
    "        self.num_steps = 0\n",
    "\n",
    "\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "\n",
    "        state = {}\n",
    "        full_state = np.copy(self.maze)\n",
    "        full_state[tuple(self.agent_pos)] = 2  # Mark agent's current position with a unique value (e.g., 2)\n",
    "        full_state[tuple(self.goal_pos)] = 3  # Mark goal position with a unique value (e.g., 3)\n",
    "        state['full_state'] = full_state\n",
    "\n",
    "        ind = [self.agent_pos + a for a in [np.array([0,-1]),\n",
    "                                            np.array([-1,-1]),\n",
    "                                            np.array([-1,0]),\n",
    "                                            np.array([-1,+1]),\n",
    "                                            np.array([0,+1]),\n",
    "                                            np.array([+1,+1]),\n",
    "                                            np.array([+1,0]),\n",
    "                                            np.array([+1,-1])]]\n",
    "\n",
    "        state['agent_observations'] = np.array([self.maze[tuple(ind[i%8])] \n",
    "                                                for i in range(2*self.agent_orientation, 2*self.agent_orientation+5)])\n",
    "        state['agent_orientation'] = self.agent_orientation\n",
    "        \n",
    "        return state\n",
    "        \n",
    "    def _set_random_pos(self):\n",
    "        \n",
    "        pos = [np.random.randint(0, self.maze_shape[0]-1), np.random.randint(0,self.maze_shape[1]-1)]\n",
    "        while self.maze[pos[0]][pos[1]] != 0:\n",
    "            pos = [np.random.randint(0, self.maze_shape[0]-1), np.random.randint(0,self.maze_shape[1]-1)]\n",
    "\n",
    "        return pos\n",
    "    \n",
    "    def _set_random_orientation(self):\n",
    "        return np.random.randint(0, 3)\n",
    "    \n",
    "    def step(self, action):\n",
    "        x, y = self.agent_pos\n",
    "        \n",
    "        actual_action = (action + self.agent_orientation) % 4\n",
    "\n",
    "        # new_pos_1 = self.agent_pos + self.vt[str(actual_action)]\n",
    "        # print(new_pos_1)\n",
    "        \n",
    "        if actual_action == 0:  # Move up\n",
    "            new_pos = [x - 1, y]\n",
    "        elif actual_action == 2:  # Move down\n",
    "            new_pos = [x + 1, y]\n",
    "        elif actual_action == 3:  # Move left\n",
    "            new_pos = [x, y - 1]\n",
    "        elif actual_action == 1:  # Move right\n",
    "            new_pos = [x, y + 1]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Action\")\n",
    "        \n",
    "        # Check if the new position is valid (inside the maze and not a wall)\n",
    "        if 0 <= new_pos[0] < len(self.maze) and 0 <= new_pos[1] < len(self.maze[0]) and self.maze[tuple(new_pos)] == 0:\n",
    "            self.agent_pos = new_pos\n",
    "\n",
    "        self.agent_orientation = (self.agent_orientation + action) % 4\n",
    "        \n",
    "        terminated = self.agent_pos == self.goal_pos\n",
    "\n",
    "        if terminated:\n",
    "            reward = 1\n",
    "        elif new_pos == self.agent_pos: \n",
    "            reward = -0.1/(len(self.maze[0])*len(self.maze[1]))  # Small penalty for each step, reward for reaching the goal\n",
    "        else: # if new_pos != self.agent_pos then the agent has NOT moved and hit wall\n",
    "            reward = -0.2/(len(self.maze[0])*len(self.maze[1]))\n",
    "\n",
    "\n",
    "        info = {}\n",
    "        truncated = True if self.num_steps > self.max_steps else False \n",
    "        \n",
    "        return self._get_observation(), reward, terminated, truncated, info\n",
    "    \n",
    "    def stretch_maze(self, maze, scale_x, scale_y):\n",
    "        original_height, original_width = maze.shape\n",
    "        # Calculate new dimensions\n",
    "        new_height = original_height * scale_y\n",
    "        new_width = original_width * scale_x\n",
    "        \n",
    "        # Create a new maze with stretched dimensions\n",
    "        stretched_maze = np.ones((new_height, new_width), dtype=int)\n",
    "\n",
    "        # Fill the new maze with values from the original maze\n",
    "        for i in range(original_height):\n",
    "            for j in range(original_width):\n",
    "                if maze[i, j] == 0:  # Path cell\n",
    "                    # Fill the corresponding region in the stretched maze\n",
    "                    stretched_maze[i*scale_y:(i+1)*scale_y, j*scale_x:(j+1)*scale_x] = 0\n",
    "\n",
    "        return stretched_maze\n",
    "\n",
    "    def render(self):\n",
    "        maze_render = np.copy(self.maze)\n",
    "        maze_render[tuple(self.agent_pos)] = 2  # Show agent position\n",
    "        maze_render[tuple(self.goal_pos)] = 4  # Show goal position\n",
    "        plt.imshow(maze_render, cmap='binary', origin='upper')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x stretch: 1 y stretch 3\n"
     ]
    }
   ],
   "source": [
    "maze = create_maze(N)\n",
    "env = MazeEnv(maze, max_steps=1000)\n",
    "env.render()\n",
    "\n",
    "state, reward, terminated, truncated, info = env.step(0)\n",
    "print(\"x stretch:\", env.t0,\"y stretch\",env.t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Space - thetas:  [(1, 1), (1, 2), (1, 3), (1, 4), (2, 1), (2, 2), (2, 3), (2, 4), (3, 1), (3, 2), (3, 3), (3, 4), (4, 1), (4, 2), (4, 3), (4, 4)]\n",
      "observations:  [(0, 0, 0, 0, 0), (0, 0, 0, 0, 1), (0, 0, 0, 1, 0), (0, 0, 0, 1, 1), (0, 0, 1, 0, 0), (0, 0, 1, 0, 1), (0, 0, 1, 1, 0), (0, 0, 1, 1, 1), (0, 1, 0, 0, 0), (0, 1, 0, 0, 1), (0, 1, 0, 1, 0), (0, 1, 0, 1, 1), (0, 1, 1, 0, 0), (0, 1, 1, 0, 1), (0, 1, 1, 1, 0), (0, 1, 1, 1, 1), (1, 0, 0, 0, 0), (1, 0, 0, 0, 1), (1, 0, 0, 1, 0), (1, 0, 0, 1, 1), (1, 0, 1, 0, 0), (1, 0, 1, 0, 1), (1, 0, 1, 1, 0), (1, 0, 1, 1, 1), (1, 1, 0, 0, 0), (1, 1, 0, 0, 1), (1, 1, 0, 1, 0), (1, 1, 0, 1, 1), (1, 1, 1, 0, 0), (1, 1, 1, 0, 1), (1, 1, 1, 1, 0), (1, 1, 1, 1, 1)]\n",
      "orientations:  [0, 1, 2, 3]\n",
      "belief b_0:  {(1, 1): np.float64(0.0625), (1, 2): np.float64(0.0625), (1, 3): np.float64(0.0625), (1, 4): np.float64(0.0625), (2, 1): np.float64(0.0625), (2, 2): np.float64(0.0625), (2, 3): np.float64(0.0625), (2, 4): np.float64(0.0625), (3, 1): np.float64(0.0625), (3, 2): np.float64(0.0625), (3, 3): np.float64(0.0625), (3, 4): np.float64(0.0625), (4, 1): np.float64(0.0625), (4, 2): np.float64(0.0625), (4, 3): np.float64(0.0625), (4, 4): np.float64(0.0625)}\n"
     ]
    }
   ],
   "source": [
    "# State Space\n",
    "thetas = [(i,j) for i in range(1,M) for j in range(1,M)]\n",
    "\n",
    "# Belief Space b:space -> probability\n",
    "belief = np.ones(len(thetas))/len(thetas)\n",
    "b = {theta: belief[i] for i, theta in enumerate(thetas)}\n",
    "\n",
    "# Obaservation Space (all possible 5-tuples of 0s and 1s)\n",
    "ob = itertools.product([0,1], repeat=5)\n",
    "observations = []\n",
    "for obs in ob:\n",
    "    observations.append(obs)\n",
    "\n",
    "orientations = [0,1,2,3]\n",
    "\n",
    "print(\"State Space - thetas: \", thetas)\n",
    "print(\"observations: \", observations)\n",
    "print(\"orientations: \", orientations)\n",
    "print(\"belief b_0: \", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:56<00:00,  1.77it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in trange(iters):\n",
    "\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Value - Policy stuff\n",
    "    # TODO\n",
    "\n",
    "\n",
    "    # Belief update\n",
    "    pr_o_s = {theta: obs_fun(observation['agent_observations'],observation['agent_orientation'],theta,env.original_maze) for theta in thetas}\n",
    "    \n",
    "    eta = np.sum([b[theta] * pr_o_s[theta] for theta in thetas])\n",
    "\n",
    "    b_o_a = {theta: pr_o_s[theta]*b[theta]/eta for theta in thetas}\n",
    "    b = b_o_a\n",
    "    \n",
    "    assert (np.sum(list(b.values()))-1) < 0.01\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        belief = np.ones(len(thetas))/len(thetas)\n",
    "        b = {theta: belief[i] for i, theta in enumerate(thetas)}\n",
    "\n",
    "        print(\"NEWMAZE\")\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 1): np.float64(0.0),\n",
       " (1, 2): np.float64(0.0),\n",
       " (1, 3): np.float64(0.999682650567072),\n",
       " (1, 4): np.float64(0.00031734943292803877),\n",
       " (2, 1): np.float64(0.0),\n",
       " (2, 2): np.float64(0.0),\n",
       " (2, 3): np.float64(0.0),\n",
       " (2, 4): np.float64(0.0),\n",
       " (3, 1): np.float64(0.0),\n",
       " (3, 2): np.float64(0.0),\n",
       " (3, 3): np.float64(0.0),\n",
       " (3, 4): np.float64(0.0),\n",
       " (4, 1): np.float64(0.0),\n",
       " (4, 2): np.float64(0.0),\n",
       " (4, 3): np.float64(0.0),\n",
       " (4, 4): np.float64(0.0)}"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.t0, env.t1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
