{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN \n",
    " \n",
    "the idea is to initialize the network of DQNsb3_POMDP with the weights of the network of DQNsb3.\n",
    "\n",
    "the only problem is that the network of DQNsb3_POMDP has addtional inputs\n",
    "\n",
    "DQNMDP input = [pos,theta] \\\n",
    "DQNMDP input = [pos,belief]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maze size\n",
    "N = 2\n",
    "\n",
    "# thetas deformations (range(a,b),range(c,d))\n",
    "l0 = 1\n",
    "h0 = 10\n",
    "l1 = 1\n",
    "h1 = 10\n",
    "\n",
    "maze = np.load(f\"maze/maze_{N}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flaccagora/.miniconda3/envs/rob/lib/python3.9/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code() takes at most 16 arguments (18 given)\n",
      "  warnings.warn(\n",
      "/home/flaccagora/.miniconda3/envs/rob/lib/python3.9/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object exploration_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code() takes at most 16 arguments (18 given)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from environment.env import MDPGYMGridEnvDeform\n",
    "\n",
    "MDP_env = MDPGYMGridEnvDeform(maze,l0,h0,l1,h1)\n",
    "MDP_model = DQN.load(\"agents/pretrained/MDP/DQNsb3_z427ps8r.zip\", env=MDP_env, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP model performance: -9.4 +/- 5.85576638878294\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(MDP_model, MDP_env, n_eval_episodes=10)\n",
    "print(f\"MDP model performance: {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "credo di poter passare al modello la policy in forma di NN direttamnete anziché stringa \"Multiinputpolicy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.env import POMDPGYMGridEnvDeform\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "POMDP_env = POMDPGYMGridEnvDeform(maze,l0,h0,l1,h1, render_mode=\"rgb_array\")\n",
    "\n",
    "POMDP_model = DQN(\"MultiInputPolicy\",POMDP_env,device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POMDP model performance: -185.0 +/- 45.0\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "mean_reward, std_reward = evaluate_policy(POMDP_model, POMDP_env, n_eval_episodes=10)\n",
    "print(f\"POMDP model performance: {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the network to get $$POMDP\\_model(pos,belief) = MDP\\_model(pos,argmax(belief))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import copy\n",
    "\n",
    "from typing import Dict, Tuple, Union\n",
    "\n",
    "from stable_baselines3.common.preprocessing import is_image_space, maybe_transpose\n",
    "from stable_baselines3.common.type_aliases import PyTorchObs\n",
    "from stable_baselines3.common.utils import is_vectorized_observation, obs_as_tensor\n",
    "from gymnasium import spaces\n",
    "\n",
    "def obs_to_tensor(env, observation: Union[np.ndarray, Dict[str, np.ndarray]]) -> Tuple[PyTorchObs, bool]:\n",
    "    \"\"\"\n",
    "    Convert an input observation to a PyTorch tensor that can be fed to a model.\n",
    "    Includes sugar-coating to handle different observations (e.g. normalizing images).\n",
    "\n",
    "    :param observation: the input observation\n",
    "    :return: The observation as PyTorch tensor\n",
    "        and whether the observation is vectorized or not\n",
    "    \"\"\"\n",
    "    vectorized_env = False\n",
    "    if isinstance(observation, dict):\n",
    "        assert isinstance(\n",
    "            env.observation_space, spaces.Dict\n",
    "        ), f\"The observation provided is a dict but the obs space is {env.observation_space}\"\n",
    "        # need to copy the dict as the dict in VecFrameStack will become a torch tensor\n",
    "        observation = copy.deepcopy(observation)\n",
    "        for key, obs in observation.items():\n",
    "            obs_space = env.observation_space.spaces[key]\n",
    "            if is_image_space(obs_space):\n",
    "                obs_ = maybe_transpose(obs, obs_space)\n",
    "            else:\n",
    "                obs_ = np.array(obs)\n",
    "            vectorized_env = vectorized_env or is_vectorized_observation(obs_, obs_space)\n",
    "            # Add batch dimension if needed\n",
    "            observation[key] = obs_.reshape((-1, *env.observation_space[key].shape))  # type: ignore[misc]\n",
    "\n",
    "    elif is_image_space(env.observation_space):\n",
    "        # Handle the different cases for images\n",
    "        # as PyTorch use channel first format\n",
    "        observation = maybe_transpose(observation, env.observation_space)\n",
    "\n",
    "    else:\n",
    "        observation = np.array(observation)\n",
    "\n",
    "    if not isinstance(observation, dict):\n",
    "        # Dict obs need to be handled separately\n",
    "        vectorized_env = is_vectorized_observation(observation, env.observation_space)\n",
    "        # Add batch dimension if needed\n",
    "        observation = observation.reshape((-1, *env.observation_space.shape))  # type: ignore[misc]\n",
    "\n",
    "    obs_tensor = obs_as_tensor(observation, 'cpu')\n",
    "    return obs_tensor, vectorized_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 4922/10001 [11:15<11:37,  7.29it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m a \u001b[38;5;241m=\u001b[39m MDP_model\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mq_net(obs_to_tensor(MDP_env,mdp_s)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     19\u001b[0m data\u001b[38;5;241m.\u001b[39mappend((obs_to_tensor(POMDP_env,s)[\u001b[38;5;241m0\u001b[39m], a))\n\u001b[0;32m---> 21\u001b[0m s, _, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43mPOMDP_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/RoboSurgery/src/environment/env.py:536\u001b[0m, in \u001b[0;36mPOMDPGYMGridEnvDeform.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m--> 536\u001b[0m new_beleif \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_belief\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbelief \u001b[38;5;241m=\u001b[39m new_beleif\n\u001b[1;32m    540\u001b[0m obs \u001b[38;5;241m=\u001b[39m OrderedDict({\n\u001b[1;32m    541\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([x_],dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32),              \u001b[38;5;66;03m# Values from 0 to 10\u001b[39;00m\n\u001b[1;32m    542\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([y_],dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32),              \u001b[38;5;66;03m# Values from 0 to 10\u001b[39;00m\n\u001b[1;32m    543\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphi\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([phi_],dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32),             \u001b[38;5;66;03m# Values from 0 to 4\u001b[39;00m\n\u001b[1;32m    544\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbelief\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbelief , \u001b[38;5;66;03m# Probability vector\u001b[39;00m\n\u001b[1;32m    545\u001b[0m                 })\n",
      "File \u001b[0;32m~/Desktop/RoboSurgery/src/environment/env.py:644\u001b[0m, in \u001b[0;36mPOMDPGYMGridEnvDeform.update_belief\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    641\u001b[0m         new_belief[t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m     P_o_s_theta \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 0 or 1 \u001b[39;00m\n\u001b[1;32m    645\u001b[0m     new_belief[t] \u001b[38;5;241m=\u001b[39m P_o_s_theta \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbelief[t]\n\u001b[1;32m    647\u001b[0m new_belief \u001b[38;5;241m=\u001b[39m new_belief \u001b[38;5;241m/\u001b[39m (torch\u001b[38;5;241m.\u001b[39msum(new_belief) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-10\u001b[39m)\n",
      "File \u001b[0;32m~/.miniconda3/envs/rob/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:2601\u001b[0m, in \u001b[0;36mall\u001b[0;34m(a, axis, out, keepdims, where)\u001b[0m\n\u001b[1;32m   2513\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_all_dispatcher)\n\u001b[1;32m   2514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2516\u001b[0m \u001b[38;5;124;03m    Test whether all array elements along a given axis evaluate to True.\u001b[39;00m\n\u001b[1;32m   2517\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2599\u001b[0m \n\u001b[1;32m   2600\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2601\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction_any_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogical_and\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2602\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/envs/rob/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:102\u001b[0m, in \u001b[0;36m_wrapreduction_any_all\u001b[0;34m(obj, ufunc, method, axis, out, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import torch\n",
    "from tqdm import trange\n",
    "\n",
    "data = []\n",
    "\n",
    "for episode in trange(num_episodes):\n",
    "    s, _ = POMDP_env.reset()\n",
    "   \n",
    "    while True:\n",
    "        mdp_s = OrderedDict([('x', s['x']),\n",
    "                    ('y', s['y']),\n",
    "                    ('phi', s['phi']),\n",
    "                    ('theta', torch.tensor(MDP_env.deformations[torch.argmax(s['belief'])])),\n",
    "                    ])\n",
    "\n",
    "        a = MDP_model.policy.q_net(obs_to_tensor(MDP_env,mdp_s)[0])\n",
    "\n",
    "        data.append((obs_to_tensor(POMDP_env,s)[0], a))\n",
    "\n",
    "        s, _, terminated, truncated, info = POMDP_env.step(torch.argmax(a))\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    # save data for training\n",
    "    if  episode == 100 or episode == 1000 or episode == 10000:\n",
    "        torch.save(data, f\"POMDP_data_{num_episodes}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(f\"POMDP_data_{10001}.pt\", weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24550"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (list of tuples): The dataset as a list of (x, y) tuples.\n",
    "                                   x is a dictionary, and y is an array.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the sample at index `idx`\n",
    "        x, y = self.data[idx]\n",
    "\n",
    "        # Convert x (dictionary) to a tensor or process it as needed\n",
    "        # Assuming all dictionary values are numerical and can be converted to tensors\n",
    "        x_tensor = {key: value.clone().detach() for key, value in x.items()}\n",
    "\n",
    "        # Convert y to a tensor\n",
    "        y_tensor = y.clone().detach().squeeze()\n",
    "\n",
    "        return x_tensor, y_tensor\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle dictionaries in x.\n",
    "    Args:\n",
    "        batch: List of (x, y) tuples where x is a dictionary and y is a tensor.\n",
    "    Returns:\n",
    "        A dictionary of batched x values and a batched tensor for y.\n",
    "    \"\"\"\n",
    "    # Separate the x and y values\n",
    "    x_batch = [item[0] for item in batch]\n",
    "    y_batch = [item[1] for item in batch]\n",
    "\n",
    "    # Combine the dictionary values into batched tensors\n",
    "    batched_x = {key: torch.stack([x[key] for x in x_batch]) for key in x_batch[0]}\n",
    "    batched_y = torch.stack(y_batch)\n",
    "\n",
    "    return batched_x, batched_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CustomDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:32<00:00, 15.22s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(POMDP_model.policy.q_net.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in trange(10):\n",
    "    for i, (batch_x, batch_y) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = POMDP_model.policy.q_net(batch_x)\n",
    "        l = loss(output, batch_y)\n",
    "        l.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POMDP model performance: -170.0 +/- 60.0\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(POMDP_model, POMDP_env, n_eval_episodes=10)\n",
    "print(f\"POMDP model performance: {mean_reward} +/- {std_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
