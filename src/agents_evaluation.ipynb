{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation purpose use eval_agent.py script. \\\n",
    "It will run your agent on the test environment **GridEnvDeform**.\n",
    "\n",
    "wrap any model and make sure the following methods are implemented:\\\n",
    "for **POMDP** agent\n",
    "- `get_action(belief,pos)` : returns the action to take given the current state\n",
    "- `update_belief(self, belief, pos, observation)` : updates the belief given the current state and action\n",
    "\n",
    "for **MDP** agent\n",
    "- `get_action(pos)` : returns the action to take given the current state\n",
    "\n",
    "**Note**: Evaluation is always performed on the **GriEnvDeform** environment. s, _ = env.reset() gives back s = ((x,y,phi),theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models recap:\n",
    "\n",
    "**MDP**\n",
    "- TabularQ --------------- **OK**\n",
    "- DQN -------------------- **OK**\n",
    "- DQNsb3 \n",
    "- PPO\n",
    "\n",
    "**POMDP**\n",
    "- TabularQ --------------- \n",
    "- DQN -------------------- \n",
    "- DQNsb3 \n",
    "- PPO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quindi faccio partire mydqn pomdp e sb3dqn-mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from agents.DQN_agent import DoubleDQNAgent\n",
    "from environment.env import GridEnvDeform\n",
    "from eval.eval import eval_agent, all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236196"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maze size\n",
    "N = 2\n",
    "\n",
    "# thetas deformations (range(a,b),range(c,d))\n",
    "l0 = 1\n",
    "h0 = 10\n",
    "l1 = 1\n",
    "h1 = 10\n",
    "\n",
    "maze = np.load(f\"maze/maze_{N}.npy\")\n",
    "env = GridEnvDeform(maze,l0,h0,l1,h1)\n",
    "\n",
    "states = [((x,y,phi),(i,j)) for x in range(1,env.max_shape[0]-1) for y in range(1,env.max_shape[1]-1) for phi in range(4) for i in range(l0,h0) for j in range(l1,h1)] \n",
    "positions = [(x,y,phi) for x in range(1,env.max_shape[0]-1) for y in range(1,env.max_shape[1]-1) for phi in range(4)]\n",
    "actions = [0,1,2,3]\n",
    "obs = list(itertools.product([0,1], repeat=5))\n",
    "thetas = [(i,j) for i in range(l0,h0) for j in range(l1,h1)]\n",
    "\n",
    "state_dict = {state: i for i, state in enumerate(states)}\n",
    "position_dict = {position: i for i, position in enumerate(positions)}\n",
    "obs_dict = {obs : i for i, obs in enumerate(obs)}\n",
    "\n",
    "# Actions are: 0-listen, 1-open-left, 2-open-right\n",
    "lenS = len(states)\n",
    "lenP = len(positions)\n",
    "lenA = len(actions)\n",
    "lenO = len(obs)\n",
    "\n",
    "lenS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Directed Sampling IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agents.Infotaxis_agent import IDS, ThetaInfotaxis\n",
    "\n",
    "agent = IDS(env)\n",
    "#agent = ThetaInfotaxis(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:13<00:00, 50.75s/it]\n"
     ]
    }
   ],
   "source": [
    "transitions, beliefs = eval_agent(\"POMDP\",agent,env, num_episodes=5,max_episode_steps=30, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed episodes: 0, out of 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_data(transitions, beliefs, path=\"plots/IDS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(replay_episode):\n\u001b[1;32m     13\u001b[0m     env\u001b[38;5;241m.\u001b[39mset_state(s)\n\u001b[0;32m---> 14\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_bis\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     17\u001b[0m env\u001b[38;5;241m.\u001b[39mclose_render()\n",
      "File \u001b[0;32m~/Desktop/RoboSurgery/src/environment/env.py:335\u001b[0m, in \u001b[0;36mGridEnvDeform.render_bis\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_state(s)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# Clear the screen\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# Draw the maze\u001b[39;00m\n\u001b[1;32m    338\u001b[0m cell_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen_height) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_shape)\n",
      "\u001b[0;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# transitions[i] = [(s,a,s',o)....()...] for i-th episode\n",
    "\n",
    "# replay buffer for episode i = succession of states\n",
    "episode = 4\n",
    "replay_episode = [t[0] for t in transitions[episode]] \n",
    "\n",
    "\n",
    "env.set_rendering()\n",
    "\n",
    "for i, s in enumerate(replay_episode):\n",
    "    env.set_state(s)\n",
    "    env.render_bis()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "env.close_render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP based Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QMDP tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the MDP solution model (tabular q in this case)\n",
    "Q = np.load(f\"agents/pretrained/MDP/tabularQ_maze_2_100k.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: ((4, 8, 3), (5, 7)), Action: 2\n",
      "State: ((4, 9, 1), (5, 7)), Action: 0\n",
      "State: ((4, 10, 1), (5, 7)), Action: 0\n",
      "State: ((4, 11, 1), (5, 7)), Action: 1\n",
      "State: ((5, 11, 2), (5, 7)), Action: 3\n",
      "State: ((5, 12, 1), (5, 7)), Action: 0\n",
      "State: ((5, 13, 1), (5, 7)), Action: 0\n",
      "State: ((5, 14, 1), (5, 7)), Action: 1\n",
      "State: ((6, 14, 2), (5, 7)), Action: 3\n",
      "State: ((6, 15, 1), (5, 7)), Action: 1\n",
      "State: ((7, 15, 2), (5, 7)), Action: 0\n",
      "State: ((8, 15, 2), (5, 7)), Action: 0\n",
      "State: ((9, 15, 2), (5, 7)), Action: 0\n",
      "State: ((10, 15, 2), (5, 7)), Action: 0\n",
      "State: ((11, 15, 2), (5, 7)), Action: 0\n",
      "State: ((12, 15, 2), (5, 7)), Action: 0\n",
      "State: ((13, 15, 2), (5, 7)), Action: 0\n",
      "State: ((14, 15, 2), (5, 7)), Action: 0\n",
      "State: ((15, 15, 2), (5, 7)), Action: 0\n",
      "State: ((16, 15, 2), (5, 7)), Action: 0\n",
      "State: ((17, 15, 2), (5, 7)), Action: 0\n",
      "State: ((18, 15, 2), (5, 7)), Action: 0\n",
      "State: ((19, 15, 2), (5, 7)), Action: 0\n",
      "State: ((20, 15, 2), (5, 7)), Action: 0\n",
      "State: ((19, 22, 1), (7, 3)), Action: 1\n",
      "State: ((20, 22, 2), (7, 3)), Action: 2\n",
      "State: ((19, 22, 0), (7, 3)), Action: 3\n",
      "State: ((19, 21, 3), (7, 3)), Action: 1\n",
      "State: ((18, 21, 0), (7, 3)), Action: 0\n",
      "State: ((17, 21, 0), (7, 3)), Action: 0\n",
      "State: ((16, 21, 0), (7, 3)), Action: 0\n",
      "State: ((15, 21, 0), (7, 3)), Action: 0\n",
      "State: ((14, 21, 0), (7, 3)), Action: 0\n",
      "State: ((13, 21, 0), (7, 3)), Action: 0\n",
      "State: ((12, 21, 0), (7, 3)), Action: 0\n",
      "State: ((11, 21, 0), (7, 3)), Action: 0\n",
      "State: ((10, 21, 0), (7, 3)), Action: 0\n",
      "State: ((6, 13, 3), (4, 3)), Action: 0\n",
      "State: ((6, 12, 3), (4, 3)), Action: 3\n",
      "State: ((7, 12, 2), (4, 3)), Action: 0\n",
      "State: ((8, 12, 2), (4, 3)), Action: 0\n",
      "State: ((7, 12, 3), (7, 6)), Action: 0\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((7, 12, 1), (7, 6)), Action: 2\n",
      "State: ((7, 11, 3), (7, 6)), Action: 2\n",
      "State: ((20, 13, 1), (9, 6)), Action: 3\n",
      "State: ((19, 13, 0), (9, 6)), Action: 0\n",
      "State: ((18, 13, 0), (9, 6)), Action: 1\n",
      "State: ((18, 14, 1), (9, 6)), Action: 0\n",
      "State: ((18, 15, 1), (9, 6)), Action: 0\n",
      "State: ((18, 16, 1), (9, 6)), Action: 0\n",
      "State: ((18, 17, 1), (9, 6)), Action: 0\n",
      "State: ((18, 18, 1), (9, 6)), Action: 0\n",
      "State: ((18, 19, 1), (9, 6)), Action: 0\n",
      "State: ((18, 20, 1), (9, 6)), Action: 0\n",
      "State: ((18, 21, 1), (9, 6)), Action: 0\n",
      "State: ((18, 22, 1), (9, 6)), Action: 0\n",
      "State: ((18, 23, 1), (9, 6)), Action: 0\n",
      "State: ((18, 24, 1), (9, 6)), Action: 0\n",
      "State: ((18, 25, 1), (9, 6)), Action: 0\n",
      "State: ((18, 26, 1), (9, 6)), Action: 0\n",
      "State: ((25, 20, 2), (9, 1)), Action: 2\n",
      "State: ((24, 20, 0), (9, 1)), Action: 3\n",
      "State: ((24, 19, 3), (9, 1)), Action: 0\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((24, 19, 1), (9, 1)), Action: 2\n",
      "State: ((24, 18, 3), (9, 1)), Action: 2\n",
      "State: ((23, 4, 3), (7, 8)), Action: 2\n",
      "State: ((23, 5, 1), (7, 8)), Action: 0\n",
      "State: ((23, 6, 1), (7, 8)), Action: 0\n",
      "State: ((23, 7, 1), (7, 8)), Action: 0\n",
      "State: ((23, 8, 1), (7, 8)), Action: 1\n",
      "State: ((24, 8, 2), (7, 8)), Action: 3\n",
      "State: ((24, 9, 1), (7, 8)), Action: 0\n",
      "State: ((24, 10, 1), (7, 8)), Action: 0\n",
      "State: ((24, 11, 1), (7, 8)), Action: 0\n",
      "State: ((24, 12, 1), (7, 8)), Action: 0\n",
      "State: ((24, 13, 1), (7, 8)), Action: 0\n",
      "State: ((24, 14, 1), (7, 8)), Action: 0\n",
      "State: ((24, 15, 1), (7, 8)), Action: 0\n",
      "State: ((24, 16, 1), (7, 8)), Action: 0\n",
      "State: ((24, 17, 1), (7, 8)), Action: 0\n",
      "State: ((24, 18, 1), (7, 8)), Action: 0\n",
      "State: ((24, 19, 1), (7, 8)), Action: 0\n",
      "State: ((24, 20, 1), (7, 8)), Action: 0\n",
      "State: ((8, 14, 1), (3, 9)), Action: 1\n",
      "State: ((9, 14, 2), (3, 9)), Action: 3\n",
      "State: ((9, 15, 1), (3, 9)), Action: 3\n",
      "State: ((8, 15, 0), (3, 9)), Action: 3\n",
      "State: ((8, 14, 3), (3, 9)), Action: 0\n",
      "State: ((8, 13, 3), (3, 9)), Action: 0\n",
      "State: ((8, 12, 3), (3, 9)), Action: 0\n",
      "State: ((8, 11, 3), (3, 9)), Action: 0\n",
      "State: ((8, 10, 3), (3, 9)), Action: 0\n",
      "State: ((8, 9, 3), (3, 9)), Action: 3\n",
      "State: ((9, 9, 2), (3, 9)), Action: 0\n",
      "State: ((10, 9, 2), (3, 9)), Action: 0\n",
      "State: ((11, 9, 2), (3, 9)), Action: 0\n",
      "State: ((12, 9, 2), (3, 9)), Action: 0\n",
      "State: ((13, 9, 2), (3, 9)), Action: 0\n",
      "State: ((14, 9, 2), (3, 9)), Action: 0\n",
      "State: ((15, 9, 2), (3, 9)), Action: 0\n",
      "State: ((16, 9, 2), (3, 9)), Action: 0\n",
      "State: ((17, 9, 2), (3, 9)), Action: 0\n",
      "State: ((18, 9, 2), (3, 9)), Action: 0\n",
      "State: ((19, 9, 2), (3, 9)), Action: 0\n",
      "State: ((20, 9, 2), (3, 9)), Action: 0\n",
      "State: ((21, 9, 2), (3, 9)), Action: 0\n",
      "State: ((22, 9, 2), (3, 9)), Action: 0\n",
      "State: ((23, 9, 2), (3, 9)), Action: 0\n",
      "State: ((24, 9, 2), (3, 9)), Action: 0\n",
      "State: ((25, 9, 2), (3, 9)), Action: 0\n",
      "State: ((26, 9, 2), (3, 9)), Action: 0\n",
      "State: ((19, 20, 0), (1, 6)), Action: 1\n",
      "State: ((19, 21, 1), (1, 6)), Action: 1\n",
      "State: ((20, 21, 2), (1, 6)), Action: 1\n",
      "State: ((20, 20, 3), (1, 6)), Action: 2\n",
      "State: ((20, 21, 1), (1, 6)), Action: 3\n",
      "State: ((19, 21, 0), (1, 6)), Action: 2\n",
      "State: ((20, 21, 2), (1, 6)), Action: 1\n",
      "State: ((20, 20, 3), (1, 6)), Action: 2\n",
      "State: ((20, 21, 1), (1, 6)), Action: 3\n",
      "State: ((19, 21, 0), (1, 6)), Action: 2\n",
      "State: ((20, 21, 2), (1, 6)), Action: 1\n",
      "State: ((20, 20, 3), (1, 6)), Action: 2\n",
      "State: ((20, 21, 1), (1, 6)), Action: 3\n",
      "State: ((19, 21, 0), (1, 6)), Action: 2\n",
      "State: ((20, 21, 2), (1, 6)), Action: 1\n",
      "State: ((20, 20, 3), (1, 6)), Action: 2\n",
      "State: ((20, 21, 1), (1, 6)), Action: 3\n",
      "State: ((19, 21, 0), (1, 6)), Action: 2\n",
      "State: ((20, 21, 2), (1, 6)), Action: 1\n",
      "State: ((20, 20, 3), (1, 6)), Action: 2\n",
      "State: ((20, 21, 1), (1, 6)), Action: 3\n",
      "State: ((19, 21, 0), (1, 6)), Action: 2\n",
      "State: ((20, 21, 2), (1, 6)), Action: 1\n",
      "State: ((20, 20, 3), (1, 6)), Action: 2\n",
      "State: ((20, 21, 1), (1, 6)), Action: 3\n",
      "State: ((19, 21, 0), (1, 6)), Action: 2\n",
      "State: ((20, 21, 2), (1, 6)), Action: 1\n",
      "State: ((20, 20, 3), (1, 6)), Action: 2\n",
      "State: ((20, 21, 1), (1, 6)), Action: 3\n",
      "State: ((19, 21, 0), (1, 6)), Action: 2\n",
      "State: ((20, 21, 2), (1, 6)), Action: 1\n",
      "State: ((20, 20, 3), (1, 6)), Action: 2\n",
      "State: ((20, 21, 1), (1, 6)), Action: 3\n",
      "State: ((19, 21, 0), (1, 6)), Action: 2\n",
      "State: ((20, 21, 2), (1, 6)), Action: 1\n",
      "State: ((20, 20, 3), (1, 6)), Action: 2\n",
      "State: ((20, 21, 1), (1, 6)), Action: 3\n",
      "State: ((19, 21, 0), (1, 6)), Action: 2\n",
      "State: ((20, 21, 2), (1, 6)), Action: 1\n",
      "State: ((20, 20, 3), (1, 6)), Action: 2\n",
      "State: ((20, 21, 1), (1, 6)), Action: 3\n",
      "State: ((19, 21, 0), (1, 6)), Action: 2\n",
      "State: ((20, 21, 2), (1, 6)), Action: 1\n",
      "State: ((20, 20, 3), (1, 6)), Action: 2\n",
      "State: ((20, 21, 1), (1, 6)), Action: 3\n",
      "State: ((19, 21, 0), (1, 6)), Action: 2\n",
      "State: ((20, 21, 2), (1, 6)), Action: 1\n",
      "State: ((20, 20, 3), (1, 6)), Action: 2\n",
      "State: ((20, 21, 1), (1, 6)), Action: 3\n",
      "State: ((19, 21, 0), (1, 6)), Action: 2\n",
      "State: ((22, 2, 2), (8, 8)), Action: 2\n",
      "State: ((21, 2, 0), (8, 8)), Action: 1\n",
      "State: ((21, 3, 1), (8, 8)), Action: 3\n",
      "State: ((20, 3, 0), (8, 8)), Action: 1\n",
      "State: ((20, 4, 1), (8, 8)), Action: 2\n",
      "State: ((20, 3, 3), (8, 8)), Action: 3\n",
      "State: ((21, 3, 2), (8, 8)), Action: 3\n",
      "State: ((21, 4, 1), (8, 8)), Action: 1\n",
      "State: ((22, 4, 2), (8, 8)), Action: 2\n",
      "State: ((21, 4, 0), (8, 8)), Action: 1\n",
      "State: ((21, 5, 1), (8, 8)), Action: 0\n",
      "State: ((21, 6, 1), (8, 8)), Action: 2\n",
      "State: ((21, 5, 3), (8, 8)), Action: 2\n",
      "State: ((21, 6, 1), (8, 8)), Action: 2\n",
      "State: ((21, 5, 3), (8, 8)), Action: 2\n",
      "State: ((21, 6, 1), (8, 8)), Action: 2\n",
      "State: ((21, 5, 3), (8, 8)), Action: 2\n",
      "State: ((21, 6, 1), (8, 8)), Action: 2\n",
      "State: ((21, 5, 3), (8, 8)), Action: 2\n",
      "State: ((21, 6, 1), (8, 8)), Action: 2\n",
      "State: ((21, 5, 3), (8, 8)), Action: 2\n",
      "State: ((21, 6, 1), (8, 8)), Action: 2\n",
      "State: ((21, 5, 3), (8, 8)), Action: 2\n",
      "State: ((21, 6, 1), (8, 8)), Action: 2\n",
      "State: ((21, 5, 3), (8, 8)), Action: 2\n",
      "State: ((21, 6, 1), (8, 8)), Action: 2\n",
      "State: ((21, 5, 3), (8, 8)), Action: 2\n",
      "State: ((21, 6, 1), (8, 8)), Action: 2\n",
      "State: ((21, 5, 3), (8, 8)), Action: 2\n",
      "State: ((21, 6, 1), (8, 8)), Action: 2\n",
      "State: ((21, 5, 3), (8, 8)), Action: 2\n",
      "State: ((21, 6, 1), (8, 8)), Action: 2\n",
      "State: ((21, 5, 3), (8, 8)), Action: 2\n",
      "State: ((21, 6, 1), (8, 8)), Action: 2\n",
      "State: ((21, 5, 3), (8, 8)), Action: 2\n",
      "State: ((21, 6, 1), (8, 8)), Action: 2\n",
      "State: ((21, 5, 3), (8, 8)), Action: 2\n",
      "State: ((21, 6, 1), (8, 8)), Action: 2\n",
      "State: ((21, 5, 3), (8, 8)), Action: 2\n",
      "State: ((21, 6, 1), (8, 8)), Action: 2\n",
      "State: ((21, 5, 3), (8, 8)), Action: 2\n",
      "State: ((21, 6, 1), (8, 8)), Action: 2\n",
      "State: ((21, 5, 3), (8, 8)), Action: 2\n",
      "State: ((21, 6, 1), (8, 8)), Action: 2\n",
      "State: ((21, 5, 3), (8, 8)), Action: 2\n",
      "State: ((21, 6, 1), (8, 8)), Action: 2\n",
      "State: ((21, 5, 3), (8, 8)), Action: 2\n",
      "State: ((21, 6, 1), (8, 8)), Action: 2\n",
      "State: ((21, 5, 3), (8, 8)), Action: 2\n",
      "State: ((21, 6, 1), (8, 8)), Action: 2\n",
      "State: ((22, 25, 0), (7, 6)), Action: 1\n",
      "State: ((22, 26, 1), (7, 6)), Action: 1\n",
      "State: ((23, 26, 2), (7, 6)), Action: 3\n",
      "State: ((23, 27, 1), (7, 6)), Action: 3\n",
      "State: ((22, 27, 0), (7, 6)), Action: 3\n",
      "State: ((22, 26, 3), (7, 6)), Action: 3\n",
      "State: ((23, 26, 2), (7, 6)), Action: 3\n",
      "State: ((23, 27, 1), (7, 6)), Action: 3\n",
      "State: ((22, 27, 0), (7, 6)), Action: 3\n",
      "State: ((22, 26, 3), (7, 6)), Action: 3\n",
      "State: ((23, 26, 2), (7, 6)), Action: 3\n",
      "State: ((23, 27, 1), (7, 6)), Action: 3\n",
      "State: ((22, 27, 0), (7, 6)), Action: 3\n",
      "State: ((22, 26, 3), (7, 6)), Action: 3\n",
      "State: ((23, 26, 2), (7, 6)), Action: 3\n",
      "State: ((23, 27, 1), (7, 6)), Action: 3\n",
      "State: ((22, 27, 0), (7, 6)), Action: 3\n",
      "State: ((22, 26, 3), (7, 6)), Action: 3\n",
      "State: ((23, 26, 2), (7, 6)), Action: 3\n",
      "State: ((23, 27, 1), (7, 6)), Action: 3\n",
      "State: ((22, 27, 0), (7, 6)), Action: 3\n",
      "State: ((22, 26, 3), (7, 6)), Action: 3\n",
      "State: ((23, 26, 2), (7, 6)), Action: 3\n",
      "State: ((23, 27, 1), (7, 6)), Action: 3\n",
      "State: ((22, 27, 0), (7, 6)), Action: 3\n",
      "State: ((22, 26, 3), (7, 6)), Action: 3\n",
      "State: ((23, 26, 2), (7, 6)), Action: 3\n",
      "State: ((23, 27, 1), (7, 6)), Action: 3\n",
      "State: ((22, 27, 0), (7, 6)), Action: 3\n",
      "State: ((22, 26, 3), (7, 6)), Action: 3\n",
      "State: ((23, 26, 2), (7, 6)), Action: 3\n",
      "State: ((23, 27, 1), (7, 6)), Action: 3\n",
      "State: ((22, 27, 0), (7, 6)), Action: 3\n",
      "State: ((22, 26, 3), (7, 6)), Action: 3\n",
      "State: ((23, 26, 2), (7, 6)), Action: 3\n",
      "State: ((23, 27, 1), (7, 6)), Action: 3\n",
      "State: ((22, 27, 0), (7, 6)), Action: 3\n",
      "State: ((22, 26, 3), (7, 6)), Action: 3\n",
      "State: ((23, 26, 2), (7, 6)), Action: 3\n",
      "State: ((23, 27, 1), (7, 6)), Action: 3\n",
      "State: ((22, 27, 0), (7, 6)), Action: 3\n",
      "State: ((22, 26, 3), (7, 6)), Action: 3\n",
      "State: ((23, 26, 2), (7, 6)), Action: 3\n",
      "State: ((23, 27, 1), (7, 6)), Action: 3\n",
      "State: ((22, 27, 0), (7, 6)), Action: 3\n",
      "State: ((22, 26, 3), (7, 6)), Action: 3\n",
      "State: ((23, 26, 2), (7, 6)), Action: 3\n",
      "State: ((23, 27, 1), (7, 6)), Action: 3\n",
      "State: ((22, 27, 0), (7, 6)), Action: 3\n",
      "State: ((22, 26, 3), (7, 6)), Action: 3\n",
      "State: ((10, 10, 0), (6, 6)), Action: 1\n",
      "State: ((10, 11, 1), (6, 6)), Action: 0\n",
      "State: ((10, 12, 1), (6, 6)), Action: 0\n",
      "State: ((10, 13, 1), (6, 6)), Action: 0\n",
      "State: ((10, 14, 1), (6, 6)), Action: 0\n",
      "State: ((10, 15, 1), (6, 6)), Action: 1\n",
      "State: ((11, 15, 2), (6, 6)), Action: 0\n",
      "State: ((12, 15, 2), (6, 6)), Action: 3\n",
      "State: ((12, 16, 1), (6, 6)), Action: 0\n",
      "State: ((12, 17, 1), (6, 6)), Action: 1\n",
      "State: ((13, 17, 2), (6, 6)), Action: 0\n",
      "State: ((14, 17, 2), (6, 6)), Action: 0\n",
      "State: ((15, 17, 2), (6, 6)), Action: 0\n",
      "State: ((16, 17, 2), (6, 6)), Action: 0\n",
      "State: ((17, 17, 2), (6, 6)), Action: 0\n",
      "State: ((18, 17, 2), (6, 6)), Action: 3\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((2, 1, 0), (7, 9)), Action: 2\n",
      "State: ((3, 1, 2), (7, 9)), Action: 2\n",
      "State: ((27, 18, 3), (5, 1)), Action: 0\n",
      "State: ((27, 17, 3), (5, 1)), Action: 1\n",
      "State: ((26, 17, 0), (5, 1)), Action: 0\n",
      "State: ((25, 17, 0), (5, 1)), Action: 0\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((24, 17, 0), (5, 1)), Action: 2\n",
      "State: ((25, 17, 2), (5, 1)), Action: 2\n",
      "State: ((14, 25, 2), (7, 7)), Action: 1\n",
      "State: ((14, 24, 3), (7, 7)), Action: 0\n",
      "State: ((14, 23, 3), (7, 7)), Action: 0\n",
      "State: ((14, 22, 3), (7, 7)), Action: 0\n",
      "State: ((14, 21, 3), (7, 7)), Action: 3\n",
      "State: ((15, 21, 2), (7, 7)), Action: 0\n",
      "State: ((16, 21, 2), (7, 7)), Action: 0\n",
      "State: ((17, 21, 2), (7, 7)), Action: 0\n",
      "State: ((18, 21, 2), (7, 7)), Action: 0\n",
      "State: ((19, 21, 2), (7, 7)), Action: 0\n",
      "State: ((20, 21, 2), (7, 7)), Action: 0\n",
      "State: ((4, 17, 1), (5, 4)), Action: 2\n",
      "State: ((4, 16, 3), (5, 4)), Action: 0\n",
      "State: ((4, 15, 3), (5, 4)), Action: 3\n",
      "State: ((5, 15, 2), (5, 4)), Action: 0\n",
      "State: ((6, 15, 2), (5, 4)), Action: 0\n",
      "State: ((7, 15, 2), (5, 4)), Action: 0\n",
      "State: ((8, 15, 2), (5, 4)), Action: 0\n",
      "State: ((9, 15, 2), (5, 4)), Action: 0\n",
      "State: ((10, 15, 2), (5, 4)), Action: 0\n",
      "State: ((11, 15, 2), (5, 4)), Action: 0\n",
      "State: ((26, 27, 3), (8, 1)), Action: 0\n",
      "State: ((26, 26, 3), (8, 1)), Action: 3\n",
      "State: ((27, 26, 2), (8, 1)), Action: 2\n",
      "State: ((26, 26, 0), (8, 1)), Action: 0\n",
      "State: ((25, 26, 0), (8, 1)), Action: 3\n",
      "State: ((25, 25, 3), (8, 1)), Action: 0\n",
      "State: ((25, 24, 3), (8, 1)), Action: 1\n",
      "State: ((24, 24, 0), (8, 1)), Action: 0\n",
      "State: ((23, 24, 0), (8, 1)), Action: 3\n",
      "State: ((23, 23, 3), (8, 1)), Action: 2\n",
      "State: ((23, 24, 1), (8, 1)), Action: 2\n",
      "State: ((23, 23, 3), (8, 1)), Action: 2\n",
      "State: ((23, 24, 1), (8, 1)), Action: 2\n",
      "State: ((23, 23, 3), (8, 1)), Action: 2\n",
      "State: ((23, 24, 1), (8, 1)), Action: 2\n",
      "State: ((23, 23, 3), (8, 1)), Action: 2\n",
      "State: ((23, 24, 1), (8, 1)), Action: 2\n",
      "State: ((23, 23, 3), (8, 1)), Action: 2\n",
      "State: ((23, 24, 1), (8, 1)), Action: 2\n",
      "State: ((23, 23, 3), (8, 1)), Action: 2\n",
      "State: ((23, 24, 1), (8, 1)), Action: 2\n",
      "State: ((23, 23, 3), (8, 1)), Action: 2\n",
      "State: ((23, 24, 1), (8, 1)), Action: 2\n",
      "State: ((23, 23, 3), (8, 1)), Action: 2\n",
      "State: ((23, 24, 1), (8, 1)), Action: 2\n",
      "State: ((23, 23, 3), (8, 1)), Action: 2\n",
      "State: ((23, 24, 1), (8, 1)), Action: 2\n",
      "State: ((23, 23, 3), (8, 1)), Action: 2\n",
      "State: ((23, 24, 1), (8, 1)), Action: 2\n",
      "State: ((23, 23, 3), (8, 1)), Action: 2\n",
      "State: ((23, 24, 1), (8, 1)), Action: 2\n",
      "State: ((23, 23, 3), (8, 1)), Action: 2\n",
      "State: ((23, 24, 1), (8, 1)), Action: 2\n",
      "State: ((23, 23, 3), (8, 1)), Action: 2\n",
      "State: ((23, 24, 1), (8, 1)), Action: 2\n",
      "State: ((23, 23, 3), (8, 1)), Action: 2\n",
      "State: ((23, 24, 1), (8, 1)), Action: 2\n",
      "State: ((23, 23, 3), (8, 1)), Action: 2\n",
      "State: ((23, 24, 1), (8, 1)), Action: 2\n",
      "State: ((23, 23, 3), (8, 1)), Action: 2\n",
      "State: ((23, 24, 1), (8, 1)), Action: 2\n",
      "State: ((23, 23, 3), (8, 1)), Action: 2\n",
      "State: ((23, 24, 1), (8, 1)), Action: 2\n",
      "State: ((23, 23, 3), (8, 1)), Action: 2\n",
      "State: ((23, 24, 1), (8, 1)), Action: 2\n",
      "State: ((23, 23, 3), (8, 1)), Action: 2\n",
      "State: ((23, 24, 1), (8, 1)), Action: 2\n",
      "State: ((23, 23, 3), (8, 1)), Action: 2\n",
      "State: ((23, 24, 1), (8, 1)), Action: 2\n",
      "State: ((23, 23, 3), (8, 1)), Action: 2\n",
      "State: ((24, 22, 0), (5, 6)), Action: 2\n",
      "State: ((25, 22, 2), (5, 6)), Action: 1\n",
      "State: ((25, 21, 3), (5, 6)), Action: 2\n",
      "State: ((25, 22, 1), (5, 6)), Action: 1\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((26, 22, 2), (5, 6)), Action: 2\n",
      "State: ((25, 22, 0), (5, 6)), Action: 2\n",
      "State: ((14, 25, 2), (1, 7)), Action: 0\n",
      "State: ((15, 25, 2), (1, 7)), Action: 3\n",
      "State: ((15, 26, 1), (1, 7)), Action: 3\n",
      "State: ((14, 26, 0), (1, 7)), Action: 0\n",
      "State: ((13, 26, 0), (1, 7)), Action: 0\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((13, 26, 2), (1, 7)), Action: 2\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((13, 26, 2), (1, 7)), Action: 2\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((13, 26, 2), (1, 7)), Action: 2\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((13, 26, 2), (1, 7)), Action: 2\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((13, 26, 2), (1, 7)), Action: 2\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((13, 26, 2), (1, 7)), Action: 2\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((13, 26, 2), (1, 7)), Action: 2\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((13, 26, 2), (1, 7)), Action: 2\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((13, 26, 2), (1, 7)), Action: 2\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((13, 26, 2), (1, 7)), Action: 2\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((13, 26, 2), (1, 7)), Action: 2\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((13, 26, 2), (1, 7)), Action: 2\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((13, 26, 2), (1, 7)), Action: 2\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((13, 26, 2), (1, 7)), Action: 2\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((13, 26, 2), (1, 7)), Action: 2\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((13, 26, 2), (1, 7)), Action: 2\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((13, 26, 2), (1, 7)), Action: 2\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((13, 26, 2), (1, 7)), Action: 2\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((13, 26, 2), (1, 7)), Action: 2\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((13, 26, 2), (1, 7)), Action: 2\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((13, 26, 2), (1, 7)), Action: 2\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((13, 26, 2), (1, 7)), Action: 2\n",
      "State: ((12, 26, 0), (1, 7)), Action: 2\n",
      "State: ((6, 13, 1), (6, 9)), Action: 3\n",
      "State: ((5, 13, 0), (6, 9)), Action: 1\n",
      "State: ((5, 14, 1), (6, 9)), Action: 3\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n",
      "State: ((5, 14, 2), (6, 9)), Action: 2\n",
      "State: ((4, 14, 0), (6, 9)), Action: 2\n"
     ]
    }
   ],
   "source": [
    "transitions, beliefs = eval_agent(\"MDP\",Q ,env, num_episodes=20,max_episode_steps=50,render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.Tabular_Q_agent import Q_agent, Thompson_agent\n",
    "    \n",
    "qagent = Q_agent(Q,env)\n",
    "tagent = Thompson_agent(Q,env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POMDP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((24, 15, 1), (9, 9))\n",
      "Action:  1\n",
      "Reward:     -0.5  \n",
      "Next State:  ((25, 15, 2), (9, 9))\n",
      "argmax and max Belief:  (6, 9) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((25, 15, 2), (9, 9))\n",
      "Action:  3\n",
      "Reward:     -1.0  \n",
      "Next State:  ((25, 16, 1), (9, 9))\n",
      "argmax and max Belief:  (6, 9) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((25, 16, 1), (9, 9))\n",
      "Action:  1\n",
      "Reward:     -1.5  \n",
      "Next State:  ((26, 16, 2), (9, 9))\n",
      "argmax and max Belief:  (6, 9) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((26, 16, 2), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -2.0  \n",
      "Next State:  ((27, 16, 2), (9, 9))\n",
      "argmax and max Belief:  (6, 9) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((27, 16, 2), (9, 9))\n",
      "Action:  3\n",
      "Reward:     -2.5  \n",
      "Next State:  ((27, 17, 1), (9, 9))\n",
      "argmax and max Belief:  (6, 9) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((27, 17, 1), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -3.0  \n",
      "Next State:  ((27, 18, 1), (9, 9))\n",
      "argmax and max Belief:  (7, 9) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((27, 18, 1), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -3.5  \n",
      "Next State:  ((27, 19, 1), (9, 9))\n",
      "argmax and max Belief:  (7, 9) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((27, 19, 1), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -4.0  \n",
      "Next State:  ((27, 20, 1), (9, 9))\n",
      "argmax and max Belief:  (7, 9) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((27, 20, 1), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -4.5  \n",
      "Next State:  ((27, 21, 1), (9, 9))\n",
      "argmax and max Belief:  (8, 9) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n",
      "State ((27, 21, 1), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -5.0  \n",
      "Next State:  ((27, 22, 1), (9, 9))\n",
      "argmax and max Belief:  (8, 9) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n",
      "State ((27, 22, 1), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -5.5  \n",
      "Next State:  ((27, 23, 1), (9, 9))\n",
      "argmax and max Belief:  (8, 9) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n",
      "State ((27, 23, 1), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -6.0  \n",
      "Next State:  ((27, 24, 1), (9, 9))\n",
      "argmax and max Belief:  (9, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((27, 24, 1), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -6.5  \n",
      "Next State:  ((27, 25, 1), (9, 9))\n",
      "argmax and max Belief:  (9, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((27, 25, 1), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -7.0  \n",
      "Next State:  ((27, 26, 1), (9, 9))\n",
      "argmax and max Belief:  (9, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((27, 26, 1), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -6.0  \n",
      "Next State:  ((27, 27, 1), (9, 9))\n",
      "argmax and max Belief:  (9, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:00<00:07,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((2, 22, 1), (7, 6))\n",
      "Action:  0\n",
      "Reward:     -2.0  \n",
      "Next State:  ((2, 23, 1), (7, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0159)\n",
      "Belief entropy:  tensor(4.1431)\n",
      "\n",
      "\n",
      "State ((2, 23, 1), (7, 6))\n",
      "Action:  0\n",
      "Reward:     -4.0  \n",
      "Next State:  ((2, 24, 1), (7, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0159)\n",
      "Belief entropy:  tensor(4.1431)\n",
      "\n",
      "\n",
      "State ((2, 24, 1), (7, 6))\n",
      "Action:  3\n",
      "Reward:     -6.0  \n",
      "Next State:  ((1, 24, 0), (7, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0159)\n",
      "Belief entropy:  tensor(4.1431)\n",
      "\n",
      "\n",
      "State ((1, 24, 0), (7, 6))\n",
      "Action:  3\n",
      "Reward:     -8.0  \n",
      "Next State:  ((1, 23, 3), (7, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0159)\n",
      "Belief entropy:  tensor(4.1431)\n",
      "\n",
      "\n",
      "State ((1, 23, 3), (7, 6))\n",
      "Action:  0\n",
      "Reward:     -10.0  \n",
      "Next State:  ((1, 22, 3), (7, 6))\n",
      "argmax and max Belief:  (7, 1) tensor(0.1111)\n",
      "Belief entropy:  tensor(2.1972)\n",
      "\n",
      "\n",
      "State ((1, 22, 3), (7, 6))\n",
      "Action:  0\n",
      "Reward:     -10.5  \n",
      "Next State:  ((1, 21, 3), (7, 6))\n",
      "argmax and max Belief:  (7, 1) tensor(0.1111)\n",
      "Belief entropy:  tensor(2.1972)\n",
      "\n",
      "\n",
      "State ((1, 21, 3), (7, 6))\n",
      "Action:  3\n",
      "Reward:     -11.0  \n",
      "Next State:  ((2, 21, 2), (7, 6))\n",
      "argmax and max Belief:  (7, 1) tensor(0.1111)\n",
      "Belief entropy:  tensor(2.1972)\n",
      "\n",
      "\n",
      "State ((2, 21, 2), (7, 6))\n",
      "Action:  0\n",
      "Reward:     -11.5  \n",
      "Next State:  ((3, 21, 2), (7, 6))\n",
      "argmax and max Belief:  (7, 2) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((3, 21, 2), (7, 6))\n",
      "Action:  0\n",
      "Reward:     -12.0  \n",
      "Next State:  ((4, 21, 2), (7, 6))\n",
      "argmax and max Belief:  (7, 2) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((4, 21, 2), (7, 6))\n",
      "Action:  0\n",
      "Reward:     -12.5  \n",
      "Next State:  ((5, 21, 2), (7, 6))\n",
      "argmax and max Belief:  (7, 2) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((5, 21, 2), (7, 6))\n",
      "Action:  0\n",
      "Reward:     -13.0  \n",
      "Next State:  ((6, 21, 2), (7, 6))\n",
      "argmax and max Belief:  (7, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((6, 21, 2), (7, 6))\n",
      "Action:  0\n",
      "Reward:     -13.5  \n",
      "Next State:  ((7, 21, 2), (7, 6))\n",
      "argmax and max Belief:  (7, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((7, 21, 2), (7, 6))\n",
      "Action:  0\n",
      "Reward:     -14.0  \n",
      "Next State:  ((8, 21, 2), (7, 6))\n",
      "argmax and max Belief:  (7, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((8, 21, 2), (7, 6))\n",
      "Action:  0\n",
      "Reward:     -14.5  \n",
      "Next State:  ((9, 21, 2), (7, 6))\n",
      "argmax and max Belief:  (7, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((9, 21, 2), (7, 6))\n",
      "Action:  0\n",
      "Reward:     -15.0  \n",
      "Next State:  ((10, 21, 2), (7, 6))\n",
      "argmax and max Belief:  (7, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((10, 21, 2), (7, 6))\n",
      "Action:  0\n",
      "Reward:     -15.5  \n",
      "Next State:  ((11, 21, 2), (7, 6))\n",
      "argmax and max Belief:  (7, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((11, 21, 2), (7, 6))\n",
      "Action:  0\n",
      "Reward:     -16.0  \n",
      "Next State:  ((12, 21, 2), (7, 6))\n",
      "argmax and max Belief:  (7, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((12, 21, 2), (7, 6))\n",
      "Action:  0\n",
      "Reward:     -16.5  \n",
      "Next State:  ((13, 21, 2), (7, 6))\n",
      "argmax and max Belief:  (7, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((13, 21, 2), (7, 6))\n",
      "Action:  0\n",
      "Reward:     -17.0  \n",
      "Next State:  ((14, 21, 2), (7, 6))\n",
      "argmax and max Belief:  (7, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((14, 21, 2), (7, 6))\n",
      "Action:  0\n",
      "Reward:     -17.5  \n",
      "Next State:  ((15, 21, 2), (7, 6))\n",
      "argmax and max Belief:  (7, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:00<00:08,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((15, 21, 2), (7, 6))\n",
      "Action:  0\n",
      "Reward:     -18.0  \n",
      "Next State:  ((16, 21, 2), (7, 6))\n",
      "argmax and max Belief:  (7, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((16, 21, 2), (7, 6))\n",
      "Action:  0\n",
      "Reward:     -18.5  \n",
      "Next State:  ((17, 21, 2), (7, 6))\n",
      "argmax and max Belief:  (7, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((17, 21, 2), (7, 6))\n",
      "Action:  0\n",
      "Reward:     -17.5  \n",
      "Next State:  ((18, 21, 2), (7, 6))\n",
      "argmax and max Belief:  (7, 6) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((1, 10, 0), (1, 3))\n",
      "Action:  3\n",
      "Reward:     -2.0  \n",
      "Next State:  ((1, 9, 3), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0222)\n",
      "Belief entropy:  tensor(3.8067)\n",
      "\n",
      "\n",
      "State ((1, 9, 3), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -4.0  \n",
      "Next State:  ((1, 8, 3), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0278)\n",
      "Belief entropy:  tensor(3.5835)\n",
      "\n",
      "\n",
      "State ((1, 8, 3), (1, 3))\n",
      "Action:  2\n",
      "Reward:     -6.0  \n",
      "Next State:  ((1, 9, 1), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0278)\n",
      "Belief entropy:  tensor(3.5835)\n",
      "\n",
      "\n",
      "State ((1, 9, 1), (1, 3))\n",
      "Action:  1\n",
      "Reward:     -8.0  \n",
      "Next State:  ((2, 9, 2), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0294)\n",
      "Belief entropy:  tensor(3.5264)\n",
      "\n",
      "\n",
      "State ((2, 9, 2), (1, 3))\n",
      "Action:  1\n",
      "Reward:     -10.0  \n",
      "Next State:  ((2, 8, 3), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0294)\n",
      "Belief entropy:  tensor(3.5264)\n",
      "\n",
      "\n",
      "State ((2, 8, 3), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -12.0  \n",
      "Next State:  ((2, 7, 3), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0588)\n",
      "Belief entropy:  tensor(2.8332)\n",
      "\n",
      "\n",
      "State ((2, 7, 3), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -14.0  \n",
      "Next State:  ((2, 6, 3), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.1111)\n",
      "Belief entropy:  tensor(2.1972)\n",
      "\n",
      "\n",
      "State ((2, 6, 3), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -16.0  \n",
      "Next State:  ((2, 5, 3), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.1111)\n",
      "Belief entropy:  tensor(2.1972)\n",
      "\n",
      "\n",
      "State ((2, 5, 3), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -18.0  \n",
      "Next State:  ((2, 4, 3), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.1111)\n",
      "Belief entropy:  tensor(2.1972)\n",
      "\n",
      "\n",
      "State ((2, 4, 3), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -18.5  \n",
      "Next State:  ((2, 3, 3), (1, 3))\n",
      "argmax and max Belief:  (1, 2) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((2, 3, 3), (1, 3))\n",
      "Action:  3\n",
      "Reward:     -19.0  \n",
      "Next State:  ((3, 3, 2), (1, 3))\n",
      "argmax and max Belief:  (1, 2) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((3, 3, 2), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -19.5  \n",
      "Next State:  ((4, 3, 2), (1, 3))\n",
      "argmax and max Belief:  (1, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((4, 3, 2), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -20.0  \n",
      "Next State:  ((5, 3, 2), (1, 3))\n",
      "argmax and max Belief:  (1, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((5, 3, 2), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -20.5  \n",
      "Next State:  ((6, 3, 2), (1, 3))\n",
      "argmax and max Belief:  (1, 3) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((6, 3, 2), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -21.0  \n",
      "Next State:  ((7, 3, 2), (1, 3))\n",
      "argmax and max Belief:  (1, 3) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:01<00:07,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((7, 3, 2), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -21.5  \n",
      "Next State:  ((8, 3, 2), (1, 3))\n",
      "argmax and max Belief:  (1, 3) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((8, 3, 2), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -20.5  \n",
      "Next State:  ((9, 3, 2), (1, 3))\n",
      "argmax and max Belief:  (1, 3) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((2, 8, 3), (3, 7))\n",
      "Action:  0\n",
      "Reward:     -0.5  \n",
      "Next State:  ((2, 7, 3), (3, 7))\n",
      "argmax and max Belief:  (3, 2) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((2, 7, 3), (3, 7))\n",
      "Action:  3\n",
      "Reward:     -1.0  \n",
      "Next State:  ((3, 7, 2), (3, 7))\n",
      "argmax and max Belief:  (3, 2) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((3, 7, 2), (3, 7))\n",
      "Action:  0\n",
      "Reward:     -1.5  \n",
      "Next State:  ((4, 7, 2), (3, 7))\n",
      "argmax and max Belief:  (3, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((4, 7, 2), (3, 7))\n",
      "Action:  0\n",
      "Reward:     -2.0  \n",
      "Next State:  ((5, 7, 2), (3, 7))\n",
      "argmax and max Belief:  (3, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((5, 7, 2), (3, 7))\n",
      "Action:  3\n",
      "Reward:     -2.5  \n",
      "Next State:  ((5, 8, 1), (3, 7))\n",
      "argmax and max Belief:  (3, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((5, 8, 1), (3, 7))\n",
      "Action:  1\n",
      "Reward:     -3.0  \n",
      "Next State:  ((6, 8, 2), (3, 7))\n",
      "argmax and max Belief:  (3, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((6, 8, 2), (3, 7))\n",
      "Action:  0\n",
      "Reward:     -3.5  \n",
      "Next State:  ((7, 8, 2), (3, 7))\n",
      "argmax and max Belief:  (3, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((7, 8, 2), (3, 7))\n",
      "Action:  0\n",
      "Reward:     -4.0  \n",
      "Next State:  ((8, 8, 2), (3, 7))\n",
      "argmax and max Belief:  (3, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((8, 8, 2), (3, 7))\n",
      "Action:  0\n",
      "Reward:     -4.5  \n",
      "Next State:  ((9, 8, 2), (3, 7))\n",
      "argmax and max Belief:  (3, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((9, 8, 2), (3, 7))\n",
      "Action:  3\n",
      "Reward:     -5.0  \n",
      "Next State:  ((9, 9, 1), (3, 7))\n",
      "argmax and max Belief:  (3, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((9, 9, 1), (3, 7))\n",
      "Action:  1\n",
      "Reward:     -5.5  \n",
      "Next State:  ((10, 9, 2), (3, 7))\n",
      "argmax and max Belief:  (3, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((10, 9, 2), (3, 7))\n",
      "Action:  0\n",
      "Reward:     -6.0  \n",
      "Next State:  ((11, 9, 2), (3, 7))\n",
      "argmax and max Belief:  (3, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((11, 9, 2), (3, 7))\n",
      "Action:  0\n",
      "Reward:     -6.5  \n",
      "Next State:  ((12, 9, 2), (3, 7))\n",
      "argmax and max Belief:  (3, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((12, 9, 2), (3, 7))\n",
      "Action:  0\n",
      "Reward:     -7.0  \n",
      "Next State:  ((13, 9, 2), (3, 7))\n",
      "argmax and max Belief:  (3, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((13, 9, 2), (3, 7))\n",
      "Action:  0\n",
      "Reward:     -7.5  \n",
      "Next State:  ((14, 9, 2), (3, 7))\n",
      "argmax and max Belief:  (3, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((14, 9, 2), (3, 7))\n",
      "Action:  0\n",
      "Reward:     -8.0  \n",
      "Next State:  ((15, 9, 2), (3, 7))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((15, 9, 2), (3, 7))\n",
      "Action:  0\n",
      "Reward:     -8.5  \n",
      "Next State:  ((16, 9, 2), (3, 7))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:01<00:07,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((16, 9, 2), (3, 7))\n",
      "Action:  0\n",
      "Reward:     -9.0  \n",
      "Next State:  ((17, 9, 2), (3, 7))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((17, 9, 2), (3, 7))\n",
      "Action:  0\n",
      "Reward:     -9.5  \n",
      "Next State:  ((18, 9, 2), (3, 7))\n",
      "argmax and max Belief:  (3, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((18, 9, 2), (3, 7))\n",
      "Action:  0\n",
      "Reward:     -10.0  \n",
      "Next State:  ((19, 9, 2), (3, 7))\n",
      "argmax and max Belief:  (3, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((19, 9, 2), (3, 7))\n",
      "Action:  0\n",
      "Reward:     -10.5  \n",
      "Next State:  ((20, 9, 2), (3, 7))\n",
      "argmax and max Belief:  (3, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((20, 9, 2), (3, 7))\n",
      "Action:  0\n",
      "Reward:     -9.5  \n",
      "Next State:  ((21, 9, 2), (3, 7))\n",
      "argmax and max Belief:  (3, 7) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((23, 2, 3), (4, 5))\n",
      "Action:  3\n",
      "Reward:     -2.0  \n",
      "Next State:  ((24, 2, 2), (4, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0159)\n",
      "Belief entropy:  tensor(4.1431)\n",
      "\n",
      "\n",
      "State ((24, 2, 2), (4, 5))\n",
      "Action:  3\n",
      "Reward:     -4.0  \n",
      "Next State:  ((24, 3, 1), (4, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0159)\n",
      "Belief entropy:  tensor(4.1431)\n",
      "\n",
      "\n",
      "State ((24, 3, 1), (4, 5))\n",
      "Action:  2\n",
      "Reward:     -6.0  \n",
      "Next State:  ((24, 2, 3), (4, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0159)\n",
      "Belief entropy:  tensor(4.1431)\n",
      "\n",
      "\n",
      "State ((24, 2, 3), (4, 5))\n",
      "Action:  1\n",
      "Reward:     -8.0  \n",
      "Next State:  ((23, 2, 0), (4, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0159)\n",
      "Belief entropy:  tensor(4.1431)\n",
      "\n",
      "\n",
      "State ((23, 2, 0), (4, 5))\n",
      "Action:  3\n",
      "Reward:     -10.0  \n",
      "Next State:  ((23, 1, 3), (4, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0159)\n",
      "Belief entropy:  tensor(4.1431)\n",
      "\n",
      "\n",
      "State ((23, 1, 3), (4, 5))\n",
      "Action:  2\n",
      "Reward:     -12.0  \n",
      "Next State:  ((23, 2, 1), (4, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0159)\n",
      "Belief entropy:  tensor(4.1431)\n",
      "\n",
      "\n",
      "State ((23, 2, 1), (4, 5))\n",
      "Action:  3\n",
      "Reward:     -14.0  \n",
      "Next State:  ((22, 2, 0), (4, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0185)\n",
      "Belief entropy:  tensor(3.9890)\n",
      "\n",
      "\n",
      "State ((22, 2, 0), (4, 5))\n",
      "Action:  0\n",
      "Reward:     -16.0  \n",
      "Next State:  ((21, 2, 0), (4, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0185)\n",
      "Belief entropy:  tensor(3.9890)\n",
      "\n",
      "\n",
      "State ((21, 2, 0), (4, 5))\n",
      "Action:  3\n",
      "Reward:     -18.0  \n",
      "Next State:  ((21, 1, 3), (4, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0185)\n",
      "Belief entropy:  tensor(3.9890)\n",
      "\n",
      "\n",
      "State ((21, 1, 3), (4, 5))\n",
      "Action:  3\n",
      "Reward:     -20.0  \n",
      "Next State:  ((22, 1, 2), (4, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0185)\n",
      "Belief entropy:  tensor(3.9890)\n",
      "\n",
      "\n",
      "State ((22, 1, 2), (4, 5))\n",
      "Action:  0\n",
      "Reward:     -22.0  \n",
      "Next State:  ((23, 1, 2), (4, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0185)\n",
      "Belief entropy:  tensor(3.9890)\n",
      "\n",
      "\n",
      "State ((23, 1, 2), (4, 5))\n",
      "Action:  0\n",
      "Reward:     -24.0  \n",
      "Next State:  ((24, 1, 2), (4, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0185)\n",
      "Belief entropy:  tensor(3.9890)\n",
      "\n",
      "\n",
      "State ((24, 1, 2), (4, 5))\n",
      "Action:  3\n",
      "Reward:     -26.0  \n",
      "Next State:  ((24, 2, 1), (4, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0185)\n",
      "Belief entropy:  tensor(3.9890)\n",
      "\n",
      "\n",
      "State ((24, 2, 1), (4, 5))\n",
      "Action:  3\n",
      "Reward:     -28.0  \n",
      "Next State:  ((23, 2, 0), (4, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0185)\n",
      "Belief entropy:  tensor(3.9890)\n",
      "\n",
      "\n",
      "State ((23, 2, 0), (4, 5))\n",
      "Action:  0\n",
      "Reward:     -30.0  \n",
      "Next State:  ((22, 2, 0), (4, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0185)\n",
      "Belief entropy:  tensor(3.9890)\n",
      "\n",
      "\n",
      "State ((22, 2, 0), (4, 5))\n",
      "Action:  0\n",
      "Reward:     -32.0  \n",
      "Next State:  ((21, 2, 0), (4, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0185)\n",
      "Belief entropy:  tensor(3.9890)\n",
      "\n",
      "\n",
      "State ((21, 2, 0), (4, 5))\n",
      "Action:  0\n",
      "Reward:     -34.0  \n",
      "Next State:  ((20, 2, 0), (4, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0185)\n",
      "Belief entropy:  tensor(3.9890)\n",
      "\n",
      "\n",
      "State ((20, 2, 0), (4, 5))\n",
      "Action:  0\n",
      "Reward:     -36.0  \n",
      "Next State:  ((19, 2, 0), (4, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0222)\n",
      "Belief entropy:  tensor(3.8067)\n",
      "\n",
      "\n",
      "State ((19, 2, 0), (4, 5))\n",
      "Action:  1\n",
      "Reward:     -38.0  \n",
      "Next State:  ((19, 3, 1), (4, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0222)\n",
      "Belief entropy:  tensor(3.8067)\n",
      "\n",
      "\n",
      "State ((19, 3, 1), (4, 5))\n",
      "Action:  3\n",
      "Reward:     -40.0  \n",
      "Next State:  ((18, 3, 0), (4, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0222)\n",
      "Belief entropy:  tensor(3.8067)\n",
      "\n",
      "\n",
      "State ((18, 3, 0), (4, 5))\n",
      "Action:  0\n",
      "Reward:     -42.0  \n",
      "Next State:  ((17, 3, 0), (4, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0222)\n",
      "Belief entropy:  tensor(3.8067)\n",
      "\n",
      "\n",
      "State ((17, 3, 0), (4, 5))\n",
      "Action:  0\n",
      "Reward:     -44.0  \n",
      "Next State:  ((16, 3, 0), (4, 5))\n",
      "argmax and max Belief:  (2, 5) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((16, 3, 0), (4, 5))\n",
      "Action:  0\n",
      "Reward:     -44.5  \n",
      "Next State:  ((15, 3, 0), (4, 5))\n",
      "argmax and max Belief:  (2, 5) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((15, 3, 0), (4, 5))\n",
      "Action:  1\n",
      "Reward:     -45.0  \n",
      "Next State:  ((15, 4, 1), (4, 5))\n",
      "argmax and max Belief:  (2, 5) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((15, 4, 1), (4, 5))\n",
      "Action:  0\n",
      "Reward:     -45.5  \n",
      "Next State:  ((15, 5, 1), (4, 5))\n",
      "argmax and max Belief:  (2, 5) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((15, 5, 1), (4, 5))\n",
      "Action:  0\n",
      "Reward:     -46.0  \n",
      "Next State:  ((15, 6, 1), (4, 5))\n",
      "argmax and max Belief:  (3, 5) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((15, 6, 1), (4, 5))\n",
      "Action:  0\n",
      "Reward:     -46.5  \n",
      "Next State:  ((15, 7, 1), (4, 5))\n",
      "argmax and max Belief:  (3, 5) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((15, 7, 1), (4, 5))\n",
      "Action:  0\n",
      "Reward:     -47.0  \n",
      "Next State:  ((15, 8, 1), (4, 5))\n",
      "argmax and max Belief:  (3, 5) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((15, 8, 1), (4, 5))\n",
      "Action:  0\n",
      "Reward:     -47.5  \n",
      "Next State:  ((15, 9, 1), (4, 5))\n",
      "argmax and max Belief:  (4, 5) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((15, 9, 1), (4, 5))\n",
      "Action:  0\n",
      "Reward:     -48.0  \n",
      "Next State:  ((15, 10, 1), (4, 5))\n",
      "argmax and max Belief:  (4, 5) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:02<00:08,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((15, 10, 1), (4, 5))\n",
      "Action:  0\n",
      "Reward:     -48.5  \n",
      "Next State:  ((15, 11, 1), (4, 5))\n",
      "argmax and max Belief:  (4, 5) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((15, 11, 1), (4, 5))\n",
      "Action:  0\n",
      "Reward:     -47.5  \n",
      "Next State:  ((15, 12, 1), (4, 5))\n",
      "argmax and max Belief:  (4, 5) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((14, 26, 2), (6, 1))\n",
      "Action:  2\n",
      "Reward:     -2.0  \n",
      "Next State:  ((13, 26, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0133)\n",
      "Belief entropy:  tensor(4.3175)\n",
      "\n",
      "\n",
      "State ((13, 26, 0), (6, 1))\n",
      "Action:  1\n",
      "Reward:     -4.0  \n",
      "Next State:  ((13, 27, 1), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0133)\n",
      "Belief entropy:  tensor(4.3175)\n",
      "\n",
      "\n",
      "State ((13, 27, 1), (6, 1))\n",
      "Action:  1\n",
      "Reward:     -6.0  \n",
      "Next State:  ((14, 27, 2), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0133)\n",
      "Belief entropy:  tensor(4.3175)\n",
      "\n",
      "\n",
      "State ((14, 27, 2), (6, 1))\n",
      "Action:  2\n",
      "Reward:     -8.0  \n",
      "Next State:  ((13, 27, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0133)\n",
      "Belief entropy:  tensor(4.3175)\n",
      "\n",
      "\n",
      "State ((13, 27, 0), (6, 1))\n",
      "Action:  3\n",
      "Reward:     -10.0  \n",
      "Next State:  ((13, 26, 3), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0133)\n",
      "Belief entropy:  tensor(4.3175)\n",
      "\n",
      "\n",
      "State ((13, 26, 3), (6, 1))\n",
      "Action:  2\n",
      "Reward:     -12.0  \n",
      "Next State:  ((13, 27, 1), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0133)\n",
      "Belief entropy:  tensor(4.3175)\n",
      "\n",
      "\n",
      "State ((13, 27, 1), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -14.0  \n",
      "Next State:  ((13, 27, 1), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0133)\n",
      "Belief entropy:  tensor(4.3175)\n",
      "\n",
      "\n",
      "State ((13, 27, 1), (6, 1))\n",
      "Action:  3\n",
      "Reward:     -16.0  \n",
      "Next State:  ((12, 27, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0133)\n",
      "Belief entropy:  tensor(4.3175)\n",
      "\n",
      "\n",
      "State ((12, 27, 0), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -18.0  \n",
      "Next State:  ((11, 27, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0133)\n",
      "Belief entropy:  tensor(4.3175)\n",
      "\n",
      "\n",
      "State ((11, 27, 0), (6, 1))\n",
      "Action:  2\n",
      "Reward:     -20.0  \n",
      "Next State:  ((12, 27, 2), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0133)\n",
      "Belief entropy:  tensor(4.3175)\n",
      "\n",
      "\n",
      "State ((12, 27, 2), (6, 1))\n",
      "Action:  2\n",
      "Reward:     -22.0  \n",
      "Next State:  ((11, 27, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0133)\n",
      "Belief entropy:  tensor(4.3175)\n",
      "\n",
      "\n",
      "State ((11, 27, 0), (6, 1))\n",
      "Action:  1\n",
      "Reward:     -24.0  \n",
      "Next State:  ((11, 27, 1), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0133)\n",
      "Belief entropy:  tensor(4.3175)\n",
      "\n",
      "\n",
      "State ((11, 27, 1), (6, 1))\n",
      "Action:  3\n",
      "Reward:     -26.0  \n",
      "Next State:  ((10, 27, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0135)\n",
      "Belief entropy:  tensor(4.3041)\n",
      "\n",
      "\n",
      "State ((10, 27, 0), (6, 1))\n",
      "Action:  3\n",
      "Reward:     -28.0  \n",
      "Next State:  ((10, 26, 3), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0135)\n",
      "Belief entropy:  tensor(4.3041)\n",
      "\n",
      "\n",
      "State ((10, 26, 3), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -30.0  \n",
      "Next State:  ((10, 25, 3), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0149)\n",
      "Belief entropy:  tensor(4.2047)\n",
      "\n",
      "\n",
      "State ((10, 25, 3), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -32.0  \n",
      "Next State:  ((10, 24, 3), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0149)\n",
      "Belief entropy:  tensor(4.2047)\n",
      "\n",
      "\n",
      "State ((10, 24, 3), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -34.0  \n",
      "Next State:  ((10, 23, 3), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0149)\n",
      "Belief entropy:  tensor(4.2047)\n",
      "\n",
      "\n",
      "State ((10, 23, 3), (6, 1))\n",
      "Action:  2\n",
      "Reward:     -36.0  \n",
      "Next State:  ((10, 24, 1), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0149)\n",
      "Belief entropy:  tensor(4.2047)\n",
      "\n",
      "\n",
      "State ((10, 24, 1), (6, 1))\n",
      "Action:  3\n",
      "Reward:     -38.0  \n",
      "Next State:  ((9, 24, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0149)\n",
      "Belief entropy:  tensor(4.2047)\n",
      "\n",
      "\n",
      "State ((9, 24, 0), (6, 1))\n",
      "Action:  3\n",
      "Reward:     -40.0  \n",
      "Next State:  ((9, 23, 3), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0149)\n",
      "Belief entropy:  tensor(4.2047)\n",
      "\n",
      "\n",
      "State ((9, 23, 3), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -42.0  \n",
      "Next State:  ((9, 22, 3), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((9, 22, 3), (6, 1))\n",
      "Action:  2\n",
      "Reward:     -44.0  \n",
      "Next State:  ((9, 23, 1), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((9, 23, 1), (6, 1))\n",
      "Action:  1\n",
      "Reward:     -46.0  \n",
      "Next State:  ((10, 23, 2), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 23, 2), (6, 1))\n",
      "Action:  1\n",
      "Reward:     -48.0  \n",
      "Next State:  ((10, 22, 3), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 22, 3), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -50.0  \n",
      "Next State:  ((10, 21, 3), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 21, 3), (6, 1))\n",
      "Action:  1\n",
      "Reward:     -52.0  \n",
      "Next State:  ((9, 21, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((9, 21, 0), (6, 1))\n",
      "Action:  1\n",
      "Reward:     -54.0  \n",
      "Next State:  ((9, 22, 1), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((9, 22, 1), (6, 1))\n",
      "Action:  3\n",
      "Reward:     -56.0  \n",
      "Next State:  ((8, 22, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((8, 22, 0), (6, 1))\n",
      "Action:  1\n",
      "Reward:     -58.0  \n",
      "Next State:  ((8, 23, 1), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((8, 23, 1), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -60.0  \n",
      "Next State:  ((8, 24, 1), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((8, 24, 1), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -62.0  \n",
      "Next State:  ((8, 25, 1), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((8, 25, 1), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -64.0  \n",
      "Next State:  ((8, 26, 1), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((8, 26, 1), (6, 1))\n",
      "Action:  1\n",
      "Reward:     -66.0  \n",
      "Next State:  ((9, 26, 2), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((9, 26, 2), (6, 1))\n",
      "Action:  2\n",
      "Reward:     -68.0  \n",
      "Next State:  ((8, 26, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((8, 26, 0), (6, 1))\n",
      "Action:  3\n",
      "Reward:     -70.0  \n",
      "Next State:  ((8, 25, 3), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((8, 25, 3), (6, 1))\n",
      "Action:  1\n",
      "Reward:     -72.0  \n",
      "Next State:  ((7, 25, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0172)\n",
      "Belief entropy:  tensor(4.0604)\n",
      "\n",
      "\n",
      "State ((7, 25, 0), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -74.0  \n",
      "Next State:  ((6, 25, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0172)\n",
      "Belief entropy:  tensor(4.0604)\n",
      "\n",
      "\n",
      "State ((6, 25, 0), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -76.0  \n",
      "Next State:  ((5, 25, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0172)\n",
      "Belief entropy:  tensor(4.0604)\n",
      "\n",
      "\n",
      "State ((5, 25, 0), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -78.0  \n",
      "Next State:  ((4, 25, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0179)\n",
      "Belief entropy:  tensor(4.0254)\n",
      "\n",
      "\n",
      "State ((4, 25, 0), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -80.0  \n",
      "Next State:  ((3, 25, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0179)\n",
      "Belief entropy:  tensor(4.0254)\n",
      "\n",
      "\n",
      "State ((3, 25, 0), (6, 1))\n",
      "Action:  2\n",
      "Reward:     -82.0  \n",
      "Next State:  ((4, 25, 2), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0179)\n",
      "Belief entropy:  tensor(4.0254)\n",
      "\n",
      "\n",
      "State ((4, 25, 2), (6, 1))\n",
      "Action:  2\n",
      "Reward:     -84.0  \n",
      "Next State:  ((3, 25, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0179)\n",
      "Belief entropy:  tensor(4.0254)\n",
      "\n",
      "\n",
      "State ((3, 25, 0), (6, 1))\n",
      "Action:  2\n",
      "Reward:     -86.0  \n",
      "Next State:  ((4, 25, 2), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0179)\n",
      "Belief entropy:  tensor(4.0254)\n",
      "\n",
      "\n",
      "State ((4, 25, 2), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -88.0  \n",
      "Next State:  ((5, 25, 2), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0179)\n",
      "Belief entropy:  tensor(4.0254)\n",
      "\n",
      "\n",
      "State ((5, 25, 2), (6, 1))\n",
      "Action:  3\n",
      "Reward:     -90.0  \n",
      "Next State:  ((5, 26, 1), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0179)\n",
      "Belief entropy:  tensor(4.0254)\n",
      "\n",
      "\n",
      "State ((5, 26, 1), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -92.0  \n",
      "Next State:  ((5, 27, 1), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0179)\n",
      "Belief entropy:  tensor(4.0254)\n",
      "\n",
      "\n",
      "State ((5, 27, 1), (6, 1))\n",
      "Action:  2\n",
      "Reward:     -94.0  \n",
      "Next State:  ((5, 26, 3), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0179)\n",
      "Belief entropy:  tensor(4.0254)\n",
      "\n",
      "\n",
      "State ((5, 26, 3), (6, 1))\n",
      "Action:  1\n",
      "Reward:     -96.0  \n",
      "Next State:  ((4, 26, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0179)\n",
      "Belief entropy:  tensor(4.0254)\n",
      "\n",
      "\n",
      "State ((4, 26, 0), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -98.0  \n",
      "Next State:  ((3, 26, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0179)\n",
      "Belief entropy:  tensor(4.0254)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:03<00:10,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((3, 26, 0), (6, 1))\n",
      "Action:  2\n",
      "Reward:     -100.0  \n",
      "Next State:  ((4, 26, 2), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0179)\n",
      "Belief entropy:  tensor(4.0254)\n",
      "\n",
      "\n",
      "State ((16, 22, 2), (4, 3))\n",
      "Action:  1\n",
      "Reward:     -2.0  \n",
      "Next State:  ((16, 21, 3), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((16, 21, 3), (4, 3))\n",
      "Action:  2\n",
      "Reward:     -4.0  \n",
      "Next State:  ((16, 22, 1), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((16, 22, 1), (4, 3))\n",
      "Action:  2\n",
      "Reward:     -6.0  \n",
      "Next State:  ((16, 21, 3), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((16, 21, 3), (4, 3))\n",
      "Action:  3\n",
      "Reward:     -8.0  \n",
      "Next State:  ((17, 21, 2), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((17, 21, 2), (4, 3))\n",
      "Action:  1\n",
      "Reward:     -10.0  \n",
      "Next State:  ((17, 20, 3), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((17, 20, 3), (4, 3))\n",
      "Action:  3\n",
      "Reward:     -12.0  \n",
      "Next State:  ((18, 20, 2), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((18, 20, 2), (4, 3))\n",
      "Action:  0\n",
      "Reward:     -14.0  \n",
      "Next State:  ((19, 20, 2), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((19, 20, 2), (4, 3))\n",
      "Action:  2\n",
      "Reward:     -16.0  \n",
      "Next State:  ((18, 20, 0), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((18, 20, 0), (4, 3))\n",
      "Action:  3\n",
      "Reward:     -18.0  \n",
      "Next State:  ((18, 19, 3), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0161)\n",
      "Belief entropy:  tensor(4.1271)\n",
      "\n",
      "\n",
      "State ((18, 19, 3), (4, 3))\n",
      "Action:  3\n",
      "Reward:     -20.0  \n",
      "Next State:  ((19, 19, 2), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0161)\n",
      "Belief entropy:  tensor(4.1271)\n",
      "\n",
      "\n",
      "State ((19, 19, 2), (4, 3))\n",
      "Action:  2\n",
      "Reward:     -22.0  \n",
      "Next State:  ((18, 19, 0), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0161)\n",
      "Belief entropy:  tensor(4.1271)\n",
      "\n",
      "\n",
      "State ((18, 19, 0), (4, 3))\n",
      "Action:  3\n",
      "Reward:     -24.0  \n",
      "Next State:  ((18, 18, 3), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0161)\n",
      "Belief entropy:  tensor(4.1271)\n",
      "\n",
      "\n",
      "State ((18, 18, 3), (4, 3))\n",
      "Action:  0\n",
      "Reward:     -26.0  \n",
      "Next State:  ((18, 17, 3), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0161)\n",
      "Belief entropy:  tensor(4.1271)\n",
      "\n",
      "\n",
      "State ((18, 17, 3), (4, 3))\n",
      "Action:  1\n",
      "Reward:     -28.0  \n",
      "Next State:  ((17, 17, 0), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0161)\n",
      "Belief entropy:  tensor(4.1271)\n",
      "\n",
      "\n",
      "State ((17, 17, 0), (4, 3))\n",
      "Action:  3\n",
      "Reward:     -30.0  \n",
      "Next State:  ((17, 16, 3), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0172)\n",
      "Belief entropy:  tensor(4.0604)\n",
      "\n",
      "\n",
      "State ((17, 16, 3), (4, 3))\n",
      "Action:  1\n",
      "Reward:     -32.0  \n",
      "Next State:  ((16, 16, 0), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0179)\n",
      "Belief entropy:  tensor(4.0254)\n",
      "\n",
      "\n",
      "State ((16, 16, 0), (4, 3))\n",
      "Action:  3\n",
      "Reward:     -34.0  \n",
      "Next State:  ((16, 15, 3), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0179)\n",
      "Belief entropy:  tensor(4.0254)\n",
      "\n",
      "\n",
      "State ((16, 15, 3), (4, 3))\n",
      "Action:  2\n",
      "Reward:     -36.0  \n",
      "Next State:  ((16, 16, 1), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0179)\n",
      "Belief entropy:  tensor(4.0254)\n",
      "\n",
      "\n",
      "State ((16, 16, 1), (4, 3))\n",
      "Action:  2\n",
      "Reward:     -38.0  \n",
      "Next State:  ((16, 15, 3), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0179)\n",
      "Belief entropy:  tensor(4.0254)\n",
      "\n",
      "\n",
      "State ((16, 15, 3), (4, 3))\n",
      "Action:  1\n",
      "Reward:     -40.0  \n",
      "Next State:  ((15, 15, 0), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0179)\n",
      "Belief entropy:  tensor(4.0254)\n",
      "\n",
      "\n",
      "State ((15, 15, 0), (4, 3))\n",
      "Action:  0\n",
      "Reward:     -42.0  \n",
      "Next State:  ((14, 15, 0), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0179)\n",
      "Belief entropy:  tensor(4.0254)\n",
      "\n",
      "\n",
      "State ((14, 15, 0), (4, 3))\n",
      "Action:  0\n",
      "Reward:     -44.0  \n",
      "Next State:  ((13, 15, 0), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0196)\n",
      "Belief entropy:  tensor(3.9318)\n",
      "\n",
      "\n",
      "State ((13, 15, 0), (4, 3))\n",
      "Action:  0\n",
      "Reward:     -46.0  \n",
      "Next State:  ((12, 15, 0), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0196)\n",
      "Belief entropy:  tensor(3.9318)\n",
      "\n",
      "\n",
      "State ((12, 15, 0), (4, 3))\n",
      "Action:  3\n",
      "Reward:     -48.0  \n",
      "Next State:  ((12, 14, 3), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0196)\n",
      "Belief entropy:  tensor(3.9318)\n",
      "\n",
      "\n",
      "State ((12, 14, 3), (4, 3))\n",
      "Action:  0\n",
      "Reward:     -50.0  \n",
      "Next State:  ((12, 13, 3), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0222)\n",
      "Belief entropy:  tensor(3.8067)\n",
      "\n",
      "\n",
      "State ((12, 13, 3), (4, 3))\n",
      "Action:  1\n",
      "Reward:     -52.0  \n",
      "Next State:  ((11, 13, 0), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0222)\n",
      "Belief entropy:  tensor(3.8067)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:04<00:09,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((11, 13, 0), (4, 3))\n",
      "Action:  3\n",
      "Reward:     -54.0  \n",
      "Next State:  ((11, 12, 3), (4, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0222)\n",
      "Belief entropy:  tensor(3.8067)\n",
      "\n",
      "\n",
      "State ((11, 12, 3), (4, 3))\n",
      "Action:  1\n",
      "Reward:     -56.0  \n",
      "Next State:  ((10, 12, 0), (4, 3))\n",
      "argmax and max Belief:  (4, 3) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((10, 12, 0), (4, 3))\n",
      "Action:  0\n",
      "Reward:     -55.0  \n",
      "Next State:  ((9, 12, 0), (4, 3))\n",
      "argmax and max Belief:  (4, 3) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((15, 8, 2), (7, 4))\n",
      "Action:  0\n",
      "Reward:     -2.0  \n",
      "Next State:  ((16, 8, 2), (7, 4))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0182)\n",
      "Belief entropy:  tensor(4.0073)\n",
      "\n",
      "\n",
      "State ((16, 8, 2), (7, 4))\n",
      "Action:  1\n",
      "Reward:     -4.0  \n",
      "Next State:  ((16, 7, 3), (7, 4))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((16, 7, 3), (7, 4))\n",
      "Action:  1\n",
      "Reward:     -6.0  \n",
      "Next State:  ((15, 7, 0), (7, 4))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((15, 7, 0), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -8.0  \n",
      "Next State:  ((15, 6, 3), (7, 4))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0244)\n",
      "Belief entropy:  tensor(3.7136)\n",
      "\n",
      "\n",
      "State ((15, 6, 3), (7, 4))\n",
      "Action:  0\n",
      "Reward:     -10.0  \n",
      "Next State:  ((15, 5, 3), (7, 4))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0244)\n",
      "Belief entropy:  tensor(3.7136)\n",
      "\n",
      "\n",
      "State ((15, 5, 3), (7, 4))\n",
      "Action:  1\n",
      "Reward:     -12.0  \n",
      "Next State:  ((14, 5, 0), (7, 4))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0244)\n",
      "Belief entropy:  tensor(3.7136)\n",
      "\n",
      "\n",
      "State ((14, 5, 0), (7, 4))\n",
      "Action:  0\n",
      "Reward:     -14.0  \n",
      "Next State:  ((13, 5, 0), (7, 4))\n",
      "argmax and max Belief:  (2, 4) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((13, 5, 0), (7, 4))\n",
      "Action:  0\n",
      "Reward:     -14.5  \n",
      "Next State:  ((12, 5, 0), (7, 4))\n",
      "argmax and max Belief:  (2, 4) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((12, 5, 0), (7, 4))\n",
      "Action:  1\n",
      "Reward:     -15.0  \n",
      "Next State:  ((12, 6, 1), (7, 4))\n",
      "argmax and max Belief:  (3, 4) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((12, 6, 1), (7, 4))\n",
      "Action:  0\n",
      "Reward:     -15.5  \n",
      "Next State:  ((12, 7, 1), (7, 4))\n",
      "argmax and max Belief:  (3, 4) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((12, 7, 1), (7, 4))\n",
      "Action:  0\n",
      "Reward:     -16.0  \n",
      "Next State:  ((12, 8, 1), (7, 4))\n",
      "argmax and max Belief:  (3, 4) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((12, 8, 1), (7, 4))\n",
      "Action:  0\n",
      "Reward:     -16.5  \n",
      "Next State:  ((12, 9, 1), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((12, 9, 1), (7, 4))\n",
      "Action:  0\n",
      "Reward:     -17.0  \n",
      "Next State:  ((12, 10, 1), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((12, 10, 1), (7, 4))\n",
      "Action:  0\n",
      "Reward:     -17.5  \n",
      "Next State:  ((12, 11, 1), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((12, 11, 1), (7, 4))\n",
      "Action:  0\n",
      "Reward:     -18.0  \n",
      "Next State:  ((12, 12, 1), (7, 4))\n",
      "argmax and max Belief:  (5, 4) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((12, 12, 1), (7, 4))\n",
      "Action:  0\n",
      "Reward:     -18.5  \n",
      "Next State:  ((12, 13, 1), (7, 4))\n",
      "argmax and max Belief:  (5, 4) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((12, 13, 1), (7, 4))\n",
      "Action:  0\n",
      "Reward:     -19.0  \n",
      "Next State:  ((12, 14, 1), (7, 4))\n",
      "argmax and max Belief:  (5, 4) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((12, 14, 1), (7, 4))\n",
      "Action:  0\n",
      "Reward:     -19.5  \n",
      "Next State:  ((12, 15, 1), (7, 4))\n",
      "argmax and max Belief:  (6, 4) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:04<00:08,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((12, 15, 1), (7, 4))\n",
      "Action:  0\n",
      "Reward:     -20.0  \n",
      "Next State:  ((12, 16, 1), (7, 4))\n",
      "argmax and max Belief:  (6, 4) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((12, 16, 1), (7, 4))\n",
      "Action:  0\n",
      "Reward:     -20.5  \n",
      "Next State:  ((12, 17, 1), (7, 4))\n",
      "argmax and max Belief:  (6, 4) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((12, 17, 1), (7, 4))\n",
      "Action:  0\n",
      "Reward:     -21.0  \n",
      "Next State:  ((12, 18, 1), (7, 4))\n",
      "argmax and max Belief:  (7, 4) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((12, 18, 1), (7, 4))\n",
      "Action:  0\n",
      "Reward:     -21.5  \n",
      "Next State:  ((12, 19, 1), (7, 4))\n",
      "argmax and max Belief:  (7, 4) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((12, 19, 1), (7, 4))\n",
      "Action:  0\n",
      "Reward:     -22.0  \n",
      "Next State:  ((12, 20, 1), (7, 4))\n",
      "argmax and max Belief:  (7, 4) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((12, 20, 1), (7, 4))\n",
      "Action:  0\n",
      "Reward:     -21.0  \n",
      "Next State:  ((12, 21, 1), (7, 4))\n",
      "argmax and max Belief:  (7, 4) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((17, 16, 2), (4, 8))\n",
      "Action:  0\n",
      "Reward:     -2.0  \n",
      "Next State:  ((18, 16, 2), (4, 8))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0164)\n",
      "Belief entropy:  tensor(4.1109)\n",
      "\n",
      "\n",
      "State ((18, 16, 2), (4, 8))\n",
      "Action:  1\n",
      "Reward:     -4.0  \n",
      "Next State:  ((18, 15, 3), (4, 8))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0164)\n",
      "Belief entropy:  tensor(4.1109)\n",
      "\n",
      "\n",
      "State ((18, 15, 3), (4, 8))\n",
      "Action:  1\n",
      "Reward:     -6.0  \n",
      "Next State:  ((17, 15, 0), (4, 8))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0164)\n",
      "Belief entropy:  tensor(4.1109)\n",
      "\n",
      "\n",
      "State ((17, 15, 0), (4, 8))\n",
      "Action:  0\n",
      "Reward:     -8.0  \n",
      "Next State:  ((16, 15, 0), (4, 8))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0179)\n",
      "Belief entropy:  tensor(4.0254)\n",
      "\n",
      "\n",
      "State ((16, 15, 0), (4, 8))\n",
      "Action:  0\n",
      "Reward:     -10.0  \n",
      "Next State:  ((15, 15, 0), (4, 8))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0179)\n",
      "Belief entropy:  tensor(4.0254)\n",
      "\n",
      "\n",
      "State ((15, 15, 0), (4, 8))\n",
      "Action:  3\n",
      "Reward:     -12.0  \n",
      "Next State:  ((15, 14, 3), (4, 8))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0179)\n",
      "Belief entropy:  tensor(4.0254)\n",
      "\n",
      "\n",
      "State ((15, 14, 3), (4, 8))\n",
      "Action:  1\n",
      "Reward:     -14.0  \n",
      "Next State:  ((14, 14, 0), (4, 8))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0179)\n",
      "Belief entropy:  tensor(4.0254)\n",
      "\n",
      "\n",
      "State ((14, 14, 0), (4, 8))\n",
      "Action:  3\n",
      "Reward:     -16.0  \n",
      "Next State:  ((14, 13, 3), (4, 8))\n",
      "argmax and max Belief:  (4, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((14, 13, 3), (4, 8))\n",
      "Action:  0\n",
      "Reward:     -16.5  \n",
      "Next State:  ((14, 12, 3), (4, 8))\n",
      "argmax and max Belief:  (4, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((14, 12, 3), (4, 8))\n",
      "Action:  3\n",
      "Reward:     -17.0  \n",
      "Next State:  ((15, 12, 2), (4, 8))\n",
      "argmax and max Belief:  (4, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((15, 12, 2), (4, 8))\n",
      "Action:  0\n",
      "Reward:     -17.5  \n",
      "Next State:  ((16, 12, 2), (4, 8))\n",
      "argmax and max Belief:  (4, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((16, 12, 2), (4, 8))\n",
      "Action:  0\n",
      "Reward:     -18.0  \n",
      "Next State:  ((17, 12, 2), (4, 8))\n",
      "argmax and max Belief:  (4, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((17, 12, 2), (4, 8))\n",
      "Action:  0\n",
      "Reward:     -18.5  \n",
      "Next State:  ((18, 12, 2), (4, 8))\n",
      "argmax and max Belief:  (4, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((18, 12, 2), (4, 8))\n",
      "Action:  0\n",
      "Reward:     -19.0  \n",
      "Next State:  ((19, 12, 2), (4, 8))\n",
      "argmax and max Belief:  (4, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((19, 12, 2), (4, 8))\n",
      "Action:  0\n",
      "Reward:     -19.5  \n",
      "Next State:  ((20, 12, 2), (4, 8))\n",
      "argmax and max Belief:  (4, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((20, 12, 2), (4, 8))\n",
      "Action:  0\n",
      "Reward:     -20.0  \n",
      "Next State:  ((21, 12, 2), (4, 8))\n",
      "argmax and max Belief:  (4, 8) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n",
      "State ((21, 12, 2), (4, 8))\n",
      "Action:  0\n",
      "Reward:     -20.5  \n",
      "Next State:  ((22, 12, 2), (4, 8))\n",
      "argmax and max Belief:  (4, 8) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n",
      "State ((22, 12, 2), (4, 8))\n",
      "Action:  0\n",
      "Reward:     -21.0  \n",
      "Next State:  ((23, 12, 2), (4, 8))\n",
      "argmax and max Belief:  (4, 8) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:05<00:06,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((23, 12, 2), (4, 8))\n",
      "Action:  0\n",
      "Reward:     -20.0  \n",
      "Next State:  ((24, 12, 2), (4, 8))\n",
      "argmax and max Belief:  (4, 8) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((14, 27, 2), (7, 8))\n",
      "Action:  0\n",
      "Reward:     -2.0  \n",
      "Next State:  ((15, 27, 2), (7, 8))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0132)\n",
      "Belief entropy:  tensor(4.3307)\n",
      "\n",
      "\n",
      "State ((15, 27, 2), (7, 8))\n",
      "Action:  2\n",
      "Reward:     -4.0  \n",
      "Next State:  ((14, 27, 0), (7, 8))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0132)\n",
      "Belief entropy:  tensor(4.3307)\n",
      "\n",
      "\n",
      "State ((14, 27, 0), (7, 8))\n",
      "Action:  3\n",
      "Reward:     -6.0  \n",
      "Next State:  ((14, 26, 3), (7, 8))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0132)\n",
      "Belief entropy:  tensor(4.3307)\n",
      "\n",
      "\n",
      "State ((14, 26, 3), (7, 8))\n",
      "Action:  0\n",
      "Reward:     -8.0  \n",
      "Next State:  ((14, 25, 3), (7, 8))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((14, 25, 3), (7, 8))\n",
      "Action:  2\n",
      "Reward:     -10.0  \n",
      "Next State:  ((14, 26, 1), (7, 8))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((14, 26, 1), (7, 8))\n",
      "Action:  2\n",
      "Reward:     -12.0  \n",
      "Next State:  ((14, 25, 3), (7, 8))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((14, 25, 3), (7, 8))\n",
      "Action:  1\n",
      "Reward:     -14.0  \n",
      "Next State:  ((13, 25, 0), (7, 8))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 25, 0), (7, 8))\n",
      "Action:  0\n",
      "Reward:     -16.0  \n",
      "Next State:  ((12, 25, 0), (7, 8))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((12, 25, 0), (7, 8))\n",
      "Action:  3\n",
      "Reward:     -18.0  \n",
      "Next State:  ((12, 24, 3), (7, 8))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((12, 24, 3), (7, 8))\n",
      "Action:  1\n",
      "Reward:     -20.0  \n",
      "Next State:  ((11, 24, 0), (7, 8))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((11, 24, 0), (7, 8))\n",
      "Action:  3\n",
      "Reward:     -22.0  \n",
      "Next State:  ((11, 23, 3), (7, 8))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((11, 23, 3), (7, 8))\n",
      "Action:  0\n",
      "Reward:     -24.0  \n",
      "Next State:  ((11, 22, 3), (7, 8))\n",
      "argmax and max Belief:  (7, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((11, 22, 3), (7, 8))\n",
      "Action:  0\n",
      "Reward:     -24.5  \n",
      "Next State:  ((11, 21, 3), (7, 8))\n",
      "argmax and max Belief:  (7, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((11, 21, 3), (7, 8))\n",
      "Action:  3\n",
      "Reward:     -25.0  \n",
      "Next State:  ((12, 21, 2), (7, 8))\n",
      "argmax and max Belief:  (7, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((12, 21, 2), (7, 8))\n",
      "Action:  0\n",
      "Reward:     -25.5  \n",
      "Next State:  ((13, 21, 2), (7, 8))\n",
      "argmax and max Belief:  (7, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((13, 21, 2), (7, 8))\n",
      "Action:  0\n",
      "Reward:     -26.0  \n",
      "Next State:  ((14, 21, 2), (7, 8))\n",
      "argmax and max Belief:  (7, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((14, 21, 2), (7, 8))\n",
      "Action:  0\n",
      "Reward:     -26.5  \n",
      "Next State:  ((15, 21, 2), (7, 8))\n",
      "argmax and max Belief:  (7, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((15, 21, 2), (7, 8))\n",
      "Action:  0\n",
      "Reward:     -27.0  \n",
      "Next State:  ((16, 21, 2), (7, 8))\n",
      "argmax and max Belief:  (7, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((16, 21, 2), (7, 8))\n",
      "Action:  0\n",
      "Reward:     -27.5  \n",
      "Next State:  ((17, 21, 2), (7, 8))\n",
      "argmax and max Belief:  (7, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((17, 21, 2), (7, 8))\n",
      "Action:  0\n",
      "Reward:     -28.0  \n",
      "Next State:  ((18, 21, 2), (7, 8))\n",
      "argmax and max Belief:  (7, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((18, 21, 2), (7, 8))\n",
      "Action:  0\n",
      "Reward:     -28.5  \n",
      "Next State:  ((19, 21, 2), (7, 8))\n",
      "argmax and max Belief:  (7, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((19, 21, 2), (7, 8))\n",
      "Action:  0\n",
      "Reward:     -29.0  \n",
      "Next State:  ((20, 21, 2), (7, 8))\n",
      "argmax and max Belief:  (7, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((20, 21, 2), (7, 8))\n",
      "Action:  0\n",
      "Reward:     -29.5  \n",
      "Next State:  ((21, 21, 2), (7, 8))\n",
      "argmax and max Belief:  (7, 8) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n",
      "State ((21, 21, 2), (7, 8))\n",
      "Action:  0\n",
      "Reward:     -30.0  \n",
      "Next State:  ((22, 21, 2), (7, 8))\n",
      "argmax and max Belief:  (7, 8) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n",
      "State ((22, 21, 2), (7, 8))\n",
      "Action:  0\n",
      "Reward:     -30.5  \n",
      "Next State:  ((23, 21, 2), (7, 8))\n",
      "argmax and max Belief:  (7, 8) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:06<00:06,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((23, 21, 2), (7, 8))\n",
      "Action:  0\n",
      "Reward:     -29.5  \n",
      "Next State:  ((24, 21, 2), (7, 8))\n",
      "argmax and max Belief:  (7, 8) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((15, 24, 1), (6, 5))\n",
      "Action:  2\n",
      "Reward:     -2.0  \n",
      "Next State:  ((15, 23, 3), (6, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 23, 3), (6, 5))\n",
      "Action:  3\n",
      "Reward:     -4.0  \n",
      "Next State:  ((16, 23, 2), (6, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 23, 2), (6, 5))\n",
      "Action:  2\n",
      "Reward:     -6.0  \n",
      "Next State:  ((15, 23, 0), (6, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 23, 0), (6, 5))\n",
      "Action:  3\n",
      "Reward:     -8.0  \n",
      "Next State:  ((15, 22, 3), (6, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((15, 22, 3), (6, 5))\n",
      "Action:  0\n",
      "Reward:     -10.0  \n",
      "Next State:  ((15, 21, 3), (6, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((15, 21, 3), (6, 5))\n",
      "Action:  1\n",
      "Reward:     -12.0  \n",
      "Next State:  ((14, 21, 0), (6, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((14, 21, 0), (6, 5))\n",
      "Action:  1\n",
      "Reward:     -14.0  \n",
      "Next State:  ((14, 22, 1), (6, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((14, 22, 1), (6, 5))\n",
      "Action:  3\n",
      "Reward:     -16.0  \n",
      "Next State:  ((13, 22, 0), (6, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0159)\n",
      "Belief entropy:  tensor(4.1431)\n",
      "\n",
      "\n",
      "State ((13, 22, 0), (6, 5))\n",
      "Action:  0\n",
      "Reward:     -18.0  \n",
      "Next State:  ((12, 22, 0), (6, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0159)\n",
      "Belief entropy:  tensor(4.1431)\n",
      "\n",
      "\n",
      "State ((12, 22, 0), (6, 5))\n",
      "Action:  0\n",
      "Reward:     -20.0  \n",
      "Next State:  ((11, 22, 0), (6, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0159)\n",
      "Belief entropy:  tensor(4.1431)\n",
      "\n",
      "\n",
      "State ((11, 22, 0), (6, 5))\n",
      "Action:  3\n",
      "Reward:     -22.0  \n",
      "Next State:  ((11, 21, 3), (6, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0159)\n",
      "Belief entropy:  tensor(4.1431)\n",
      "\n",
      "\n",
      "State ((11, 21, 3), (6, 5))\n",
      "Action:  0\n",
      "Reward:     -24.0  \n",
      "Next State:  ((11, 20, 3), (6, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0159)\n",
      "Belief entropy:  tensor(4.1431)\n",
      "\n",
      "\n",
      "State ((11, 20, 3), (6, 5))\n",
      "Action:  2\n",
      "Reward:     -26.0  \n",
      "Next State:  ((11, 21, 1), (6, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0159)\n",
      "Belief entropy:  tensor(4.1431)\n",
      "\n",
      "\n",
      "State ((11, 21, 1), (6, 5))\n",
      "Action:  1\n",
      "Reward:     -28.0  \n",
      "Next State:  ((12, 21, 2), (6, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0159)\n",
      "Belief entropy:  tensor(4.1431)\n",
      "\n",
      "\n",
      "State ((12, 21, 2), (6, 5))\n",
      "Action:  1\n",
      "Reward:     -30.0  \n",
      "Next State:  ((12, 20, 3), (6, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0159)\n",
      "Belief entropy:  tensor(4.1431)\n",
      "\n",
      "\n",
      "State ((12, 20, 3), (6, 5))\n",
      "Action:  0\n",
      "Reward:     -32.0  \n",
      "Next State:  ((12, 19, 3), (6, 5))\n",
      "argmax and max Belief:  (6, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((12, 19, 3), (6, 5))\n",
      "Action:  0\n",
      "Reward:     -32.5  \n",
      "Next State:  ((12, 18, 3), (6, 5))\n",
      "argmax and max Belief:  (6, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:06<00:05,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((12, 18, 3), (6, 5))\n",
      "Action:  3\n",
      "Reward:     -33.0  \n",
      "Next State:  ((13, 18, 2), (6, 5))\n",
      "argmax and max Belief:  (6, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((13, 18, 2), (6, 5))\n",
      "Action:  0\n",
      "Reward:     -33.5  \n",
      "Next State:  ((14, 18, 2), (6, 5))\n",
      "argmax and max Belief:  (6, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((14, 18, 2), (6, 5))\n",
      "Action:  0\n",
      "Reward:     -32.5  \n",
      "Next State:  ((15, 18, 2), (6, 5))\n",
      "argmax and max Belief:  (6, 5) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((18, 7, 1), (2, 1))\n",
      "Action:  0\n",
      "Reward:     -2.0  \n",
      "Next State:  ((18, 8, 1), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0189)\n",
      "Belief entropy:  tensor(3.9703)\n",
      "\n",
      "\n",
      "State ((18, 8, 1), (2, 1))\n",
      "Action:  2\n",
      "Reward:     -4.0  \n",
      "Next State:  ((18, 7, 3), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 7, 3), (2, 1))\n",
      "Action:  1\n",
      "Reward:     -6.0  \n",
      "Next State:  ((17, 7, 0), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((17, 7, 0), (2, 1))\n",
      "Action:  0\n",
      "Reward:     -8.0  \n",
      "Next State:  ((16, 7, 0), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0244)\n",
      "Belief entropy:  tensor(3.7136)\n",
      "\n",
      "\n",
      "State ((16, 7, 0), (2, 1))\n",
      "Action:  3\n",
      "Reward:     -10.0  \n",
      "Next State:  ((16, 6, 3), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0244)\n",
      "Belief entropy:  tensor(3.7136)\n",
      "\n",
      "\n",
      "State ((16, 6, 3), (2, 1))\n",
      "Action:  2\n",
      "Reward:     -12.0  \n",
      "Next State:  ((16, 7, 1), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0244)\n",
      "Belief entropy:  tensor(3.7136)\n",
      "\n",
      "\n",
      "State ((16, 7, 1), (2, 1))\n",
      "Action:  0\n",
      "Reward:     -14.0  \n",
      "Next State:  ((16, 8, 1), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0244)\n",
      "Belief entropy:  tensor(3.7136)\n",
      "\n",
      "\n",
      "State ((16, 8, 1), (2, 1))\n",
      "Action:  3\n",
      "Reward:     -16.0  \n",
      "Next State:  ((15, 8, 0), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0244)\n",
      "Belief entropy:  tensor(3.7136)\n",
      "\n",
      "\n",
      "State ((15, 8, 0), (2, 1))\n",
      "Action:  0\n",
      "Reward:     -18.0  \n",
      "Next State:  ((14, 8, 0), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0244)\n",
      "Belief entropy:  tensor(3.7136)\n",
      "\n",
      "\n",
      "State ((14, 8, 0), (2, 1))\n",
      "Action:  0\n",
      "Reward:     -20.0  \n",
      "Next State:  ((13, 8, 0), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0294)\n",
      "Belief entropy:  tensor(3.5264)\n",
      "\n",
      "\n",
      "State ((13, 8, 0), (2, 1))\n",
      "Action:  0\n",
      "Reward:     -22.0  \n",
      "Next State:  ((12, 8, 0), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0294)\n",
      "Belief entropy:  tensor(3.5264)\n",
      "\n",
      "\n",
      "State ((12, 8, 0), (2, 1))\n",
      "Action:  0\n",
      "Reward:     -24.0  \n",
      "Next State:  ((11, 8, 0), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0294)\n",
      "Belief entropy:  tensor(3.5264)\n",
      "\n",
      "\n",
      "State ((11, 8, 0), (2, 1))\n",
      "Action:  0\n",
      "Reward:     -26.0  \n",
      "Next State:  ((10, 8, 0), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0370)\n",
      "Belief entropy:  tensor(3.2958)\n",
      "\n",
      "\n",
      "State ((10, 8, 0), (2, 1))\n",
      "Action:  0\n",
      "Reward:     -28.0  \n",
      "Next State:  ((9, 8, 0), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0370)\n",
      "Belief entropy:  tensor(3.2958)\n",
      "\n",
      "\n",
      "State ((9, 8, 0), (2, 1))\n",
      "Action:  0\n",
      "Reward:     -30.0  \n",
      "Next State:  ((8, 8, 0), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0370)\n",
      "Belief entropy:  tensor(3.2958)\n",
      "\n",
      "\n",
      "State ((8, 8, 0), (2, 1))\n",
      "Action:  3\n",
      "Reward:     -32.0  \n",
      "Next State:  ((8, 7, 3), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0400)\n",
      "Belief entropy:  tensor(3.2189)\n",
      "\n",
      "\n",
      "State ((8, 7, 3), (2, 1))\n",
      "Action:  2\n",
      "Reward:     -34.0  \n",
      "Next State:  ((8, 8, 1), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0400)\n",
      "Belief entropy:  tensor(3.2189)\n",
      "\n",
      "\n",
      "State ((8, 8, 1), (2, 1))\n",
      "Action:  3\n",
      "Reward:     -36.0  \n",
      "Next State:  ((7, 8, 0), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0556)\n",
      "Belief entropy:  tensor(2.8904)\n",
      "\n",
      "\n",
      "State ((7, 8, 0), (2, 1))\n",
      "Action:  3\n",
      "Reward:     -38.0  \n",
      "Next State:  ((7, 7, 3), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0588)\n",
      "Belief entropy:  tensor(2.8332)\n",
      "\n",
      "\n",
      "State ((7, 7, 3), (2, 1))\n",
      "Action:  1\n",
      "Reward:     -40.0  \n",
      "Next State:  ((6, 7, 0), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0588)\n",
      "Belief entropy:  tensor(2.8332)\n",
      "\n",
      "\n",
      "State ((6, 7, 0), (2, 1))\n",
      "Action:  3\n",
      "Reward:     -42.0  \n",
      "Next State:  ((6, 6, 3), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0588)\n",
      "Belief entropy:  tensor(2.8332)\n",
      "\n",
      "\n",
      "State ((6, 6, 3), (2, 1))\n",
      "Action:  1\n",
      "Reward:     -44.0  \n",
      "Next State:  ((5, 6, 0), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0588)\n",
      "Belief entropy:  tensor(2.8332)\n",
      "\n",
      "\n",
      "State ((5, 6, 0), (2, 1))\n",
      "Action:  3\n",
      "Reward:     -46.0  \n",
      "Next State:  ((5, 5, 3), (2, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0588)\n",
      "Belief entropy:  tensor(2.8332)\n",
      "\n",
      "\n",
      "State ((5, 5, 3), (2, 1))\n",
      "Action:  1\n",
      "Reward:     -48.0  \n",
      "Next State:  ((4, 5, 0), (2, 1))\n",
      "argmax and max Belief:  (2, 1) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [00:07<00:04,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((4, 5, 0), (2, 1))\n",
      "Action:  0\n",
      "Reward:     -48.5  \n",
      "Next State:  ((3, 5, 0), (2, 1))\n",
      "argmax and max Belief:  (2, 1) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((3, 5, 0), (2, 1))\n",
      "Action:  1\n",
      "Reward:     -47.5  \n",
      "Next State:  ((3, 6, 1), (2, 1))\n",
      "argmax and max Belief:  (2, 1) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((27, 20, 3), (6, 9))\n",
      "Action:  1\n",
      "Reward:     -2.0  \n",
      "Next State:  ((26, 20, 0), (6, 9))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0128)\n",
      "Belief entropy:  tensor(4.3567)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [00:07<00:03,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((26, 20, 0), (6, 9))\n",
      "Action:  2\n",
      "Reward:     -4.0  \n",
      "Next State:  ((27, 20, 2), (6, 9))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0128)\n",
      "Belief entropy:  tensor(4.3567)\n",
      "\n",
      "\n",
      "State ((27, 20, 2), (6, 9))\n",
      "Action:  1\n",
      "Reward:     -6.0  \n",
      "Next State:  ((27, 19, 3), (6, 9))\n",
      "argmax and max Belief:  (6, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((27, 19, 3), (6, 9))\n",
      "Action:  0\n",
      "Reward:     -5.0  \n",
      "Next State:  ((27, 18, 3), (6, 9))\n",
      "argmax and max Belief:  (6, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((11, 10, 2), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -2.0  \n",
      "Next State:  ((12, 10, 2), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0208)\n",
      "Belief entropy:  tensor(3.8712)\n",
      "\n",
      "\n",
      "State ((12, 10, 2), (6, 1))\n",
      "Action:  3\n",
      "Reward:     -4.0  \n",
      "Next State:  ((12, 11, 1), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0208)\n",
      "Belief entropy:  tensor(3.8712)\n",
      "\n",
      "\n",
      "State ((12, 11, 1), (6, 1))\n",
      "Action:  3\n",
      "Reward:     -6.0  \n",
      "Next State:  ((11, 11, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0208)\n",
      "Belief entropy:  tensor(3.8712)\n",
      "\n",
      "\n",
      "State ((11, 11, 0), (6, 1))\n",
      "Action:  1\n",
      "Reward:     -8.0  \n",
      "Next State:  ((11, 12, 1), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0222)\n",
      "Belief entropy:  tensor(3.8067)\n",
      "\n",
      "\n",
      "State ((11, 12, 1), (6, 1))\n",
      "Action:  3\n",
      "Reward:     -10.0  \n",
      "Next State:  ((10, 12, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0256)\n",
      "Belief entropy:  tensor(3.6636)\n",
      "\n",
      "\n",
      "State ((10, 12, 0), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -12.0  \n",
      "Next State:  ((9, 12, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0256)\n",
      "Belief entropy:  tensor(3.6636)\n",
      "\n",
      "\n",
      "State ((9, 12, 0), (6, 1))\n",
      "Action:  3\n",
      "Reward:     -14.0  \n",
      "Next State:  ((9, 11, 3), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0256)\n",
      "Belief entropy:  tensor(3.6636)\n",
      "\n",
      "\n",
      "State ((9, 11, 3), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -16.0  \n",
      "Next State:  ((9, 10, 3), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0263)\n",
      "Belief entropy:  tensor(3.6376)\n",
      "\n",
      "\n",
      "State ((9, 10, 3), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -18.0  \n",
      "Next State:  ((9, 9, 3), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0286)\n",
      "Belief entropy:  tensor(3.5553)\n",
      "\n",
      "\n",
      "State ((9, 9, 3), (6, 1))\n",
      "Action:  1\n",
      "Reward:     -20.0  \n",
      "Next State:  ((8, 9, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0286)\n",
      "Belief entropy:  tensor(3.5553)\n",
      "\n",
      "\n",
      "State ((8, 9, 0), (6, 1))\n",
      "Action:  3\n",
      "Reward:     -22.0  \n",
      "Next State:  ((8, 8, 3), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0312)\n",
      "Belief entropy:  tensor(3.4657)\n",
      "\n",
      "\n",
      "State ((8, 8, 3), (6, 1))\n",
      "Action:  1\n",
      "Reward:     -24.0  \n",
      "Next State:  ((7, 8, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0400)\n",
      "Belief entropy:  tensor(3.2189)\n",
      "\n",
      "\n",
      "State ((7, 8, 0), (6, 1))\n",
      "Action:  3\n",
      "Reward:     -26.0  \n",
      "Next State:  ((7, 7, 3), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0588)\n",
      "Belief entropy:  tensor(2.8332)\n",
      "\n",
      "\n",
      "State ((7, 7, 3), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -28.0  \n",
      "Next State:  ((7, 6, 3), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0588)\n",
      "Belief entropy:  tensor(2.8332)\n",
      "\n",
      "\n",
      "State ((7, 6, 3), (6, 1))\n",
      "Action:  1\n",
      "Reward:     -30.0  \n",
      "Next State:  ((6, 6, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0588)\n",
      "Belief entropy:  tensor(2.8332)\n",
      "\n",
      "\n",
      "State ((6, 6, 0), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -32.0  \n",
      "Next State:  ((5, 6, 0), (6, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0588)\n",
      "Belief entropy:  tensor(2.8332)\n",
      "\n",
      "\n",
      "State ((5, 6, 0), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -34.0  \n",
      "Next State:  ((4, 6, 0), (6, 1))\n",
      "argmax and max Belief:  (3, 1) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((4, 6, 0), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -34.5  \n",
      "Next State:  ((3, 6, 0), (6, 1))\n",
      "argmax and max Belief:  (6, 1) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((3, 6, 0), (6, 1))\n",
      "Action:  1\n",
      "Reward:     -35.0  \n",
      "Next State:  ((3, 7, 1), (6, 1))\n",
      "argmax and max Belief:  (6, 1) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((3, 7, 1), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -35.5  \n",
      "Next State:  ((3, 8, 1), (6, 1))\n",
      "argmax and max Belief:  (6, 1) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((3, 8, 1), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -36.0  \n",
      "Next State:  ((3, 9, 1), (6, 1))\n",
      "argmax and max Belief:  (6, 1) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((3, 9, 1), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -36.5  \n",
      "Next State:  ((3, 10, 1), (6, 1))\n",
      "argmax and max Belief:  (6, 1) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((3, 10, 1), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -37.0  \n",
      "Next State:  ((3, 11, 1), (6, 1))\n",
      "argmax and max Belief:  (6, 1) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((3, 11, 1), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -37.5  \n",
      "Next State:  ((3, 12, 1), (6, 1))\n",
      "argmax and max Belief:  (6, 1) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((3, 12, 1), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -38.0  \n",
      "Next State:  ((3, 13, 1), (6, 1))\n",
      "argmax and max Belief:  (6, 1) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((3, 13, 1), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -38.5  \n",
      "Next State:  ((3, 14, 1), (6, 1))\n",
      "argmax and max Belief:  (6, 1) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((3, 14, 1), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -39.0  \n",
      "Next State:  ((3, 15, 1), (6, 1))\n",
      "argmax and max Belief:  (6, 1) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((3, 15, 1), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -39.5  \n",
      "Next State:  ((3, 16, 1), (6, 1))\n",
      "argmax and max Belief:  (6, 1) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((3, 16, 1), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -40.0  \n",
      "Next State:  ((3, 17, 1), (6, 1))\n",
      "argmax and max Belief:  (6, 1) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [00:07<00:03,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((3, 17, 1), (6, 1))\n",
      "Action:  0\n",
      "Reward:     -39.0  \n",
      "Next State:  ((3, 18, 1), (6, 1))\n",
      "argmax and max Belief:  (6, 1) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((8, 26, 2), (8, 9))\n",
      "Action:  3\n",
      "Reward:     -2.0  \n",
      "Next State:  ((8, 27, 1), (8, 9))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0135)\n",
      "Belief entropy:  tensor(4.3041)\n",
      "\n",
      "\n",
      "State ((8, 27, 1), (8, 9))\n",
      "Action:  2\n",
      "Reward:     -4.0  \n",
      "Next State:  ((8, 26, 3), (8, 9))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0135)\n",
      "Belief entropy:  tensor(4.3041)\n",
      "\n",
      "\n",
      "State ((8, 26, 3), (8, 9))\n",
      "Action:  1\n",
      "Reward:     -6.0  \n",
      "Next State:  ((7, 26, 0), (8, 9))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0137)\n",
      "Belief entropy:  tensor(4.2905)\n",
      "\n",
      "\n",
      "State ((7, 26, 0), (8, 9))\n",
      "Action:  3\n",
      "Reward:     -8.0  \n",
      "Next State:  ((7, 25, 3), (8, 9))\n",
      "argmax and max Belief:  (8, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((7, 25, 3), (8, 9))\n",
      "Action:  0\n",
      "Reward:     -8.5  \n",
      "Next State:  ((7, 24, 3), (8, 9))\n",
      "argmax and max Belief:  (8, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((7, 24, 3), (8, 9))\n",
      "Action:  3\n",
      "Reward:     -9.0  \n",
      "Next State:  ((8, 24, 2), (8, 9))\n",
      "argmax and max Belief:  (8, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((8, 24, 2), (8, 9))\n",
      "Action:  0\n",
      "Reward:     -9.5  \n",
      "Next State:  ((9, 24, 2), (8, 9))\n",
      "argmax and max Belief:  (8, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((9, 24, 2), (8, 9))\n",
      "Action:  0\n",
      "Reward:     -10.0  \n",
      "Next State:  ((10, 24, 2), (8, 9))\n",
      "argmax and max Belief:  (8, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((10, 24, 2), (8, 9))\n",
      "Action:  0\n",
      "Reward:     -10.5  \n",
      "Next State:  ((11, 24, 2), (8, 9))\n",
      "argmax and max Belief:  (8, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((11, 24, 2), (8, 9))\n",
      "Action:  0\n",
      "Reward:     -11.0  \n",
      "Next State:  ((12, 24, 2), (8, 9))\n",
      "argmax and max Belief:  (8, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((12, 24, 2), (8, 9))\n",
      "Action:  0\n",
      "Reward:     -11.5  \n",
      "Next State:  ((13, 24, 2), (8, 9))\n",
      "argmax and max Belief:  (8, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((13, 24, 2), (8, 9))\n",
      "Action:  0\n",
      "Reward:     -12.0  \n",
      "Next State:  ((14, 24, 2), (8, 9))\n",
      "argmax and max Belief:  (8, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((14, 24, 2), (8, 9))\n",
      "Action:  0\n",
      "Reward:     -12.5  \n",
      "Next State:  ((15, 24, 2), (8, 9))\n",
      "argmax and max Belief:  (8, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((15, 24, 2), (8, 9))\n",
      "Action:  0\n",
      "Reward:     -13.0  \n",
      "Next State:  ((16, 24, 2), (8, 9))\n",
      "argmax and max Belief:  (8, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((16, 24, 2), (8, 9))\n",
      "Action:  0\n",
      "Reward:     -13.5  \n",
      "Next State:  ((17, 24, 2), (8, 9))\n",
      "argmax and max Belief:  (8, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((17, 24, 2), (8, 9))\n",
      "Action:  0\n",
      "Reward:     -14.0  \n",
      "Next State:  ((18, 24, 2), (8, 9))\n",
      "argmax and max Belief:  (8, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((18, 24, 2), (8, 9))\n",
      "Action:  0\n",
      "Reward:     -14.5  \n",
      "Next State:  ((19, 24, 2), (8, 9))\n",
      "argmax and max Belief:  (8, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((19, 24, 2), (8, 9))\n",
      "Action:  0\n",
      "Reward:     -15.0  \n",
      "Next State:  ((20, 24, 2), (8, 9))\n",
      "argmax and max Belief:  (8, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((20, 24, 2), (8, 9))\n",
      "Action:  0\n",
      "Reward:     -15.5  \n",
      "Next State:  ((21, 24, 2), (8, 9))\n",
      "argmax and max Belief:  (8, 8) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n",
      "State ((21, 24, 2), (8, 9))\n",
      "Action:  0\n",
      "Reward:     -16.0  \n",
      "Next State:  ((22, 24, 2), (8, 9))\n",
      "argmax and max Belief:  (8, 8) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n",
      "State ((22, 24, 2), (8, 9))\n",
      "Action:  0\n",
      "Reward:     -16.5  \n",
      "Next State:  ((23, 24, 2), (8, 9))\n",
      "argmax and max Belief:  (8, 8) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [00:08<00:02,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((23, 24, 2), (8, 9))\n",
      "Action:  0\n",
      "Reward:     -17.0  \n",
      "Next State:  ((24, 24, 2), (8, 9))\n",
      "argmax and max Belief:  (8, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((24, 24, 2), (8, 9))\n",
      "Action:  0\n",
      "Reward:     -17.5  \n",
      "Next State:  ((25, 24, 2), (8, 9))\n",
      "argmax and max Belief:  (8, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((25, 24, 2), (8, 9))\n",
      "Action:  0\n",
      "Reward:     -18.0  \n",
      "Next State:  ((26, 24, 2), (8, 9))\n",
      "argmax and max Belief:  (8, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((26, 24, 2), (8, 9))\n",
      "Action:  0\n",
      "Reward:     -17.0  \n",
      "Next State:  ((27, 24, 2), (8, 9))\n",
      "argmax and max Belief:  (8, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((5, 20, 3), (6, 5))\n",
      "Action:  0\n",
      "Reward:     -2.0  \n",
      "Next State:  ((5, 19, 3), (6, 5))\n",
      "argmax and max Belief:  (6, 2) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((5, 19, 3), (6, 5))\n",
      "Action:  0\n",
      "Reward:     -2.5  \n",
      "Next State:  ((5, 18, 3), (6, 5))\n",
      "argmax and max Belief:  (6, 2) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((5, 18, 3), (6, 5))\n",
      "Action:  3\n",
      "Reward:     -3.0  \n",
      "Next State:  ((6, 18, 2), (6, 5))\n",
      "argmax and max Belief:  (6, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((6, 18, 2), (6, 5))\n",
      "Action:  0\n",
      "Reward:     -3.5  \n",
      "Next State:  ((7, 18, 2), (6, 5))\n",
      "argmax and max Belief:  (6, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((7, 18, 2), (6, 5))\n",
      "Action:  0\n",
      "Reward:     -4.0  \n",
      "Next State:  ((8, 18, 2), (6, 5))\n",
      "argmax and max Belief:  (6, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((8, 18, 2), (6, 5))\n",
      "Action:  0\n",
      "Reward:     -4.5  \n",
      "Next State:  ((9, 18, 2), (6, 5))\n",
      "argmax and max Belief:  (6, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((9, 18, 2), (6, 5))\n",
      "Action:  0\n",
      "Reward:     -5.0  \n",
      "Next State:  ((10, 18, 2), (6, 5))\n",
      "argmax and max Belief:  (6, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((10, 18, 2), (6, 5))\n",
      "Action:  0\n",
      "Reward:     -5.5  \n",
      "Next State:  ((11, 18, 2), (6, 5))\n",
      "argmax and max Belief:  (6, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((11, 18, 2), (6, 5))\n",
      "Action:  0\n",
      "Reward:     -6.0  \n",
      "Next State:  ((12, 18, 2), (6, 5))\n",
      "argmax and max Belief:  (6, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [00:08<00:01,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((12, 18, 2), (6, 5))\n",
      "Action:  0\n",
      "Reward:     -6.5  \n",
      "Next State:  ((13, 18, 2), (6, 5))\n",
      "argmax and max Belief:  (6, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((13, 18, 2), (6, 5))\n",
      "Action:  0\n",
      "Reward:     -7.0  \n",
      "Next State:  ((14, 18, 2), (6, 5))\n",
      "argmax and max Belief:  (6, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((14, 18, 2), (6, 5))\n",
      "Action:  0\n",
      "Reward:     -6.0  \n",
      "Next State:  ((15, 18, 2), (6, 5))\n",
      "argmax and max Belief:  (6, 5) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((13, 1, 2), (6, 3))\n",
      "Action:  3\n",
      "Reward:     -2.0  \n",
      "Next State:  ((13, 2, 1), (6, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0370)\n",
      "Belief entropy:  tensor(3.2958)\n",
      "\n",
      "\n",
      "State ((13, 2, 1), (6, 3))\n",
      "Action:  3\n",
      "Reward:     -4.0  \n",
      "Next State:  ((12, 2, 0), (6, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0370)\n",
      "Belief entropy:  tensor(3.2958)\n",
      "\n",
      "\n",
      "State ((12, 2, 0), (6, 3))\n",
      "Action:  0\n",
      "Reward:     -6.0  \n",
      "Next State:  ((11, 2, 0), (6, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0370)\n",
      "Belief entropy:  tensor(3.2958)\n",
      "\n",
      "\n",
      "State ((11, 2, 0), (6, 3))\n",
      "Action:  0\n",
      "Reward:     -8.0  \n",
      "Next State:  ((10, 2, 0), (6, 3))\n",
      "argmax and max Belief:  (1, 3) tensor(0.1111)\n",
      "Belief entropy:  tensor(2.1972)\n",
      "\n",
      "\n",
      "State ((10, 2, 0), (6, 3))\n",
      "Action:  0\n",
      "Reward:     -8.5  \n",
      "Next State:  ((9, 2, 0), (6, 3))\n",
      "argmax and max Belief:  (1, 3) tensor(0.1111)\n",
      "Belief entropy:  tensor(2.1972)\n",
      "\n",
      "\n",
      "State ((9, 2, 0), (6, 3))\n",
      "Action:  1\n",
      "Reward:     -9.0  \n",
      "Next State:  ((9, 3, 1), (6, 3))\n",
      "argmax and max Belief:  (2, 3) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((9, 3, 1), (6, 3))\n",
      "Action:  0\n",
      "Reward:     -9.5  \n",
      "Next State:  ((9, 4, 1), (6, 3))\n",
      "argmax and max Belief:  (2, 3) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((9, 4, 1), (6, 3))\n",
      "Action:  0\n",
      "Reward:     -10.0  \n",
      "Next State:  ((9, 5, 1), (6, 3))\n",
      "argmax and max Belief:  (2, 3) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((9, 5, 1), (6, 3))\n",
      "Action:  0\n",
      "Reward:     -10.5  \n",
      "Next State:  ((9, 6, 1), (6, 3))\n",
      "argmax and max Belief:  (3, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((9, 6, 1), (6, 3))\n",
      "Action:  0\n",
      "Reward:     -11.0  \n",
      "Next State:  ((9, 7, 1), (6, 3))\n",
      "argmax and max Belief:  (3, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((9, 7, 1), (6, 3))\n",
      "Action:  0\n",
      "Reward:     -11.5  \n",
      "Next State:  ((9, 8, 1), (6, 3))\n",
      "argmax and max Belief:  (3, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((9, 8, 1), (6, 3))\n",
      "Action:  0\n",
      "Reward:     -12.0  \n",
      "Next State:  ((9, 9, 1), (6, 3))\n",
      "argmax and max Belief:  (4, 3) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((9, 9, 1), (6, 3))\n",
      "Action:  0\n",
      "Reward:     -12.5  \n",
      "Next State:  ((9, 10, 1), (6, 3))\n",
      "argmax and max Belief:  (4, 3) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((9, 10, 1), (6, 3))\n",
      "Action:  0\n",
      "Reward:     -13.0  \n",
      "Next State:  ((9, 11, 1), (6, 3))\n",
      "argmax and max Belief:  (4, 3) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((9, 11, 1), (6, 3))\n",
      "Action:  0\n",
      "Reward:     -13.5  \n",
      "Next State:  ((9, 12, 1), (6, 3))\n",
      "argmax and max Belief:  (5, 3) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((9, 12, 1), (6, 3))\n",
      "Action:  0\n",
      "Reward:     -14.0  \n",
      "Next State:  ((9, 13, 1), (6, 3))\n",
      "argmax and max Belief:  (5, 3) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((9, 13, 1), (6, 3))\n",
      "Action:  0\n",
      "Reward:     -14.5  \n",
      "Next State:  ((9, 14, 1), (6, 3))\n",
      "argmax and max Belief:  (5, 3) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((9, 14, 1), (6, 3))\n",
      "Action:  0\n",
      "Reward:     -15.0  \n",
      "Next State:  ((9, 15, 1), (6, 3))\n",
      "argmax and max Belief:  (6, 3) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((9, 15, 1), (6, 3))\n",
      "Action:  0\n",
      "Reward:     -15.5  \n",
      "Next State:  ((9, 16, 1), (6, 3))\n",
      "argmax and max Belief:  (6, 3) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((9, 16, 1), (6, 3))\n",
      "Action:  0\n",
      "Reward:     -16.0  \n",
      "Next State:  ((9, 17, 1), (6, 3))\n",
      "argmax and max Belief:  (6, 3) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [00:09<00:01,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((9, 17, 1), (6, 3))\n",
      "Action:  0\n",
      "Reward:     -15.0  \n",
      "Next State:  ((9, 18, 1), (6, 3))\n",
      "argmax and max Belief:  (6, 3) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((8, 21, 1), (7, 9))\n",
      "Action:  3\n",
      "Reward:     -0.5  \n",
      "Next State:  ((7, 21, 0), (7, 9))\n",
      "argmax and max Belief:  (7, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((7, 21, 0), (7, 9))\n",
      "Action:  2\n",
      "Reward:     -1.0  \n",
      "Next State:  ((8, 21, 2), (7, 9))\n",
      "argmax and max Belief:  (7, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((8, 21, 2), (7, 9))\n",
      "Action:  0\n",
      "Reward:     -1.5  \n",
      "Next State:  ((9, 21, 2), (7, 9))\n",
      "argmax and max Belief:  (7, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((9, 21, 2), (7, 9))\n",
      "Action:  0\n",
      "Reward:     -2.0  \n",
      "Next State:  ((10, 21, 2), (7, 9))\n",
      "argmax and max Belief:  (7, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((10, 21, 2), (7, 9))\n",
      "Action:  0\n",
      "Reward:     -2.5  \n",
      "Next State:  ((11, 21, 2), (7, 9))\n",
      "argmax and max Belief:  (7, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((11, 21, 2), (7, 9))\n",
      "Action:  0\n",
      "Reward:     -3.0  \n",
      "Next State:  ((12, 21, 2), (7, 9))\n",
      "argmax and max Belief:  (7, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((12, 21, 2), (7, 9))\n",
      "Action:  0\n",
      "Reward:     -3.5  \n",
      "Next State:  ((13, 21, 2), (7, 9))\n",
      "argmax and max Belief:  (7, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((13, 21, 2), (7, 9))\n",
      "Action:  0\n",
      "Reward:     -4.0  \n",
      "Next State:  ((14, 21, 2), (7, 9))\n",
      "argmax and max Belief:  (7, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((14, 21, 2), (7, 9))\n",
      "Action:  0\n",
      "Reward:     -4.5  \n",
      "Next State:  ((15, 21, 2), (7, 9))\n",
      "argmax and max Belief:  (7, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((15, 21, 2), (7, 9))\n",
      "Action:  0\n",
      "Reward:     -5.0  \n",
      "Next State:  ((16, 21, 2), (7, 9))\n",
      "argmax and max Belief:  (7, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((16, 21, 2), (7, 9))\n",
      "Action:  0\n",
      "Reward:     -5.5  \n",
      "Next State:  ((17, 21, 2), (7, 9))\n",
      "argmax and max Belief:  (7, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((17, 21, 2), (7, 9))\n",
      "Action:  0\n",
      "Reward:     -6.0  \n",
      "Next State:  ((18, 21, 2), (7, 9))\n",
      "argmax and max Belief:  (7, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((18, 21, 2), (7, 9))\n",
      "Action:  0\n",
      "Reward:     -6.5  \n",
      "Next State:  ((19, 21, 2), (7, 9))\n",
      "argmax and max Belief:  (7, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((19, 21, 2), (7, 9))\n",
      "Action:  0\n",
      "Reward:     -7.0  \n",
      "Next State:  ((20, 21, 2), (7, 9))\n",
      "argmax and max Belief:  (7, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((20, 21, 2), (7, 9))\n",
      "Action:  0\n",
      "Reward:     -7.5  \n",
      "Next State:  ((21, 21, 2), (7, 9))\n",
      "argmax and max Belief:  (7, 8) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n",
      "State ((21, 21, 2), (7, 9))\n",
      "Action:  0\n",
      "Reward:     -8.0  \n",
      "Next State:  ((22, 21, 2), (7, 9))\n",
      "argmax and max Belief:  (7, 8) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n",
      "State ((22, 21, 2), (7, 9))\n",
      "Action:  0\n",
      "Reward:     -8.5  \n",
      "Next State:  ((23, 21, 2), (7, 9))\n",
      "argmax and max Belief:  (7, 8) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [00:09<00:00,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((23, 21, 2), (7, 9))\n",
      "Action:  0\n",
      "Reward:     -9.0  \n",
      "Next State:  ((24, 21, 2), (7, 9))\n",
      "argmax and max Belief:  (7, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((24, 21, 2), (7, 9))\n",
      "Action:  0\n",
      "Reward:     -9.5  \n",
      "Next State:  ((25, 21, 2), (7, 9))\n",
      "argmax and max Belief:  (7, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((25, 21, 2), (7, 9))\n",
      "Action:  0\n",
      "Reward:     -10.0  \n",
      "Next State:  ((26, 21, 2), (7, 9))\n",
      "argmax and max Belief:  (7, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((26, 21, 2), (7, 9))\n",
      "Action:  0\n",
      "Reward:     -9.0  \n",
      "Next State:  ((27, 21, 2), (7, 9))\n",
      "argmax and max Belief:  (7, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((1, 13, 0), (3, 3))\n",
      "Action:  3\n",
      "Reward:     -2.0  \n",
      "Next State:  ((1, 12, 3), (3, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0159)\n",
      "Belief entropy:  tensor(4.1431)\n",
      "\n",
      "\n",
      "State ((1, 12, 3), (3, 3))\n",
      "Action:  3\n",
      "Reward:     -4.0  \n",
      "Next State:  ((2, 12, 2), (3, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0196)\n",
      "Belief entropy:  tensor(3.9318)\n",
      "\n",
      "\n",
      "State ((2, 12, 2), (3, 3))\n",
      "Action:  1\n",
      "Reward:     -6.0  \n",
      "Next State:  ((2, 11, 3), (3, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0196)\n",
      "Belief entropy:  tensor(3.9318)\n",
      "\n",
      "\n",
      "State ((2, 11, 3), (3, 3))\n",
      "Action:  0\n",
      "Reward:     -8.0  \n",
      "Next State:  ((2, 10, 3), (3, 3))\n",
      "argmax and max Belief:  (3, 1) tensor(0.0588)\n",
      "Belief entropy:  tensor(2.8332)\n",
      "\n",
      "\n",
      "State ((2, 10, 3), (3, 3))\n",
      "Action:  0\n",
      "Reward:     -8.5  \n",
      "Next State:  ((2, 9, 3), (3, 3))\n",
      "argmax and max Belief:  (3, 1) tensor(0.0588)\n",
      "Belief entropy:  tensor(2.8332)\n",
      "\n",
      "\n",
      "State ((2, 9, 3), (3, 3))\n",
      "Action:  3\n",
      "Reward:     -9.0  \n",
      "Next State:  ((3, 9, 2), (3, 3))\n",
      "argmax and max Belief:  (3, 2) tensor(0.0625)\n",
      "Belief entropy:  tensor(2.7726)\n",
      "\n",
      "\n",
      "State ((3, 9, 2), (3, 3))\n",
      "Action:  0\n",
      "Reward:     -9.5  \n",
      "Next State:  ((4, 9, 2), (3, 3))\n",
      "argmax and max Belief:  (3, 2) tensor(0.0667)\n",
      "Belief entropy:  tensor(2.7081)\n",
      "\n",
      "\n",
      "State ((4, 9, 2), (3, 3))\n",
      "Action:  0\n",
      "Reward:     -10.0  \n",
      "Next State:  ((5, 9, 2), (3, 3))\n",
      "argmax and max Belief:  (3, 2) tensor(0.0667)\n",
      "Belief entropy:  tensor(2.7081)\n",
      "\n",
      "\n",
      "State ((5, 9, 2), (3, 3))\n",
      "Action:  0\n",
      "Reward:     -10.5  \n",
      "Next State:  ((6, 9, 2), (3, 3))\n",
      "argmax and max Belief:  (3, 3) tensor(0.0769)\n",
      "Belief entropy:  tensor(2.5649)\n",
      "\n",
      "\n",
      "State ((6, 9, 2), (3, 3))\n",
      "Action:  0\n",
      "Reward:     -11.0  \n",
      "Next State:  ((7, 9, 2), (3, 3))\n",
      "argmax and max Belief:  (3, 3) tensor(0.0769)\n",
      "Belief entropy:  tensor(2.5649)\n",
      "\n",
      "\n",
      "State ((7, 9, 2), (3, 3))\n",
      "Action:  1\n",
      "Reward:     -11.5  \n",
      "Next State:  ((7, 8, 3), (3, 3))\n",
      "argmax and max Belief:  (3, 3) tensor(0.0769)\n",
      "Belief entropy:  tensor(2.5649)\n",
      "\n",
      "\n",
      "State ((7, 8, 3), (3, 3))\n",
      "Action:  2\n",
      "Reward:     -12.0  \n",
      "Next State:  ((7, 9, 1), (3, 3))\n",
      "argmax and max Belief:  (3, 3) tensor(0.0769)\n",
      "Belief entropy:  tensor(2.5649)\n",
      "\n",
      "\n",
      "State ((7, 9, 1), (3, 3))\n",
      "Action:  2\n",
      "Reward:     -12.5  \n",
      "Next State:  ((7, 8, 3), (3, 3))\n",
      "argmax and max Belief:  (3, 3) tensor(0.0769)\n",
      "Belief entropy:  tensor(2.5649)\n",
      "\n",
      "\n",
      "State ((7, 8, 3), (3, 3))\n",
      "Action:  3\n",
      "Reward:     -13.0  \n",
      "Next State:  ((8, 8, 2), (3, 3))\n",
      "argmax and max Belief:  (3, 3) tensor(0.0769)\n",
      "Belief entropy:  tensor(2.5649)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [00:10<00:00,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((8, 8, 2), (3, 3))\n",
      "Action:  0\n",
      "Reward:     -13.5  \n",
      "Next State:  ((9, 8, 2), (3, 3))\n",
      "argmax and max Belief:  (3, 3) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((9, 8, 2), (3, 3))\n",
      "Action:  3\n",
      "Reward:     -12.5  \n",
      "Next State:  ((9, 9, 1), (3, 3))\n",
      "argmax and max Belief:  (3, 3) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((7, 19, 0), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -0.5  \n",
      "Next State:  ((6, 19, 0), (9, 9))\n",
      "argmax and max Belief:  (9, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((6, 19, 0), (9, 9))\n",
      "Action:  1\n",
      "Reward:     -1.0  \n",
      "Next State:  ((6, 20, 1), (9, 9))\n",
      "argmax and max Belief:  (9, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((6, 20, 1), (9, 9))\n",
      "Action:  2\n",
      "Reward:     -1.5  \n",
      "Next State:  ((6, 19, 3), (9, 9))\n",
      "argmax and max Belief:  (9, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((6, 19, 3), (9, 9))\n",
      "Action:  2\n",
      "Reward:     -2.0  \n",
      "Next State:  ((6, 20, 1), (9, 9))\n",
      "argmax and max Belief:  (9, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((6, 20, 1), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -2.5  \n",
      "Next State:  ((6, 21, 1), (9, 9))\n",
      "argmax and max Belief:  (9, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((6, 21, 1), (9, 9))\n",
      "Action:  2\n",
      "Reward:     -3.0  \n",
      "Next State:  ((6, 20, 3), (9, 9))\n",
      "argmax and max Belief:  (9, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((6, 20, 3), (9, 9))\n",
      "Action:  3\n",
      "Reward:     -3.5  \n",
      "Next State:  ((7, 20, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((7, 20, 2), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -4.0  \n",
      "Next State:  ((8, 20, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((8, 20, 2), (9, 9))\n",
      "Action:  2\n",
      "Reward:     -4.5  \n",
      "Next State:  ((7, 20, 0), (9, 9))\n",
      "argmax and max Belief:  (9, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((7, 20, 0), (9, 9))\n",
      "Action:  1\n",
      "Reward:     -5.0  \n",
      "Next State:  ((7, 21, 1), (9, 9))\n",
      "argmax and max Belief:  (9, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((7, 21, 1), (9, 9))\n",
      "Action:  1\n",
      "Reward:     -5.5  \n",
      "Next State:  ((8, 21, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((8, 21, 2), (9, 9))\n",
      "Action:  3\n",
      "Reward:     -6.0  \n",
      "Next State:  ((8, 22, 1), (9, 9))\n",
      "argmax and max Belief:  (9, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((8, 22, 1), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -6.5  \n",
      "Next State:  ((8, 23, 1), (9, 9))\n",
      "argmax and max Belief:  (9, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((8, 23, 1), (9, 9))\n",
      "Action:  1\n",
      "Reward:     -7.0  \n",
      "Next State:  ((9, 23, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((9, 23, 2), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -7.5  \n",
      "Next State:  ((10, 23, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((10, 23, 2), (9, 9))\n",
      "Action:  1\n",
      "Reward:     -8.0  \n",
      "Next State:  ((10, 22, 3), (9, 9))\n",
      "argmax and max Belief:  (9, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((10, 22, 3), (9, 9))\n",
      "Action:  2\n",
      "Reward:     -8.5  \n",
      "Next State:  ((10, 23, 1), (9, 9))\n",
      "argmax and max Belief:  (9, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((10, 23, 1), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -9.0  \n",
      "Next State:  ((10, 24, 1), (9, 9))\n",
      "argmax and max Belief:  (9, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((10, 24, 1), (9, 9))\n",
      "Action:  1\n",
      "Reward:     -9.5  \n",
      "Next State:  ((11, 24, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((11, 24, 2), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -10.0  \n",
      "Next State:  ((12, 24, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((12, 24, 2), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -10.5  \n",
      "Next State:  ((13, 24, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((13, 24, 2), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -11.0  \n",
      "Next State:  ((14, 24, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((14, 24, 2), (9, 9))\n",
      "Action:  3\n",
      "Reward:     -11.5  \n",
      "Next State:  ((14, 25, 1), (9, 9))\n",
      "argmax and max Belief:  (9, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((14, 25, 1), (9, 9))\n",
      "Action:  1\n",
      "Reward:     -12.0  \n",
      "Next State:  ((15, 25, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((15, 25, 2), (9, 9))\n",
      "Action:  3\n",
      "Reward:     -12.5  \n",
      "Next State:  ((15, 26, 1), (9, 9))\n",
      "argmax and max Belief:  (9, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((15, 26, 1), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -13.0  \n",
      "Next State:  ((15, 27, 1), (9, 9))\n",
      "argmax and max Belief:  (9, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((15, 27, 1), (9, 9))\n",
      "Action:  1\n",
      "Reward:     -13.5  \n",
      "Next State:  ((16, 27, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((16, 27, 2), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -14.0  \n",
      "Next State:  ((17, 27, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((17, 27, 2), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -14.5  \n",
      "Next State:  ((18, 27, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((18, 27, 2), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -15.0  \n",
      "Next State:  ((19, 27, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((19, 27, 2), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -15.5  \n",
      "Next State:  ((20, 27, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((20, 27, 2), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -16.0  \n",
      "Next State:  ((21, 27, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 8) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n",
      "State ((21, 27, 2), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -16.5  \n",
      "Next State:  ((22, 27, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 8) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:11<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((22, 27, 2), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -17.0  \n",
      "Next State:  ((23, 27, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 8) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n",
      "State ((23, 27, 2), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -17.5  \n",
      "Next State:  ((24, 27, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((24, 27, 2), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -18.0  \n",
      "Next State:  ((25, 27, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((25, 27, 2), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -18.5  \n",
      "Next State:  ((26, 27, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((26, 27, 2), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -17.5  \n",
      "Next State:  ((27, 27, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transitions, beliefs = eval_agent(\"POMDP\",tagent ,env, num_episodes=20,max_episode_steps=50,render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved as gif\n"
     ]
    }
   ],
   "source": [
    "env.save_gif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed episodes: 15, out of 20\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_data(transitions, beliefs, path=\"plots/Thompson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.set_rendering()\n",
    "while True:\n",
    "    try:\n",
    "        env.render_bis()\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent = DoubleDQNAgent(5, 4, lr = 0.01, batch_size=64,target_update_freq=100, wandb=False)\n",
    "agent.load(\"agents/pretrained/MDP/double_dqn_45000.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_agent(\"MDP\",agent,env, num_episodes=20,max_episode_steps=100,render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QMDP\n",
    "eval_agent(\"POMDP\",agent,env, num_episodes=20,max_episode_steps=100,render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepThompson_agent():\n",
    "    def __init__(self, Q,env):\n",
    "        self.Q = Q\n",
    "        self.env = env\n",
    "\n",
    "    def get_entropy(self, belief):\n",
    "        return -torch.sum(belief * torch.log(belief + 1e-10))\n",
    "\n",
    "    def get_action(self, belief,pos):\n",
    "        theta = self.env.deformations[torch.multinomial(belief, 1).item()]\n",
    "\n",
    "        s = (pos[0],pos[1],pos[2],theta[0],theta[1])\n",
    "\n",
    "        # Agent takes an action using a greedy policy (without exploration)\n",
    "        action = self.Q.choose_deterministic_action(s)\n",
    "        return action\n",
    "    \n",
    "    def update_belief(self, belief, pos, observation):\n",
    "        \"\"\"\"\n",
    "        perform update over theta\n",
    "        \n",
    "        $$b'_{x,a,o}(theta) = \\eta \\cdot p(o|x,theta) \\cdot b(theta)$$\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        new_belief = torch.zeros_like(belief)\n",
    "\n",
    "        for t, theta in enumerate(self.env.deformations):\n",
    "            P_o_s_theta = np.all(self.env.get_observation(s = (pos,theta)) == observation) # 0 or 1 \n",
    "\n",
    "            new_belief[t] = P_o_s_theta * belief[t]\n",
    "        \n",
    "        new_belief = new_belief / (torch.sum(new_belief) + 1e-10)\n",
    "\n",
    "        return new_belief\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pomdp_agent = DeepThompson_agent(agent,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POMDP\n",
      "eval_agent_pomdp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((14, 20, 1), (3, 4))\n",
      "Action:  2\n",
      "Reward:     -2.0  \n",
      "Next State:  ((14, 19, 3), (3, 4))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0164)\n",
      "Belief entropy:  tensor(4.1109)\n",
      "\n",
      "\n",
      "State ((14, 19, 3), (3, 4))\n",
      "Action:  0\n",
      "Reward:     -4.0  \n",
      "Next State:  ((14, 18, 3), (3, 4))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0164)\n",
      "Belief entropy:  tensor(4.1109)\n",
      "\n",
      "\n",
      "State ((14, 18, 3), (3, 4))\n",
      "Action:  0\n",
      "Reward:     -6.0  \n",
      "Next State:  ((14, 17, 3), (3, 4))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0164)\n",
      "Belief entropy:  tensor(4.1109)\n",
      "\n",
      "\n",
      "State ((14, 17, 3), (3, 4))\n",
      "Action:  1\n",
      "Reward:     -8.0  \n",
      "Next State:  ((13, 17, 0), (3, 4))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0175)\n",
      "Belief entropy:  tensor(4.0431)\n",
      "\n",
      "\n",
      "State ((13, 17, 0), (3, 4))\n",
      "Action:  3\n",
      "Reward:     -10.0  \n",
      "Next State:  ((13, 16, 3), (3, 4))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0196)\n",
      "Belief entropy:  tensor(3.9318)\n",
      "\n",
      "\n",
      "State ((13, 16, 3), (3, 4))\n",
      "Action:  0\n",
      "Reward:     -12.0  \n",
      "Next State:  ((13, 15, 3), (3, 4))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0196)\n",
      "Belief entropy:  tensor(3.9318)\n",
      "\n",
      "\n",
      "State ((13, 15, 3), (3, 4))\n",
      "Action:  0\n",
      "Reward:     -14.0  \n",
      "Next State:  ((13, 14, 3), (3, 4))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0196)\n",
      "Belief entropy:  tensor(3.9318)\n",
      "\n",
      "\n",
      "State ((13, 14, 3), (3, 4))\n",
      "Action:  0\n",
      "Reward:     -16.0  \n",
      "Next State:  ((13, 13, 3), (3, 4))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0222)\n",
      "Belief entropy:  tensor(3.8067)\n",
      "\n",
      "\n",
      "State ((13, 13, 3), (3, 4))\n",
      "Action:  0\n",
      "Reward:     -18.0  \n",
      "Next State:  ((13, 12, 3), (3, 4))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0222)\n",
      "Belief entropy:  tensor(3.8067)\n",
      "\n",
      "\n",
      "State ((13, 12, 3), (3, 4))\n",
      "Action:  1\n",
      "Reward:     -20.0  \n",
      "Next State:  ((12, 12, 0), (3, 4))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0222)\n",
      "Belief entropy:  tensor(3.8067)\n",
      "\n",
      "\n",
      "State ((12, 12, 0), (3, 4))\n",
      "Action:  3\n",
      "Reward:     -22.0  \n",
      "Next State:  ((12, 11, 3), (3, 4))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0222)\n",
      "Belief entropy:  tensor(3.8067)\n",
      "\n",
      "\n",
      "State ((12, 11, 3), (3, 4))\n",
      "Action:  1\n",
      "Reward:     -24.0  \n",
      "Next State:  ((11, 11, 0), (3, 4))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0222)\n",
      "Belief entropy:  tensor(3.8067)\n",
      "\n",
      "\n",
      "State ((11, 11, 0), (3, 4))\n",
      "Action:  3\n",
      "Reward:     -26.0  \n",
      "Next State:  ((11, 10, 3), (3, 4))\n",
      "argmax and max Belief:  (3, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((11, 10, 3), (3, 4))\n",
      "Action:  0\n",
      "Reward:     -26.5  \n",
      "Next State:  ((11, 9, 3), (3, 4))\n",
      "argmax and max Belief:  (3, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:04,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((11, 9, 3), (3, 4))\n",
      "Action:  3\n",
      "Reward:     -25.5  \n",
      "Next State:  ((12, 9, 2), (3, 4))\n",
      "argmax and max Belief:  (3, 4) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((6, 15, 3), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -2.0  \n",
      "Next State:  ((6, 14, 3), (7, 3))\n",
      "argmax and max Belief:  (7, 3) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((6, 14, 3), (7, 3))\n",
      "Action:  3\n",
      "Reward:     -2.5  \n",
      "Next State:  ((7, 14, 2), (7, 3))\n",
      "argmax and max Belief:  (7, 3) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((7, 14, 2), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -3.0  \n",
      "Next State:  ((8, 14, 2), (7, 3))\n",
      "argmax and max Belief:  (7, 3) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((8, 14, 2), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -3.5  \n",
      "Next State:  ((9, 14, 2), (7, 3))\n",
      "argmax and max Belief:  (7, 3) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((9, 14, 2), (7, 3))\n",
      "Action:  3\n",
      "Reward:     -4.0  \n",
      "Next State:  ((9, 15, 1), (7, 3))\n",
      "argmax and max Belief:  (7, 3) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((9, 15, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -4.5  \n",
      "Next State:  ((9, 16, 1), (7, 3))\n",
      "argmax and max Belief:  (7, 3) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:00<00:03,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((9, 16, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -5.0  \n",
      "Next State:  ((9, 17, 1), (7, 3))\n",
      "argmax and max Belief:  (7, 3) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((9, 17, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -5.5  \n",
      "Next State:  ((9, 18, 1), (7, 3))\n",
      "argmax and max Belief:  (7, 3) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((9, 18, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -6.0  \n",
      "Next State:  ((9, 19, 1), (7, 3))\n",
      "argmax and max Belief:  (7, 3) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((9, 19, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -6.5  \n",
      "Next State:  ((9, 20, 1), (7, 3))\n",
      "argmax and max Belief:  (7, 3) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((9, 20, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -5.5  \n",
      "Next State:  ((9, 21, 1), (7, 3))\n",
      "argmax and max Belief:  (7, 3) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((20, 7, 1), (1, 3))\n",
      "Action:  3\n",
      "Reward:     -2.0  \n",
      "Next State:  ((19, 7, 0), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((19, 7, 0), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -4.0  \n",
      "Next State:  ((18, 7, 0), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 7, 0), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -6.0  \n",
      "Next State:  ((17, 7, 0), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((17, 7, 0), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -8.0  \n",
      "Next State:  ((16, 7, 0), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0244)\n",
      "Belief entropy:  tensor(3.7136)\n",
      "\n",
      "\n",
      "State ((16, 7, 0), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -10.0  \n",
      "Next State:  ((15, 7, 0), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0244)\n",
      "Belief entropy:  tensor(3.7136)\n",
      "\n",
      "\n",
      "State ((15, 7, 0), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -12.0  \n",
      "Next State:  ((14, 7, 0), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0244)\n",
      "Belief entropy:  tensor(3.7136)\n",
      "\n",
      "\n",
      "State ((14, 7, 0), (1, 3))\n",
      "Action:  3\n",
      "Reward:     -14.0  \n",
      "Next State:  ((14, 6, 3), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0244)\n",
      "Belief entropy:  tensor(3.7136)\n",
      "\n",
      "\n",
      "State ((14, 6, 3), (1, 3))\n",
      "Action:  1\n",
      "Reward:     -16.0  \n",
      "Next State:  ((13, 6, 0), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0303)\n",
      "Belief entropy:  tensor(3.4965)\n",
      "\n",
      "\n",
      "State ((13, 6, 0), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -18.0  \n",
      "Next State:  ((12, 6, 0), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0303)\n",
      "Belief entropy:  tensor(3.4965)\n",
      "\n",
      "\n",
      "State ((12, 6, 0), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -20.0  \n",
      "Next State:  ((11, 6, 0), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0303)\n",
      "Belief entropy:  tensor(3.4965)\n",
      "\n",
      "\n",
      "State ((11, 6, 0), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -22.0  \n",
      "Next State:  ((10, 6, 0), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0400)\n",
      "Belief entropy:  tensor(3.2189)\n",
      "\n",
      "\n",
      "State ((10, 6, 0), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -24.0  \n",
      "Next State:  ((9, 6, 0), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0400)\n",
      "Belief entropy:  tensor(3.2189)\n",
      "\n",
      "\n",
      "State ((9, 6, 0), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -26.0  \n",
      "Next State:  ((8, 6, 0), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0400)\n",
      "Belief entropy:  tensor(3.2189)\n",
      "\n",
      "\n",
      "State ((8, 6, 0), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -28.0  \n",
      "Next State:  ((7, 6, 0), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0588)\n",
      "Belief entropy:  tensor(2.8332)\n",
      "\n",
      "\n",
      "State ((7, 6, 0), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -30.0  \n",
      "Next State:  ((6, 6, 0), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0588)\n",
      "Belief entropy:  tensor(2.8332)\n",
      "\n",
      "\n",
      "State ((6, 6, 0), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -32.0  \n",
      "Next State:  ((5, 6, 0), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0588)\n",
      "Belief entropy:  tensor(2.8332)\n",
      "\n",
      "\n",
      "State ((5, 6, 0), (1, 3))\n",
      "Action:  3\n",
      "Reward:     -34.0  \n",
      "Next State:  ((5, 5, 3), (1, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0588)\n",
      "Belief entropy:  tensor(2.8332)\n",
      "\n",
      "\n",
      "State ((5, 5, 3), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -36.0  \n",
      "Next State:  ((5, 4, 3), (1, 3))\n",
      "argmax and max Belief:  (1, 2) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((5, 4, 3), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -36.5  \n",
      "Next State:  ((5, 3, 3), (1, 3))\n",
      "argmax and max Belief:  (1, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((5, 3, 3), (1, 3))\n",
      "Action:  3\n",
      "Reward:     -37.0  \n",
      "Next State:  ((6, 3, 2), (1, 3))\n",
      "argmax and max Belief:  (1, 3) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((6, 3, 2), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -37.5  \n",
      "Next State:  ((7, 3, 2), (1, 3))\n",
      "argmax and max Belief:  (1, 3) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((7, 3, 2), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -38.0  \n",
      "Next State:  ((8, 3, 2), (1, 3))\n",
      "argmax and max Belief:  (1, 3) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:01<00:03,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((8, 3, 2), (1, 3))\n",
      "Action:  0\n",
      "Reward:     -37.0  \n",
      "Next State:  ((9, 3, 2), (1, 3))\n",
      "argmax and max Belief:  (1, 3) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((20, 22, 2), (9, 9))\n",
      "Action:  1\n",
      "Reward:     -0.5  \n",
      "Next State:  ((20, 21, 3), (9, 9))\n",
      "argmax and max Belief:  (7, 7) tensor(0.1111)\n",
      "Belief entropy:  tensor(2.1972)\n",
      "\n",
      "\n",
      "State ((20, 21, 3), (9, 9))\n",
      "Action:  2\n",
      "Reward:     -1.0  \n",
      "Next State:  ((20, 22, 1), (9, 9))\n",
      "argmax and max Belief:  (8, 7) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((20, 22, 1), (9, 9))\n",
      "Action:  1\n",
      "Reward:     -1.5  \n",
      "Next State:  ((21, 22, 2), (9, 9))\n",
      "argmax and max Belief:  (8, 8) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((21, 22, 2), (9, 9))\n",
      "Action:  3\n",
      "Reward:     -2.0  \n",
      "Next State:  ((21, 23, 1), (9, 9))\n",
      "argmax and max Belief:  (8, 8) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((21, 23, 1), (9, 9))\n",
      "Action:  1\n",
      "Reward:     -2.5  \n",
      "Next State:  ((22, 23, 2), (9, 9))\n",
      "argmax and max Belief:  (8, 8) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((22, 23, 2), (9, 9))\n",
      "Action:  3\n",
      "Reward:     -3.0  \n",
      "Next State:  ((22, 24, 1), (9, 9))\n",
      "argmax and max Belief:  (9, 8) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n",
      "State ((22, 24, 1), (9, 9))\n",
      "Action:  1\n",
      "Reward:     -3.5  \n",
      "Next State:  ((23, 24, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 8) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n",
      "State ((23, 24, 2), (9, 9))\n",
      "Action:  3\n",
      "Reward:     -4.0  \n",
      "Next State:  ((23, 25, 1), (9, 9))\n",
      "argmax and max Belief:  (9, 8) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n",
      "State ((23, 25, 1), (9, 9))\n",
      "Action:  1\n",
      "Reward:     -4.5  \n",
      "Next State:  ((24, 25, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((24, 25, 2), (9, 9))\n",
      "Action:  3\n",
      "Reward:     -5.0  \n",
      "Next State:  ((24, 26, 1), (9, 9))\n",
      "argmax and max Belief:  (9, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((24, 26, 1), (9, 9))\n",
      "Action:  1\n",
      "Reward:     -5.5  \n",
      "Next State:  ((25, 26, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((25, 26, 2), (9, 9))\n",
      "Action:  3\n",
      "Reward:     -6.0  \n",
      "Next State:  ((25, 27, 1), (9, 9))\n",
      "argmax and max Belief:  (9, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((25, 27, 1), (9, 9))\n",
      "Action:  1\n",
      "Reward:     -6.5  \n",
      "Next State:  ((26, 27, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((26, 27, 2), (9, 9))\n",
      "Action:  0\n",
      "Reward:     -5.5  \n",
      "Next State:  ((27, 27, 2), (9, 9))\n",
      "argmax and max Belief:  (9, 9) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:01<00:02,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((16, 19, 0), (6, 4))\n",
      "Action:  0\n",
      "Reward:     -2.0  \n",
      "Next State:  ((15, 19, 0), (6, 4))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0164)\n",
      "Belief entropy:  tensor(4.1109)\n",
      "\n",
      "\n",
      "State ((15, 19, 0), (6, 4))\n",
      "Action:  3\n",
      "Reward:     -4.0  \n",
      "Next State:  ((15, 18, 3), (6, 4))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0164)\n",
      "Belief entropy:  tensor(4.1109)\n",
      "\n",
      "\n",
      "State ((15, 18, 3), (6, 4))\n",
      "Action:  1\n",
      "Reward:     -6.0  \n",
      "Next State:  ((14, 18, 0), (6, 4))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0164)\n",
      "Belief entropy:  tensor(4.1109)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:02<00:01,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((14, 18, 0), (6, 4))\n",
      "Action:  3\n",
      "Reward:     -8.0  \n",
      "Next State:  ((14, 17, 3), (6, 4))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0164)\n",
      "Belief entropy:  tensor(4.1109)\n",
      "\n",
      "\n",
      "State ((14, 17, 3), (6, 4))\n",
      "Action:  1\n",
      "Reward:     -10.0  \n",
      "Next State:  ((13, 17, 0), (6, 4))\n",
      "argmax and max Belief:  (6, 4) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((13, 17, 0), (6, 4))\n",
      "Action:  0\n",
      "Reward:     -10.5  \n",
      "Next State:  ((12, 17, 0), (6, 4))\n",
      "argmax and max Belief:  (6, 4) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((12, 17, 0), (6, 4))\n",
      "Action:  1\n",
      "Reward:     -9.5  \n",
      "Next State:  ((12, 18, 1), (6, 4))\n",
      "argmax and max Belief:  (6, 4) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((14, 10, 3), (5, 8))\n",
      "Action:  1\n",
      "Reward:     -2.0  \n",
      "Next State:  ((13, 10, 0), (5, 8))\n",
      "argmax and max Belief:  (5, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((13, 10, 0), (5, 8))\n",
      "Action:  1\n",
      "Reward:     -2.5  \n",
      "Next State:  ((13, 11, 1), (5, 8))\n",
      "argmax and max Belief:  (5, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((13, 11, 1), (5, 8))\n",
      "Action:  0\n",
      "Reward:     -3.0  \n",
      "Next State:  ((13, 12, 1), (5, 8))\n",
      "argmax and max Belief:  (5, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((13, 12, 1), (5, 8))\n",
      "Action:  0\n",
      "Reward:     -3.5  \n",
      "Next State:  ((13, 13, 1), (5, 8))\n",
      "argmax and max Belief:  (5, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((13, 13, 1), (5, 8))\n",
      "Action:  0\n",
      "Reward:     -4.0  \n",
      "Next State:  ((13, 14, 1), (5, 8))\n",
      "argmax and max Belief:  (5, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((13, 14, 1), (5, 8))\n",
      "Action:  1\n",
      "Reward:     -4.5  \n",
      "Next State:  ((14, 14, 2), (5, 8))\n",
      "argmax and max Belief:  (5, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((14, 14, 2), (5, 8))\n",
      "Action:  3\n",
      "Reward:     -5.0  \n",
      "Next State:  ((14, 15, 1), (5, 8))\n",
      "argmax and max Belief:  (5, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((14, 15, 1), (5, 8))\n",
      "Action:  1\n",
      "Reward:     -5.5  \n",
      "Next State:  ((15, 15, 2), (5, 8))\n",
      "argmax and max Belief:  (5, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((15, 15, 2), (5, 8))\n",
      "Action:  0\n",
      "Reward:     -6.0  \n",
      "Next State:  ((16, 15, 2), (5, 8))\n",
      "argmax and max Belief:  (5, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((16, 15, 2), (5, 8))\n",
      "Action:  0\n",
      "Reward:     -6.5  \n",
      "Next State:  ((17, 15, 2), (5, 8))\n",
      "argmax and max Belief:  (5, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((17, 15, 2), (5, 8))\n",
      "Action:  0\n",
      "Reward:     -7.0  \n",
      "Next State:  ((18, 15, 2), (5, 8))\n",
      "argmax and max Belief:  (5, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((18, 15, 2), (5, 8))\n",
      "Action:  0\n",
      "Reward:     -7.5  \n",
      "Next State:  ((19, 15, 2), (5, 8))\n",
      "argmax and max Belief:  (5, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:02<00:01,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((19, 15, 2), (5, 8))\n",
      "Action:  0\n",
      "Reward:     -8.0  \n",
      "Next State:  ((20, 15, 2), (5, 8))\n",
      "argmax and max Belief:  (5, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((20, 15, 2), (5, 8))\n",
      "Action:  0\n",
      "Reward:     -8.5  \n",
      "Next State:  ((21, 15, 2), (5, 8))\n",
      "argmax and max Belief:  (5, 8) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n",
      "State ((21, 15, 2), (5, 8))\n",
      "Action:  0\n",
      "Reward:     -9.0  \n",
      "Next State:  ((22, 15, 2), (5, 8))\n",
      "argmax and max Belief:  (5, 8) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (5, 8))\n",
      "Action:  0\n",
      "Reward:     -9.5  \n",
      "Next State:  ((23, 15, 2), (5, 8))\n",
      "argmax and max Belief:  (5, 8) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n",
      "State ((23, 15, 2), (5, 8))\n",
      "Action:  0\n",
      "Reward:     -8.5  \n",
      "Next State:  ((24, 15, 2), (5, 8))\n",
      "argmax and max Belief:  (5, 8) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((14, 9, 1), (5, 2))\n",
      "Action:  3\n",
      "Reward:     -2.0  \n",
      "Next State:  ((13, 9, 0), (5, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0208)\n",
      "Belief entropy:  tensor(3.8712)\n",
      "\n",
      "\n",
      "State ((13, 9, 0), (5, 2))\n",
      "Action:  0\n",
      "Reward:     -4.0  \n",
      "Next State:  ((12, 9, 0), (5, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0208)\n",
      "Belief entropy:  tensor(3.8712)\n",
      "\n",
      "\n",
      "State ((12, 9, 0), (5, 2))\n",
      "Action:  0\n",
      "Reward:     -6.0  \n",
      "Next State:  ((11, 9, 0), (5, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0208)\n",
      "Belief entropy:  tensor(3.8712)\n",
      "\n",
      "\n",
      "State ((11, 9, 0), (5, 2))\n",
      "Action:  0\n",
      "Reward:     -8.0  \n",
      "Next State:  ((10, 9, 0), (5, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0244)\n",
      "Belief entropy:  tensor(3.7136)\n",
      "\n",
      "\n",
      "State ((10, 9, 0), (5, 2))\n",
      "Action:  0\n",
      "Reward:     -10.0  \n",
      "Next State:  ((9, 9, 0), (5, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0244)\n",
      "Belief entropy:  tensor(3.7136)\n",
      "\n",
      "\n",
      "State ((9, 9, 0), (5, 2))\n",
      "Action:  3\n",
      "Reward:     -12.0  \n",
      "Next State:  ((9, 8, 3), (5, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0263)\n",
      "Belief entropy:  tensor(3.6376)\n",
      "\n",
      "\n",
      "State ((9, 8, 3), (5, 2))\n",
      "Action:  1\n",
      "Reward:     -14.0  \n",
      "Next State:  ((8, 8, 0), (5, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0263)\n",
      "Belief entropy:  tensor(3.6376)\n",
      "\n",
      "\n",
      "State ((8, 8, 0), (5, 2))\n",
      "Action:  0\n",
      "Reward:     -16.0  \n",
      "Next State:  ((7, 8, 0), (5, 2))\n",
      "argmax and max Belief:  (3, 2) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((7, 8, 0), (5, 2))\n",
      "Action:  0\n",
      "Reward:     -16.5  \n",
      "Next State:  ((6, 8, 0), (5, 2))\n",
      "argmax and max Belief:  (3, 2) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((6, 8, 0), (5, 2))\n",
      "Action:  1\n",
      "Reward:     -17.0  \n",
      "Next State:  ((6, 9, 1), (5, 2))\n",
      "argmax and max Belief:  (4, 2) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((6, 9, 1), (5, 2))\n",
      "Action:  0\n",
      "Reward:     -17.5  \n",
      "Next State:  ((6, 10, 1), (5, 2))\n",
      "argmax and max Belief:  (4, 2) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((6, 10, 1), (5, 2))\n",
      "Action:  0\n",
      "Reward:     -18.0  \n",
      "Next State:  ((6, 11, 1), (5, 2))\n",
      "argmax and max Belief:  (4, 2) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((6, 11, 1), (5, 2))\n",
      "Action:  0\n",
      "Reward:     -18.5  \n",
      "Next State:  ((6, 12, 1), (5, 2))\n",
      "argmax and max Belief:  (5, 2) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((6, 12, 1), (5, 2))\n",
      "Action:  0\n",
      "Reward:     -19.0  \n",
      "Next State:  ((6, 13, 1), (5, 2))\n",
      "argmax and max Belief:  (5, 2) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((6, 13, 1), (5, 2))\n",
      "Action:  0\n",
      "Reward:     -19.5  \n",
      "Next State:  ((6, 14, 1), (5, 2))\n",
      "argmax and max Belief:  (5, 2) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((6, 14, 1), (5, 2))\n",
      "Action:  0\n",
      "Reward:     -18.5  \n",
      "Next State:  ((6, 15, 1), (5, 2))\n",
      "argmax and max Belief:  (5, 2) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:02<00:01,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((12, 26, 0), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -2.0  \n",
      "Next State:  ((12, 25, 3), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((12, 25, 3), (5, 7))\n",
      "Action:  1\n",
      "Reward:     -4.0  \n",
      "Next State:  ((11, 25, 0), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((11, 25, 0), (5, 7))\n",
      "Action:  0\n",
      "Reward:     -6.0  \n",
      "Next State:  ((10, 25, 0), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0149)\n",
      "Belief entropy:  tensor(4.2047)\n",
      "\n",
      "\n",
      "State ((10, 25, 0), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -8.0  \n",
      "Next State:  ((10, 24, 3), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0149)\n",
      "Belief entropy:  tensor(4.2047)\n",
      "\n",
      "\n",
      "State ((10, 24, 3), (5, 7))\n",
      "Action:  0\n",
      "Reward:     -10.0  \n",
      "Next State:  ((10, 23, 3), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0149)\n",
      "Belief entropy:  tensor(4.2047)\n",
      "\n",
      "\n",
      "State ((10, 23, 3), (5, 7))\n",
      "Action:  0\n",
      "Reward:     -12.0  \n",
      "Next State:  ((10, 22, 3), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 22, 3), (5, 7))\n",
      "Action:  0\n",
      "Reward:     -14.0  \n",
      "Next State:  ((10, 21, 3), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 21, 3), (5, 7))\n",
      "Action:  0\n",
      "Reward:     -16.0  \n",
      "Next State:  ((10, 20, 3), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 20, 3), (5, 7))\n",
      "Action:  1\n",
      "Reward:     -18.0  \n",
      "Next State:  ((9, 20, 0), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((9, 20, 0), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -20.0  \n",
      "Next State:  ((9, 19, 3), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0189)\n",
      "Belief entropy:  tensor(3.9703)\n",
      "\n",
      "\n",
      "State ((9, 19, 3), (5, 7))\n",
      "Action:  0\n",
      "Reward:     -22.0  \n",
      "Next State:  ((9, 18, 3), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0189)\n",
      "Belief entropy:  tensor(3.9703)\n",
      "\n",
      "\n",
      "State ((9, 18, 3), (5, 7))\n",
      "Action:  0\n",
      "Reward:     -24.0  \n",
      "Next State:  ((9, 17, 3), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0189)\n",
      "Belief entropy:  tensor(3.9703)\n",
      "\n",
      "\n",
      "State ((9, 17, 3), (5, 7))\n",
      "Action:  0\n",
      "Reward:     -26.0  \n",
      "Next State:  ((9, 16, 3), (5, 7))\n",
      "argmax and max Belief:  (5, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((9, 16, 3), (5, 7))\n",
      "Action:  0\n",
      "Reward:     -26.5  \n",
      "Next State:  ((9, 15, 3), (5, 7))\n",
      "argmax and max Belief:  (5, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((9, 15, 3), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -27.0  \n",
      "Next State:  ((10, 15, 2), (5, 7))\n",
      "argmax and max Belief:  (5, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((10, 15, 2), (5, 7))\n",
      "Action:  0\n",
      "Reward:     -27.5  \n",
      "Next State:  ((11, 15, 2), (5, 7))\n",
      "argmax and max Belief:  (5, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((11, 15, 2), (5, 7))\n",
      "Action:  0\n",
      "Reward:     -28.0  \n",
      "Next State:  ((12, 15, 2), (5, 7))\n",
      "argmax and max Belief:  (5, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((12, 15, 2), (5, 7))\n",
      "Action:  0\n",
      "Reward:     -28.5  \n",
      "Next State:  ((13, 15, 2), (5, 7))\n",
      "argmax and max Belief:  (5, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((13, 15, 2), (5, 7))\n",
      "Action:  0\n",
      "Reward:     -29.0  \n",
      "Next State:  ((14, 15, 2), (5, 7))\n",
      "argmax and max Belief:  (5, 5) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((14, 15, 2), (5, 7))\n",
      "Action:  0\n",
      "Reward:     -29.5  \n",
      "Next State:  ((15, 15, 2), (5, 7))\n",
      "argmax and max Belief:  (5, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((15, 15, 2), (5, 7))\n",
      "Action:  0\n",
      "Reward:     -30.0  \n",
      "Next State:  ((16, 15, 2), (5, 7))\n",
      "argmax and max Belief:  (5, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((16, 15, 2), (5, 7))\n",
      "Action:  0\n",
      "Reward:     -30.5  \n",
      "Next State:  ((17, 15, 2), (5, 7))\n",
      "argmax and max Belief:  (5, 6) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((17, 15, 2), (5, 7))\n",
      "Action:  0\n",
      "Reward:     -31.0  \n",
      "Next State:  ((18, 15, 2), (5, 7))\n",
      "argmax and max Belief:  (5, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:03<00:01,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((18, 15, 2), (5, 7))\n",
      "Action:  0\n",
      "Reward:     -31.5  \n",
      "Next State:  ((19, 15, 2), (5, 7))\n",
      "argmax and max Belief:  (5, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((19, 15, 2), (5, 7))\n",
      "Action:  0\n",
      "Reward:     -32.0  \n",
      "Next State:  ((20, 15, 2), (5, 7))\n",
      "argmax and max Belief:  (5, 7) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((20, 15, 2), (5, 7))\n",
      "Action:  0\n",
      "Reward:     -31.0  \n",
      "Next State:  ((21, 15, 2), (5, 7))\n",
      "argmax and max Belief:  (5, 7) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((17, 3, 3), (7, 3))\n",
      "Action:  1\n",
      "Reward:     -2.0  \n",
      "Next State:  ((16, 3, 0), (7, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0278)\n",
      "Belief entropy:  tensor(3.5835)\n",
      "\n",
      "\n",
      "State ((16, 3, 0), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -4.0  \n",
      "Next State:  ((15, 3, 0), (7, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0278)\n",
      "Belief entropy:  tensor(3.5835)\n",
      "\n",
      "\n",
      "State ((15, 3, 0), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -6.0  \n",
      "Next State:  ((14, 3, 0), (7, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0278)\n",
      "Belief entropy:  tensor(3.5835)\n",
      "\n",
      "\n",
      "State ((14, 3, 0), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -8.0  \n",
      "Next State:  ((13, 3, 0), (7, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0370)\n",
      "Belief entropy:  tensor(3.2958)\n",
      "\n",
      "\n",
      "State ((13, 3, 0), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -10.0  \n",
      "Next State:  ((12, 3, 0), (7, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0370)\n",
      "Belief entropy:  tensor(3.2958)\n",
      "\n",
      "\n",
      "State ((12, 3, 0), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -12.0  \n",
      "Next State:  ((11, 3, 0), (7, 3))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0370)\n",
      "Belief entropy:  tensor(3.2958)\n",
      "\n",
      "\n",
      "State ((11, 3, 0), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -14.0  \n",
      "Next State:  ((10, 3, 0), (7, 3))\n",
      "argmax and max Belief:  (2, 3) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((10, 3, 0), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -14.5  \n",
      "Next State:  ((9, 3, 0), (7, 3))\n",
      "argmax and max Belief:  (2, 3) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((9, 3, 0), (7, 3))\n",
      "Action:  1\n",
      "Reward:     -15.0  \n",
      "Next State:  ((9, 4, 1), (7, 3))\n",
      "argmax and max Belief:  (2, 3) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((9, 4, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -15.5  \n",
      "Next State:  ((9, 5, 1), (7, 3))\n",
      "argmax and max Belief:  (2, 3) tensor(0.1250)\n",
      "Belief entropy:  tensor(2.0794)\n",
      "\n",
      "\n",
      "State ((9, 5, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -16.0  \n",
      "Next State:  ((9, 6, 1), (7, 3))\n",
      "argmax and max Belief:  (3, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((9, 6, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -16.5  \n",
      "Next State:  ((9, 7, 1), (7, 3))\n",
      "argmax and max Belief:  (3, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((9, 7, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -17.0  \n",
      "Next State:  ((9, 8, 1), (7, 3))\n",
      "argmax and max Belief:  (3, 3) tensor(0.1429)\n",
      "Belief entropy:  tensor(1.9459)\n",
      "\n",
      "\n",
      "State ((9, 8, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -17.5  \n",
      "Next State:  ((9, 9, 1), (7, 3))\n",
      "argmax and max Belief:  (4, 3) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((9, 9, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -18.0  \n",
      "Next State:  ((9, 10, 1), (7, 3))\n",
      "argmax and max Belief:  (4, 3) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((9, 10, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -18.5  \n",
      "Next State:  ((9, 11, 1), (7, 3))\n",
      "argmax and max Belief:  (4, 3) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((9, 11, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -19.0  \n",
      "Next State:  ((9, 12, 1), (7, 3))\n",
      "argmax and max Belief:  (5, 3) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((9, 12, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -19.5  \n",
      "Next State:  ((9, 13, 1), (7, 3))\n",
      "argmax and max Belief:  (5, 3) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((9, 13, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -20.0  \n",
      "Next State:  ((9, 14, 1), (7, 3))\n",
      "argmax and max Belief:  (5, 3) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((9, 14, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -20.5  \n",
      "Next State:  ((9, 15, 1), (7, 3))\n",
      "argmax and max Belief:  (6, 3) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((9, 15, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -21.0  \n",
      "Next State:  ((9, 16, 1), (7, 3))\n",
      "argmax and max Belief:  (6, 3) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((9, 16, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -21.5  \n",
      "Next State:  ((9, 17, 1), (7, 3))\n",
      "argmax and max Belief:  (6, 3) tensor(0.2500)\n",
      "Belief entropy:  tensor(1.3863)\n",
      "\n",
      "\n",
      "State ((9, 17, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -22.0  \n",
      "Next State:  ((9, 18, 1), (7, 3))\n",
      "argmax and max Belief:  (7, 3) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((9, 18, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -22.5  \n",
      "Next State:  ((9, 19, 1), (7, 3))\n",
      "argmax and max Belief:  (7, 3) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((9, 19, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -23.0  \n",
      "Next State:  ((9, 20, 1), (7, 3))\n",
      "argmax and max Belief:  (7, 3) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:04<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((9, 20, 1), (7, 3))\n",
      "Action:  0\n",
      "Reward:     -22.0  \n",
      "Next State:  ((9, 21, 1), (7, 3))\n",
      "argmax and max Belief:  (7, 3) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n",
      "State ((9, 9, 3), (5, 4))\n",
      "Action:  0\n",
      "Reward:     -0.5  \n",
      "Next State:  ((9, 8, 3), (5, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((9, 8, 3), (5, 4))\n",
      "Action:  3\n",
      "Reward:     -1.0  \n",
      "Next State:  ((10, 8, 2), (5, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((10, 8, 2), (5, 4))\n",
      "Action:  0\n",
      "Reward:     -1.5  \n",
      "Next State:  ((11, 8, 2), (5, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((11, 8, 2), (5, 4))\n",
      "Action:  0\n",
      "Reward:     -2.0  \n",
      "Next State:  ((12, 8, 2), (5, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((12, 8, 2), (5, 4))\n",
      "Action:  3\n",
      "Reward:     -2.5  \n",
      "Next State:  ((12, 9, 1), (5, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((12, 9, 1), (5, 4))\n",
      "Action:  0\n",
      "Reward:     -3.0  \n",
      "Next State:  ((12, 10, 1), (5, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((12, 10, 1), (5, 4))\n",
      "Action:  0\n",
      "Reward:     -3.5  \n",
      "Next State:  ((12, 11, 1), (5, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.3333)\n",
      "Belief entropy:  tensor(1.0986)\n",
      "\n",
      "\n",
      "State ((12, 11, 1), (5, 4))\n",
      "Action:  0\n",
      "Reward:     -4.0  \n",
      "Next State:  ((12, 12, 1), (5, 4))\n",
      "argmax and max Belief:  (5, 4) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n",
      "State ((12, 12, 1), (5, 4))\n",
      "Action:  0\n",
      "Reward:     -4.5  \n",
      "Next State:  ((12, 13, 1), (5, 4))\n",
      "argmax and max Belief:  (5, 4) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n",
      "State ((12, 13, 1), (5, 4))\n",
      "Action:  0\n",
      "Reward:     -5.0  \n",
      "Next State:  ((12, 14, 1), (5, 4))\n",
      "argmax and max Belief:  (5, 4) tensor(0.5000)\n",
      "Belief entropy:  tensor(0.6931)\n",
      "\n",
      "\n",
      "State ((12, 14, 1), (5, 4))\n",
      "Action:  0\n",
      "Reward:     -4.0  \n",
      "Next State:  ((12, 15, 1), (5, 4))\n",
      "argmax and max Belief:  (5, 4) tensor(1.)\n",
      "Belief entropy:  tensor(-0.)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[(((14, 20, 1), (3, 4)), 2, -2, ((14, 19, 3), (3, 4)), np.False_),\n",
       "   (((14, 19, 3), (3, 4)), 0, -2, ((14, 18, 3), (3, 4)), np.False_),\n",
       "   (((14, 18, 3), (3, 4)), 0, -2, ((14, 17, 3), (3, 4)), np.False_),\n",
       "   (((14, 17, 3), (3, 4)), 1, -2, ((13, 17, 0), (3, 4)), np.False_),\n",
       "   (((13, 17, 0), (3, 4)), 3, -2, ((13, 16, 3), (3, 4)), np.False_),\n",
       "   (((13, 16, 3), (3, 4)), 0, -2, ((13, 15, 3), (3, 4)), np.False_),\n",
       "   (((13, 15, 3), (3, 4)), 0, -2, ((13, 14, 3), (3, 4)), np.False_),\n",
       "   (((13, 14, 3), (3, 4)), 0, -2, ((13, 13, 3), (3, 4)), np.False_),\n",
       "   (((13, 13, 3), (3, 4)), 0, -2, ((13, 12, 3), (3, 4)), np.False_),\n",
       "   (((13, 12, 3), (3, 4)), 1, -2, ((12, 12, 0), (3, 4)), np.False_),\n",
       "   (((12, 12, 0), (3, 4)), 3, -2, ((12, 11, 3), (3, 4)), np.False_),\n",
       "   (((12, 11, 3), (3, 4)), 1, -2, ((11, 11, 0), (3, 4)), np.False_),\n",
       "   (((11, 11, 0), (3, 4)), 3, -2, ((11, 10, 3), (3, 4)), np.False_),\n",
       "   (((11, 10, 3), (3, 4)), 0, -0.5, ((11, 9, 3), (3, 4)), np.False_),\n",
       "   (((11, 9, 3), (3, 4)), 3, 1, ((12, 9, 2), (3, 4)), np.True_)],\n",
       "  [(((6, 15, 3), (7, 3)), 0, -2, ((6, 14, 3), (7, 3)), np.False_),\n",
       "   (((6, 14, 3), (7, 3)), 3, -0.5, ((7, 14, 2), (7, 3)), np.False_),\n",
       "   (((7, 14, 2), (7, 3)), 0, -0.5, ((8, 14, 2), (7, 3)), np.False_),\n",
       "   (((8, 14, 2), (7, 3)), 0, -0.5, ((9, 14, 2), (7, 3)), np.False_),\n",
       "   (((9, 14, 2), (7, 3)), 3, -0.5, ((9, 15, 1), (7, 3)), np.False_),\n",
       "   (((9, 15, 1), (7, 3)), 0, -0.5, ((9, 16, 1), (7, 3)), np.False_),\n",
       "   (((9, 16, 1), (7, 3)), 0, -0.5, ((9, 17, 1), (7, 3)), np.False_),\n",
       "   (((9, 17, 1), (7, 3)), 0, -0.5, ((9, 18, 1), (7, 3)), np.False_),\n",
       "   (((9, 18, 1), (7, 3)), 0, -0.5, ((9, 19, 1), (7, 3)), np.False_),\n",
       "   (((9, 19, 1), (7, 3)), 0, -0.5, ((9, 20, 1), (7, 3)), np.False_),\n",
       "   (((9, 20, 1), (7, 3)), 0, 1, ((9, 21, 1), (7, 3)), np.True_)],\n",
       "  [(((20, 7, 1), (1, 3)), 3, -2, ((19, 7, 0), (1, 3)), np.False_),\n",
       "   (((19, 7, 0), (1, 3)), 0, -2, ((18, 7, 0), (1, 3)), np.False_),\n",
       "   (((18, 7, 0), (1, 3)), 0, -2, ((17, 7, 0), (1, 3)), np.False_),\n",
       "   (((17, 7, 0), (1, 3)), 0, -2, ((16, 7, 0), (1, 3)), np.False_),\n",
       "   (((16, 7, 0), (1, 3)), 0, -2, ((15, 7, 0), (1, 3)), np.False_),\n",
       "   (((15, 7, 0), (1, 3)), 0, -2, ((14, 7, 0), (1, 3)), np.False_),\n",
       "   (((14, 7, 0), (1, 3)), 3, -2, ((14, 6, 3), (1, 3)), np.False_),\n",
       "   (((14, 6, 3), (1, 3)), 1, -2, ((13, 6, 0), (1, 3)), np.False_),\n",
       "   (((13, 6, 0), (1, 3)), 0, -2, ((12, 6, 0), (1, 3)), np.False_),\n",
       "   (((12, 6, 0), (1, 3)), 0, -2, ((11, 6, 0), (1, 3)), np.False_),\n",
       "   (((11, 6, 0), (1, 3)), 0, -2, ((10, 6, 0), (1, 3)), np.False_),\n",
       "   (((10, 6, 0), (1, 3)), 0, -2, ((9, 6, 0), (1, 3)), np.False_),\n",
       "   (((9, 6, 0), (1, 3)), 0, -2, ((8, 6, 0), (1, 3)), np.False_),\n",
       "   (((8, 6, 0), (1, 3)), 0, -2, ((7, 6, 0), (1, 3)), np.False_),\n",
       "   (((7, 6, 0), (1, 3)), 0, -2, ((6, 6, 0), (1, 3)), np.False_),\n",
       "   (((6, 6, 0), (1, 3)), 0, -2, ((5, 6, 0), (1, 3)), np.False_),\n",
       "   (((5, 6, 0), (1, 3)), 3, -2, ((5, 5, 3), (1, 3)), np.False_),\n",
       "   (((5, 5, 3), (1, 3)), 0, -2, ((5, 4, 3), (1, 3)), np.False_),\n",
       "   (((5, 4, 3), (1, 3)), 0, -0.5, ((5, 3, 3), (1, 3)), np.False_),\n",
       "   (((5, 3, 3), (1, 3)), 3, -0.5, ((6, 3, 2), (1, 3)), np.False_),\n",
       "   (((6, 3, 2), (1, 3)), 0, -0.5, ((7, 3, 2), (1, 3)), np.False_),\n",
       "   (((7, 3, 2), (1, 3)), 0, -0.5, ((8, 3, 2), (1, 3)), np.False_),\n",
       "   (((8, 3, 2), (1, 3)), 0, 1, ((9, 3, 2), (1, 3)), np.True_)],\n",
       "  [(((20, 22, 2), (9, 9)), 1, -0.5, ((20, 21, 3), (9, 9)), np.False_),\n",
       "   (((20, 21, 3), (9, 9)), 2, -0.5, ((20, 22, 1), (9, 9)), np.False_),\n",
       "   (((20, 22, 1), (9, 9)), 1, -0.5, ((21, 22, 2), (9, 9)), np.False_),\n",
       "   (((21, 22, 2), (9, 9)), 3, -0.5, ((21, 23, 1), (9, 9)), np.False_),\n",
       "   (((21, 23, 1), (9, 9)), 1, -0.5, ((22, 23, 2), (9, 9)), np.False_),\n",
       "   (((22, 23, 2), (9, 9)), 3, -0.5, ((22, 24, 1), (9, 9)), np.False_),\n",
       "   (((22, 24, 1), (9, 9)), 1, -0.5, ((23, 24, 2), (9, 9)), np.False_),\n",
       "   (((23, 24, 2), (9, 9)), 3, -0.5, ((23, 25, 1), (9, 9)), np.False_),\n",
       "   (((23, 25, 1), (9, 9)), 1, -0.5, ((24, 25, 2), (9, 9)), np.False_),\n",
       "   (((24, 25, 2), (9, 9)), 3, -0.5, ((24, 26, 1), (9, 9)), np.False_),\n",
       "   (((24, 26, 1), (9, 9)), 1, -0.5, ((25, 26, 2), (9, 9)), np.False_),\n",
       "   (((25, 26, 2), (9, 9)), 3, -0.5, ((25, 27, 1), (9, 9)), np.False_),\n",
       "   (((25, 27, 1), (9, 9)), 1, -0.5, ((26, 27, 2), (9, 9)), np.False_),\n",
       "   (((26, 27, 2), (9, 9)), 0, 1, ((27, 27, 2), (9, 9)), np.True_)],\n",
       "  [(((16, 19, 0), (6, 4)), 0, -2, ((15, 19, 0), (6, 4)), np.False_),\n",
       "   (((15, 19, 0), (6, 4)), 3, -2, ((15, 18, 3), (6, 4)), np.False_),\n",
       "   (((15, 18, 3), (6, 4)), 1, -2, ((14, 18, 0), (6, 4)), np.False_),\n",
       "   (((14, 18, 0), (6, 4)), 3, -2, ((14, 17, 3), (6, 4)), np.False_),\n",
       "   (((14, 17, 3), (6, 4)), 1, -2, ((13, 17, 0), (6, 4)), np.False_),\n",
       "   (((13, 17, 0), (6, 4)), 0, -0.5, ((12, 17, 0), (6, 4)), np.False_),\n",
       "   (((12, 17, 0), (6, 4)), 1, 1, ((12, 18, 1), (6, 4)), np.True_)],\n",
       "  [(((14, 10, 3), (5, 8)), 1, -2, ((13, 10, 0), (5, 8)), np.False_),\n",
       "   (((13, 10, 0), (5, 8)), 1, -0.5, ((13, 11, 1), (5, 8)), np.False_),\n",
       "   (((13, 11, 1), (5, 8)), 0, -0.5, ((13, 12, 1), (5, 8)), np.False_),\n",
       "   (((13, 12, 1), (5, 8)), 0, -0.5, ((13, 13, 1), (5, 8)), np.False_),\n",
       "   (((13, 13, 1), (5, 8)), 0, -0.5, ((13, 14, 1), (5, 8)), np.False_),\n",
       "   (((13, 14, 1), (5, 8)), 1, -0.5, ((14, 14, 2), (5, 8)), np.False_),\n",
       "   (((14, 14, 2), (5, 8)), 3, -0.5, ((14, 15, 1), (5, 8)), np.False_),\n",
       "   (((14, 15, 1), (5, 8)), 1, -0.5, ((15, 15, 2), (5, 8)), np.False_),\n",
       "   (((15, 15, 2), (5, 8)), 0, -0.5, ((16, 15, 2), (5, 8)), np.False_),\n",
       "   (((16, 15, 2), (5, 8)), 0, -0.5, ((17, 15, 2), (5, 8)), np.False_),\n",
       "   (((17, 15, 2), (5, 8)), 0, -0.5, ((18, 15, 2), (5, 8)), np.False_),\n",
       "   (((18, 15, 2), (5, 8)), 0, -0.5, ((19, 15, 2), (5, 8)), np.False_),\n",
       "   (((19, 15, 2), (5, 8)), 0, -0.5, ((20, 15, 2), (5, 8)), np.False_),\n",
       "   (((20, 15, 2), (5, 8)), 0, -0.5, ((21, 15, 2), (5, 8)), np.False_),\n",
       "   (((21, 15, 2), (5, 8)), 0, -0.5, ((22, 15, 2), (5, 8)), np.False_),\n",
       "   (((22, 15, 2), (5, 8)), 0, -0.5, ((23, 15, 2), (5, 8)), np.False_),\n",
       "   (((23, 15, 2), (5, 8)), 0, 1, ((24, 15, 2), (5, 8)), np.True_)],\n",
       "  [(((14, 9, 1), (5, 2)), 3, -2, ((13, 9, 0), (5, 2)), np.False_),\n",
       "   (((13, 9, 0), (5, 2)), 0, -2, ((12, 9, 0), (5, 2)), np.False_),\n",
       "   (((12, 9, 0), (5, 2)), 0, -2, ((11, 9, 0), (5, 2)), np.False_),\n",
       "   (((11, 9, 0), (5, 2)), 0, -2, ((10, 9, 0), (5, 2)), np.False_),\n",
       "   (((10, 9, 0), (5, 2)), 0, -2, ((9, 9, 0), (5, 2)), np.False_),\n",
       "   (((9, 9, 0), (5, 2)), 3, -2, ((9, 8, 3), (5, 2)), np.False_),\n",
       "   (((9, 8, 3), (5, 2)), 1, -2, ((8, 8, 0), (5, 2)), np.False_),\n",
       "   (((8, 8, 0), (5, 2)), 0, -2, ((7, 8, 0), (5, 2)), np.False_),\n",
       "   (((7, 8, 0), (5, 2)), 0, -0.5, ((6, 8, 0), (5, 2)), np.False_),\n",
       "   (((6, 8, 0), (5, 2)), 1, -0.5, ((6, 9, 1), (5, 2)), np.False_),\n",
       "   (((6, 9, 1), (5, 2)), 0, -0.5, ((6, 10, 1), (5, 2)), np.False_),\n",
       "   (((6, 10, 1), (5, 2)), 0, -0.5, ((6, 11, 1), (5, 2)), np.False_),\n",
       "   (((6, 11, 1), (5, 2)), 0, -0.5, ((6, 12, 1), (5, 2)), np.False_),\n",
       "   (((6, 12, 1), (5, 2)), 0, -0.5, ((6, 13, 1), (5, 2)), np.False_),\n",
       "   (((6, 13, 1), (5, 2)), 0, -0.5, ((6, 14, 1), (5, 2)), np.False_),\n",
       "   (((6, 14, 1), (5, 2)), 0, 1, ((6, 15, 1), (5, 2)), np.True_)],\n",
       "  [(((12, 26, 0), (5, 7)), 3, -2, ((12, 25, 3), (5, 7)), np.False_),\n",
       "   (((12, 25, 3), (5, 7)), 1, -2, ((11, 25, 0), (5, 7)), np.False_),\n",
       "   (((11, 25, 0), (5, 7)), 0, -2, ((10, 25, 0), (5, 7)), np.False_),\n",
       "   (((10, 25, 0), (5, 7)), 3, -2, ((10, 24, 3), (5, 7)), np.False_),\n",
       "   (((10, 24, 3), (5, 7)), 0, -2, ((10, 23, 3), (5, 7)), np.False_),\n",
       "   (((10, 23, 3), (5, 7)), 0, -2, ((10, 22, 3), (5, 7)), np.False_),\n",
       "   (((10, 22, 3), (5, 7)), 0, -2, ((10, 21, 3), (5, 7)), np.False_),\n",
       "   (((10, 21, 3), (5, 7)), 0, -2, ((10, 20, 3), (5, 7)), np.False_),\n",
       "   (((10, 20, 3), (5, 7)), 1, -2, ((9, 20, 0), (5, 7)), np.False_),\n",
       "   (((9, 20, 0), (5, 7)), 3, -2, ((9, 19, 3), (5, 7)), np.False_),\n",
       "   (((9, 19, 3), (5, 7)), 0, -2, ((9, 18, 3), (5, 7)), np.False_),\n",
       "   (((9, 18, 3), (5, 7)), 0, -2, ((9, 17, 3), (5, 7)), np.False_),\n",
       "   (((9, 17, 3), (5, 7)), 0, -2, ((9, 16, 3), (5, 7)), np.False_),\n",
       "   (((9, 16, 3), (5, 7)), 0, -0.5, ((9, 15, 3), (5, 7)), np.False_),\n",
       "   (((9, 15, 3), (5, 7)), 3, -0.5, ((10, 15, 2), (5, 7)), np.False_),\n",
       "   (((10, 15, 2), (5, 7)), 0, -0.5, ((11, 15, 2), (5, 7)), np.False_),\n",
       "   (((11, 15, 2), (5, 7)), 0, -0.5, ((12, 15, 2), (5, 7)), np.False_),\n",
       "   (((12, 15, 2), (5, 7)), 0, -0.5, ((13, 15, 2), (5, 7)), np.False_),\n",
       "   (((13, 15, 2), (5, 7)), 0, -0.5, ((14, 15, 2), (5, 7)), np.False_),\n",
       "   (((14, 15, 2), (5, 7)), 0, -0.5, ((15, 15, 2), (5, 7)), np.False_),\n",
       "   (((15, 15, 2), (5, 7)), 0, -0.5, ((16, 15, 2), (5, 7)), np.False_),\n",
       "   (((16, 15, 2), (5, 7)), 0, -0.5, ((17, 15, 2), (5, 7)), np.False_),\n",
       "   (((17, 15, 2), (5, 7)), 0, -0.5, ((18, 15, 2), (5, 7)), np.False_),\n",
       "   (((18, 15, 2), (5, 7)), 0, -0.5, ((19, 15, 2), (5, 7)), np.False_),\n",
       "   (((19, 15, 2), (5, 7)), 0, -0.5, ((20, 15, 2), (5, 7)), np.False_),\n",
       "   (((20, 15, 2), (5, 7)), 0, 1, ((21, 15, 2), (5, 7)), np.True_)],\n",
       "  [(((17, 3, 3), (7, 3)), 1, -2, ((16, 3, 0), (7, 3)), np.False_),\n",
       "   (((16, 3, 0), (7, 3)), 0, -2, ((15, 3, 0), (7, 3)), np.False_),\n",
       "   (((15, 3, 0), (7, 3)), 0, -2, ((14, 3, 0), (7, 3)), np.False_),\n",
       "   (((14, 3, 0), (7, 3)), 0, -2, ((13, 3, 0), (7, 3)), np.False_),\n",
       "   (((13, 3, 0), (7, 3)), 0, -2, ((12, 3, 0), (7, 3)), np.False_),\n",
       "   (((12, 3, 0), (7, 3)), 0, -2, ((11, 3, 0), (7, 3)), np.False_),\n",
       "   (((11, 3, 0), (7, 3)), 0, -2, ((10, 3, 0), (7, 3)), np.False_),\n",
       "   (((10, 3, 0), (7, 3)), 0, -0.5, ((9, 3, 0), (7, 3)), np.False_),\n",
       "   (((9, 3, 0), (7, 3)), 1, -0.5, ((9, 4, 1), (7, 3)), np.False_),\n",
       "   (((9, 4, 1), (7, 3)), 0, -0.5, ((9, 5, 1), (7, 3)), np.False_),\n",
       "   (((9, 5, 1), (7, 3)), 0, -0.5, ((9, 6, 1), (7, 3)), np.False_),\n",
       "   (((9, 6, 1), (7, 3)), 0, -0.5, ((9, 7, 1), (7, 3)), np.False_),\n",
       "   (((9, 7, 1), (7, 3)), 0, -0.5, ((9, 8, 1), (7, 3)), np.False_),\n",
       "   (((9, 8, 1), (7, 3)), 0, -0.5, ((9, 9, 1), (7, 3)), np.False_),\n",
       "   (((9, 9, 1), (7, 3)), 0, -0.5, ((9, 10, 1), (7, 3)), np.False_),\n",
       "   (((9, 10, 1), (7, 3)), 0, -0.5, ((9, 11, 1), (7, 3)), np.False_),\n",
       "   (((9, 11, 1), (7, 3)), 0, -0.5, ((9, 12, 1), (7, 3)), np.False_),\n",
       "   (((9, 12, 1), (7, 3)), 0, -0.5, ((9, 13, 1), (7, 3)), np.False_),\n",
       "   (((9, 13, 1), (7, 3)), 0, -0.5, ((9, 14, 1), (7, 3)), np.False_),\n",
       "   (((9, 14, 1), (7, 3)), 0, -0.5, ((9, 15, 1), (7, 3)), np.False_),\n",
       "   (((9, 15, 1), (7, 3)), 0, -0.5, ((9, 16, 1), (7, 3)), np.False_),\n",
       "   (((9, 16, 1), (7, 3)), 0, -0.5, ((9, 17, 1), (7, 3)), np.False_),\n",
       "   (((9, 17, 1), (7, 3)), 0, -0.5, ((9, 18, 1), (7, 3)), np.False_),\n",
       "   (((9, 18, 1), (7, 3)), 0, -0.5, ((9, 19, 1), (7, 3)), np.False_),\n",
       "   (((9, 19, 1), (7, 3)), 0, -0.5, ((9, 20, 1), (7, 3)), np.False_),\n",
       "   (((9, 20, 1), (7, 3)), 0, 1, ((9, 21, 1), (7, 3)), np.True_)],\n",
       "  [(((9, 9, 3), (5, 4)), 0, -0.5, ((9, 8, 3), (5, 4)), np.False_),\n",
       "   (((9, 8, 3), (5, 4)), 3, -0.5, ((10, 8, 2), (5, 4)), np.False_),\n",
       "   (((10, 8, 2), (5, 4)), 0, -0.5, ((11, 8, 2), (5, 4)), np.False_),\n",
       "   (((11, 8, 2), (5, 4)), 0, -0.5, ((12, 8, 2), (5, 4)), np.False_),\n",
       "   (((12, 8, 2), (5, 4)), 3, -0.5, ((12, 9, 1), (5, 4)), np.False_),\n",
       "   (((12, 9, 1), (5, 4)), 0, -0.5, ((12, 10, 1), (5, 4)), np.False_),\n",
       "   (((12, 10, 1), (5, 4)), 0, -0.5, ((12, 11, 1), (5, 4)), np.False_),\n",
       "   (((12, 11, 1), (5, 4)), 0, -0.5, ((12, 12, 1), (5, 4)), np.False_),\n",
       "   (((12, 12, 1), (5, 4)), 0, -0.5, ((12, 13, 1), (5, 4)), np.False_),\n",
       "   (((12, 13, 1), (5, 4)), 0, -0.5, ((12, 14, 1), (5, 4)), np.False_),\n",
       "   (((12, 14, 1), (5, 4)), 0, 1, ((12, 15, 1), (5, 4)), np.True_)]],\n",
       " [[tensor([0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123]),\n",
       "   tensor([0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175,\n",
       "           0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175,\n",
       "           0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175,\n",
       "           0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175,\n",
       "           0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175,\n",
       "           0.0175, 0.0175, 0.0175, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0175, 0.0175, 0.0175, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0175, 0.0175, 0.0175, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0175, 0.0175, 0.0175, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "           0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "           0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "           0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "           0.0196, 0.0196, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0196, 0.0196, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0196, 0.0196, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0196, 0.0196, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0196, 0.0196, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "           0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "           0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "           0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "           0.0196, 0.0196, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0196, 0.0196, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0196, 0.0196, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0196, 0.0196, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0196, 0.0196, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "           0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "           0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "           0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "           0.0196, 0.0196, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0196, 0.0196, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0196, 0.0196, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0196, 0.0196, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0196, 0.0196, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222,\n",
       "           0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222,\n",
       "           0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222,\n",
       "           0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222,\n",
       "           0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222,\n",
       "           0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222,\n",
       "           0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222,\n",
       "           0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222,\n",
       "           0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222,\n",
       "           0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222,\n",
       "           0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0222, 0.0222, 0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0.])],\n",
       "  [tensor([0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0.])],\n",
       "  [tensor([0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123]),\n",
       "   tensor([0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0303, 0.0303, 0.0303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0400, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0.])],\n",
       "  [tensor([0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1111, 0.1111, 0.1111,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1111, 0.1111, 0.1111,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1111, 0.1111, 0.1111]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1667, 0.1667, 0.1667,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1667, 0.1667, 0.1667]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2500, 0.2500,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2500, 0.2500]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2500, 0.2500,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2500, 0.2500]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2500, 0.2500,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2500, 0.2500]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000]),\n",
       "   tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "   tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "   tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "   tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "   tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "   tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 1.])],\n",
       "  [tensor([0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123]),\n",
       "   tensor([0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164, 0.0164,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0164, 0.0164, 0.0164, 0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0.])],\n",
       "  [tensor([0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0.])],\n",
       "  [tensor([0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123]),\n",
       "   tensor([0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208,\n",
       "           0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208,\n",
       "           0.0208, 0.0208, 0.0208, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0208, 0.0208, 0.0208, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0208, 0.0208, 0.0208, 0.0000, 0.0000, 0.0000, 0.0208, 0.0208, 0.0208,\n",
       "           0.0208, 0.0208, 0.0208, 0.0000, 0.0000, 0.0000, 0.0208, 0.0208, 0.0208,\n",
       "           0.0208, 0.0208, 0.0208, 0.0000, 0.0000, 0.0000, 0.0208, 0.0208, 0.0208,\n",
       "           0.0208, 0.0208, 0.0208, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0208, 0.0208, 0.0208, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208,\n",
       "           0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208,\n",
       "           0.0208, 0.0208, 0.0208, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0208, 0.0208, 0.0208, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0208, 0.0208, 0.0208, 0.0000, 0.0000, 0.0000, 0.0208, 0.0208, 0.0208,\n",
       "           0.0208, 0.0208, 0.0208, 0.0000, 0.0000, 0.0000, 0.0208, 0.0208, 0.0208,\n",
       "           0.0208, 0.0208, 0.0208, 0.0000, 0.0000, 0.0000, 0.0208, 0.0208, 0.0208,\n",
       "           0.0208, 0.0208, 0.0208, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0208, 0.0208, 0.0208, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208,\n",
       "           0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208,\n",
       "           0.0208, 0.0208, 0.0208, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0208, 0.0208, 0.0208, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0208, 0.0208, 0.0208, 0.0000, 0.0000, 0.0000, 0.0208, 0.0208, 0.0208,\n",
       "           0.0208, 0.0208, 0.0208, 0.0000, 0.0000, 0.0000, 0.0208, 0.0208, 0.0208,\n",
       "           0.0208, 0.0208, 0.0208, 0.0000, 0.0000, 0.0000, 0.0208, 0.0208, 0.0208,\n",
       "           0.0208, 0.0208, 0.0208, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0208, 0.0208, 0.0208, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244,\n",
       "           0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0244, 0.0244, 0.0244,\n",
       "           0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0244, 0.0244, 0.0244,\n",
       "           0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0244, 0.0244, 0.0244,\n",
       "           0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244,\n",
       "           0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244,\n",
       "           0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0244, 0.0244, 0.0244,\n",
       "           0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0244, 0.0244, 0.0244,\n",
       "           0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0244, 0.0244, 0.0244,\n",
       "           0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0244, 0.0244, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263,\n",
       "           0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263,\n",
       "           0.0263, 0.0263, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0263, 0.0263, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0263, 0.0263, 0.0000, 0.0000, 0.0000, 0.0000, 0.0263, 0.0263, 0.0263,\n",
       "           0.0263, 0.0263, 0.0000, 0.0000, 0.0000, 0.0000, 0.0263, 0.0263, 0.0263,\n",
       "           0.0263, 0.0263, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0263, 0.0263, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0263, 0.0263, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263,\n",
       "           0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263,\n",
       "           0.0263, 0.0263, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0263, 0.0263, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0263, 0.0263, 0.0000, 0.0000, 0.0000, 0.0000, 0.0263, 0.0263, 0.0263,\n",
       "           0.0263, 0.0263, 0.0000, 0.0000, 0.0000, 0.0000, 0.0263, 0.0263, 0.0263,\n",
       "           0.0263, 0.0263, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0263, 0.0263, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0263, 0.0263, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0.])],\n",
       "  [tensor([0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123]),\n",
       "   tensor([0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
       "           0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
       "           0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
       "           0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
       "           0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
       "           0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
       "           0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
       "           0.0145, 0.0145, 0.0145, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0145, 0.0145, 0.0145, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
       "           0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
       "           0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
       "           0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
       "           0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
       "           0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
       "           0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
       "           0.0145, 0.0145, 0.0145, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0145, 0.0145, 0.0145, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
       "           0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
       "           0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
       "           0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
       "           0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
       "           0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
       "           0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
       "           0.0149, 0.0149, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0149, 0.0149, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
       "           0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
       "           0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
       "           0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
       "           0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
       "           0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
       "           0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
       "           0.0149, 0.0149, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0149, 0.0149, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
       "           0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
       "           0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
       "           0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
       "           0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
       "           0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
       "           0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
       "           0.0149, 0.0149, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0149, 0.0149, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0167, 0.0167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0167, 0.0167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0167, 0.0167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0167, 0.0167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0167, 0.0167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0167, 0.0167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
       "           0.0167, 0.0167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0167, 0.0167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0167, 0.0167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189,\n",
       "           0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189,\n",
       "           0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189,\n",
       "           0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189,\n",
       "           0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189,\n",
       "           0.0189, 0.0189, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0189, 0.0189, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0189, 0.0189, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0189, 0.0189, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189,\n",
       "           0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189,\n",
       "           0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189,\n",
       "           0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189,\n",
       "           0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189,\n",
       "           0.0189, 0.0189, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0189, 0.0189, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0189, 0.0189, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0189, 0.0189, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189,\n",
       "           0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189,\n",
       "           0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189,\n",
       "           0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189,\n",
       "           0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189, 0.0189,\n",
       "           0.0189, 0.0189, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0189, 0.0189, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0189, 0.0189, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0189, 0.0189, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0.])],\n",
       "  [tensor([0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123]),\n",
       "   tensor([0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0278, 0.0278, 0.0278, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0370, 0.0370, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0.])],\n",
       "  [tensor([0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "           0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "   tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0.])]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# thompson sampling\n",
    "eval_agent(\"POMDP\",pomdp_agent,env, num_episodes=10,max_episode_steps=50,render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN sb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flaccagora/.miniconda3/envs/rob/lib/python3.9/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code() takes at most 16 arguments (18 given)\n",
      "  warnings.warn(\n",
      "/home/flaccagora/.miniconda3/envs/rob/lib/python3.9/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object exploration_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code() takes at most 16 arguments (18 given)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7ffb340d4280>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from environment.env import MDPGYMGridEnvDeform\n",
    "\n",
    "env = MDPGYMGridEnvDeform(maze,l0,h0,l1,h1)\n",
    "model = DQN(\"MultiInputPolicy\",env)\n",
    "# model.load(\"agents/pretrained/1kvpnpin/model.zip\")\n",
    "model.load(\"agents/pretrained/MDP/model.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class MDP_DQNsb3():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def get_action(self,s):\n",
    "        print(s)\n",
    "        s = OrderedDict({\n",
    "                \"x\": torch.tensor([s[0][0]]),              # Values from 0 to 10\n",
    "                \"y\": torch.tensor([s[0][1]]),              # Values from 0 to 10\n",
    "                \"phi\": torch.tensor([s[0][2]]),             # Values from 0 to 4\n",
    "                \"theta\": torch.tensor(s[1]) ,                                \n",
    "            })\n",
    "        print(s)\n",
    "        # Agent takes an action using a greedy policy (without exploration)\n",
    "        action = self.model.predict(s,deterministic=True)[0]\n",
    "        return action\n",
    "    \n",
    "mdpmodel = MDP_DQNsb3(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((1, 3, 3), (5, 3)), -0.5, np.False_, False, {})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s, _ = env.reset()\n",
    "env.step(0,execute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, 15, 0), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([0])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, 0), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -2.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -4.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -6.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -8.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -10.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -12.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -14.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -16.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -18.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -20.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -22.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -24.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -26.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -28.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -30.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -32.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -34.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -36.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -38.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -40.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -42.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -44.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -46.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -48.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -50.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -52.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -54.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -56.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -58.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -60.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -62.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -64.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -66.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -68.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -70.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -72.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -74.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -76.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -78.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -80.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -82.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -84.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -86.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -88.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -90.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -92.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -94.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -96.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -98.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -100.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -102.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -104.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -106.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -108.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -110.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -112.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -114.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -116.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -118.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -120.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -122.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -124.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -126.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -128.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -130.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -132.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -134.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -136.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -138.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -140.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -142.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -144.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -146.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -148.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -150.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -152.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -154.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -156.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -158.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -160.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -162.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -164.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -166.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -168.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -170.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -172.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -174.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -176.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -178.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -180.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -182.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -184.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -186.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -188.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -190.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -192.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 15, array([0])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([15])), ('phi', tensor([[0]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 15, array([0])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -194.0  \n",
      "Next State:  ((1, 14, array([3])), (4, 8))\n",
      "\n",
      "\n",
      "((1, 14, array([3])), (4, 8))\n",
      "OrderedDict([('x', tensor([1])), ('y', tensor([14])), ('phi', tensor([[3]])), ('theta', tensor([4, 8]))])\n",
      "State ((1, 14, array([3])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -196.0  \n",
      "Next State:  ((2, 14, array([2])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 14, array([2])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([14])), ('phi', tensor([[2]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 14, array([2])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -198.0  \n",
      "Next State:  ((2, 15, array([1])), (4, 8))\n",
      "\n",
      "\n",
      "((2, 15, array([1])), (4, 8))\n",
      "OrderedDict([('x', tensor([2])), ('y', tensor([15])), ('phi', tensor([[1]])), ('theta', tensor([4, 8]))])\n",
      "State ((2, 15, array([1])), (4, 8))\n",
      "Action:  [3]\n",
      "Reward:     -200.0  \n",
      "Next State:  ((1, 15, array([0])), (4, 8))\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(((1, 15, 0), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 15, array([0])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 14, array([3])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((1, 14, array([3])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 14, array([2])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 14, array([2])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((2, 15, array([1])), (4, 8)),\n",
       "   np.False_),\n",
       "  (((2, 15, array([1])), (4, 8)),\n",
       "   array([3]),\n",
       "   -2,\n",
       "   ((1, 15, array([0])), (4, 8)),\n",
       "   np.False_)]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_agent(\"MDP\",mdpmodel,env, num_episodes=1,max_episode_steps=100,render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class DeepThompson_DQNsb3():\n",
    "    def __init__(self, model,env):\n",
    "        self.model = model\n",
    "        self.env = env\n",
    "\n",
    "    def get_entropy(self, belief):\n",
    "        return -torch.sum(belief * torch.log(belief + 1e-10))\n",
    "\n",
    "    def get_action(self, belief,pos):\n",
    "        theta = self.env.deformations[torch.multinomial(belief, 1).item()]\n",
    "\n",
    "        s = OrderedDict({\n",
    "                \"x\": torch.tensor([pos[0]],dtype=torch.int32),              # Values from 0 to 10\n",
    "                \"y\": torch.tensor([pos[1]],dtype=torch.int32),              # Values from 0 to 10\n",
    "                \"phi\": torch.tensor([pos[2]],dtype=torch.int32),             # Values from 0 to 4\n",
    "                \"theta\": torch.tensor(theta) , # Probability vector\n",
    "            })\n",
    "\n",
    "        # Agent takes an action using a greedy policy (without exploration)\n",
    "        action = self.model.predict(s,deterministic=True)[0]\n",
    "        return action\n",
    "    \n",
    "    def update_belief(self, belief, pos, observation):\n",
    "        \"\"\"\"\n",
    "        perform update over theta\n",
    "        \n",
    "        $$b'_{x,a,o}(theta) = \\eta \\cdot p(o|x,theta) \\cdot b(theta)$$\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        new_belief = torch.zeros_like(belief)\n",
    "\n",
    "        for t, theta in enumerate(self.env.deformations):\n",
    "            P_o_s_theta = np.all(self.env.get_observation(s = (pos,theta)) == observation) # 0 or 1 \n",
    "\n",
    "            new_belief[t] = P_o_s_theta * belief[t]\n",
    "        \n",
    "        new_belief = new_belief / (torch.sum(new_belief) + 1e-10)\n",
    "\n",
    "        return new_belief\n",
    "\n",
    "pomdp_agent = DeepThompson_DQNsb3(model,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POMDP\n",
      "eval_agent_pomdp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((19, 7, 1), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -2.0  \n",
      "Next State:  ((18, 7, 0), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0196)\n",
      "Belief entropy:  tensor(3.9318)\n",
      "\n",
      "\n",
      "State ((18, 7, 0), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -4.0  \n",
      "Next State:  ((18, 6, 3), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 6, 3), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -6.0  \n",
      "Next State:  ((19, 6, 2), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((19, 6, 2), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -8.0  \n",
      "Next State:  ((19, 7, 1), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((19, 7, 1), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -10.0  \n",
      "Next State:  ((18, 7, 0), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 7, 0), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -12.0  \n",
      "Next State:  ((18, 6, 3), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 6, 3), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -14.0  \n",
      "Next State:  ((19, 6, 2), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((19, 6, 2), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -16.0  \n",
      "Next State:  ((19, 7, 1), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((19, 7, 1), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -18.0  \n",
      "Next State:  ((18, 7, 0), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 7, 0), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -20.0  \n",
      "Next State:  ((18, 6, 3), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 6, 3), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -22.0  \n",
      "Next State:  ((19, 6, 2), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((19, 6, 2), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -24.0  \n",
      "Next State:  ((19, 7, 1), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((19, 7, 1), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -26.0  \n",
      "Next State:  ((18, 7, 0), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 7, 0), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -28.0  \n",
      "Next State:  ((18, 6, 3), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 6, 3), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -30.0  \n",
      "Next State:  ((19, 6, 2), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((19, 6, 2), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -32.0  \n",
      "Next State:  ((19, 7, 1), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((19, 7, 1), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -34.0  \n",
      "Next State:  ((18, 7, 0), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 7, 0), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -36.0  \n",
      "Next State:  ((18, 6, 3), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 6, 3), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -38.0  \n",
      "Next State:  ((19, 6, 2), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((19, 6, 2), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -40.0  \n",
      "Next State:  ((19, 7, 1), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((19, 7, 1), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -42.0  \n",
      "Next State:  ((18, 7, 0), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 7, 0), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -44.0  \n",
      "Next State:  ((18, 6, 3), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 6, 3), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -46.0  \n",
      "Next State:  ((19, 6, 2), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((19, 6, 2), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -48.0  \n",
      "Next State:  ((19, 7, 1), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((19, 7, 1), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -50.0  \n",
      "Next State:  ((18, 7, 0), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 7, 0), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -52.0  \n",
      "Next State:  ((18, 6, 3), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 6, 3), (7, 1))\n",
      "Action:  2\n",
      "Reward:     -54.0  \n",
      "Next State:  ((18, 7, 1), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 7, 1), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -56.0  \n",
      "Next State:  ((17, 7, 0), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((17, 7, 0), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -58.0  \n",
      "Next State:  ((17, 6, 3), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((17, 6, 3), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -60.0  \n",
      "Next State:  ((18, 6, 2), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 6, 2), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -62.0  \n",
      "Next State:  ((18, 7, 1), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 7, 1), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -64.0  \n",
      "Next State:  ((17, 7, 0), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((17, 7, 0), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -66.0  \n",
      "Next State:  ((17, 6, 3), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((17, 6, 3), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -68.0  \n",
      "Next State:  ((18, 6, 2), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 6, 2), (7, 1))\n",
      "Action:  2\n",
      "Reward:     -70.0  \n",
      "Next State:  ((17, 6, 0), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((17, 6, 0), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -72.0  \n",
      "Next State:  ((17, 5, 3), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((17, 5, 3), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -74.0  \n",
      "Next State:  ((18, 5, 2), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 5, 2), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -76.0  \n",
      "Next State:  ((18, 6, 1), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 6, 1), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -78.0  \n",
      "Next State:  ((17, 6, 0), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((17, 6, 0), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -80.0  \n",
      "Next State:  ((17, 5, 3), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((17, 5, 3), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -82.0  \n",
      "Next State:  ((18, 5, 2), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 5, 2), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -84.0  \n",
      "Next State:  ((18, 6, 1), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 6, 1), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -86.0  \n",
      "Next State:  ((17, 6, 0), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((17, 6, 0), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -88.0  \n",
      "Next State:  ((17, 5, 3), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:11,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((17, 5, 3), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -90.0  \n",
      "Next State:  ((18, 5, 2), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 5, 2), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -92.0  \n",
      "Next State:  ((18, 6, 1), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 6, 1), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -94.0  \n",
      "Next State:  ((17, 6, 0), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((17, 6, 0), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -96.0  \n",
      "Next State:  ((17, 5, 3), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((17, 5, 3), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -98.0  \n",
      "Next State:  ((18, 5, 2), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((18, 5, 2), (7, 1))\n",
      "Action:  3\n",
      "Reward:     -100.0  \n",
      "Next State:  ((18, 6, 1), (7, 1))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0204)\n",
      "Belief entropy:  tensor(3.8918)\n",
      "\n",
      "\n",
      "State ((8, 8, 3), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -2.0  \n",
      "Next State:  ((9, 8, 2), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 8, 2), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -4.0  \n",
      "Next State:  ((9, 9, 1), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 9, 1), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -6.0  \n",
      "Next State:  ((8, 9, 0), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 9, 0), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -8.0  \n",
      "Next State:  ((8, 8, 3), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 8, 3), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -10.0  \n",
      "Next State:  ((9, 8, 2), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 8, 2), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -12.0  \n",
      "Next State:  ((9, 9, 1), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 9, 1), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -14.0  \n",
      "Next State:  ((8, 9, 0), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 9, 0), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -16.0  \n",
      "Next State:  ((8, 8, 3), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 8, 3), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -18.0  \n",
      "Next State:  ((9, 8, 2), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 8, 2), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -20.0  \n",
      "Next State:  ((9, 9, 1), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 9, 1), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -22.0  \n",
      "Next State:  ((8, 9, 0), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 9, 0), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -24.0  \n",
      "Next State:  ((8, 8, 3), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 8, 3), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -26.0  \n",
      "Next State:  ((9, 8, 2), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 8, 2), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -28.0  \n",
      "Next State:  ((9, 9, 1), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 9, 1), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -30.0  \n",
      "Next State:  ((8, 9, 0), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 9, 0), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -32.0  \n",
      "Next State:  ((8, 8, 3), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 8, 3), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -34.0  \n",
      "Next State:  ((9, 8, 2), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 8, 2), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -36.0  \n",
      "Next State:  ((9, 9, 1), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 9, 1), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -38.0  \n",
      "Next State:  ((8, 9, 0), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 9, 0), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -40.0  \n",
      "Next State:  ((8, 8, 3), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 8, 3), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -42.0  \n",
      "Next State:  ((9, 8, 2), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 8, 2), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -44.0  \n",
      "Next State:  ((9, 9, 1), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 9, 1), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -46.0  \n",
      "Next State:  ((8, 9, 0), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 9, 0), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -48.0  \n",
      "Next State:  ((8, 8, 3), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 8, 3), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -50.0  \n",
      "Next State:  ((9, 8, 2), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 8, 2), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -52.0  \n",
      "Next State:  ((9, 9, 1), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 9, 1), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -54.0  \n",
      "Next State:  ((8, 9, 0), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 9, 0), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -56.0  \n",
      "Next State:  ((8, 8, 3), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 8, 3), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -58.0  \n",
      "Next State:  ((9, 8, 2), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 8, 2), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -60.0  \n",
      "Next State:  ((9, 9, 1), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 9, 1), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -62.0  \n",
      "Next State:  ((8, 9, 0), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 9, 0), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -64.0  \n",
      "Next State:  ((8, 8, 3), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 8, 3), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -66.0  \n",
      "Next State:  ((9, 8, 2), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 8, 2), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -68.0  \n",
      "Next State:  ((9, 9, 1), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 9, 1), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -70.0  \n",
      "Next State:  ((8, 9, 0), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 9, 0), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -72.0  \n",
      "Next State:  ((8, 8, 3), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 8, 3), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -74.0  \n",
      "Next State:  ((9, 8, 2), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 8, 2), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -76.0  \n",
      "Next State:  ((9, 9, 1), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 9, 1), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -78.0  \n",
      "Next State:  ((8, 9, 0), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 9, 0), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -80.0  \n",
      "Next State:  ((8, 8, 3), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 8, 3), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -82.0  \n",
      "Next State:  ((9, 8, 2), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 8, 2), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -84.0  \n",
      "Next State:  ((9, 9, 1), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 9, 1), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -86.0  \n",
      "Next State:  ((8, 9, 0), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 9, 0), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -88.0  \n",
      "Next State:  ((8, 8, 3), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 8, 3), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -90.0  \n",
      "Next State:  ((9, 8, 2), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 8, 2), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -92.0  \n",
      "Next State:  ((9, 9, 1), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 9, 1), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -94.0  \n",
      "Next State:  ((8, 9, 0), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((8, 9, 0), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -96.0  \n",
      "Next State:  ((8, 8, 3), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:02<00:10,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((8, 8, 3), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -98.0  \n",
      "Next State:  ((9, 8, 2), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((9, 8, 2), (5, 7))\n",
      "Action:  3\n",
      "Reward:     -100.0  \n",
      "Next State:  ((9, 9, 1), (5, 7))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0238)\n",
      "Belief entropy:  tensor(3.7377)\n",
      "\n",
      "\n",
      "State ((10, 23, 0), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -2.0  \n",
      "Next State:  ((10, 22, 3), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 22, 3), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -4.0  \n",
      "Next State:  ((11, 22, 2), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 22, 2), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -6.0  \n",
      "Next State:  ((11, 23, 1), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 23, 1), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -8.0  \n",
      "Next State:  ((10, 23, 0), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 23, 0), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -10.0  \n",
      "Next State:  ((10, 22, 3), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 22, 3), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -12.0  \n",
      "Next State:  ((11, 22, 2), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 22, 2), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -14.0  \n",
      "Next State:  ((11, 23, 1), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 23, 1), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -16.0  \n",
      "Next State:  ((10, 23, 0), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 23, 0), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -18.0  \n",
      "Next State:  ((10, 22, 3), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 22, 3), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -20.0  \n",
      "Next State:  ((11, 22, 2), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 22, 2), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -22.0  \n",
      "Next State:  ((11, 23, 1), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 23, 1), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -24.0  \n",
      "Next State:  ((10, 23, 0), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 23, 0), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -26.0  \n",
      "Next State:  ((10, 22, 3), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 22, 3), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -28.0  \n",
      "Next State:  ((11, 22, 2), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 22, 2), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -30.0  \n",
      "Next State:  ((11, 23, 1), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 23, 1), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -32.0  \n",
      "Next State:  ((10, 23, 0), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 23, 0), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -34.0  \n",
      "Next State:  ((10, 22, 3), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 22, 3), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -36.0  \n",
      "Next State:  ((11, 22, 2), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 22, 2), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -38.0  \n",
      "Next State:  ((11, 23, 1), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 23, 1), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -40.0  \n",
      "Next State:  ((10, 23, 0), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 23, 0), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -42.0  \n",
      "Next State:  ((10, 22, 3), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 22, 3), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -44.0  \n",
      "Next State:  ((11, 22, 2), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 22, 2), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -46.0  \n",
      "Next State:  ((11, 23, 1), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 23, 1), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -48.0  \n",
      "Next State:  ((10, 23, 0), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 23, 0), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -50.0  \n",
      "Next State:  ((10, 22, 3), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 22, 3), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -52.0  \n",
      "Next State:  ((11, 22, 2), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 22, 2), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -54.0  \n",
      "Next State:  ((11, 23, 1), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 23, 1), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -56.0  \n",
      "Next State:  ((10, 23, 0), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 23, 0), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -58.0  \n",
      "Next State:  ((10, 22, 3), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 22, 3), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -60.0  \n",
      "Next State:  ((11, 22, 2), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 22, 2), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -62.0  \n",
      "Next State:  ((11, 23, 1), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 23, 1), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -64.0  \n",
      "Next State:  ((10, 23, 0), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 23, 0), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -66.0  \n",
      "Next State:  ((10, 22, 3), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 22, 3), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -68.0  \n",
      "Next State:  ((11, 22, 2), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 22, 2), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -70.0  \n",
      "Next State:  ((11, 23, 1), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 23, 1), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -72.0  \n",
      "Next State:  ((10, 23, 0), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 23, 0), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -74.0  \n",
      "Next State:  ((10, 22, 3), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 22, 3), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -76.0  \n",
      "Next State:  ((11, 22, 2), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 22, 2), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -78.0  \n",
      "Next State:  ((11, 23, 1), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 23, 1), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -80.0  \n",
      "Next State:  ((10, 23, 0), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 23, 0), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -82.0  \n",
      "Next State:  ((10, 22, 3), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 22, 3), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -84.0  \n",
      "Next State:  ((11, 22, 2), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:03<00:08,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((11, 22, 2), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -86.0  \n",
      "Next State:  ((11, 23, 1), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 23, 1), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -88.0  \n",
      "Next State:  ((10, 23, 0), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 23, 0), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -90.0  \n",
      "Next State:  ((10, 22, 3), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 22, 3), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -92.0  \n",
      "Next State:  ((11, 22, 2), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 22, 2), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -94.0  \n",
      "Next State:  ((11, 23, 1), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((11, 23, 1), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -96.0  \n",
      "Next State:  ((10, 23, 0), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 23, 0), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -98.0  \n",
      "Next State:  ((10, 22, 3), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((10, 22, 3), (2, 6))\n",
      "Action:  3\n",
      "Reward:     -100.0  \n",
      "Next State:  ((11, 22, 2), (2, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0167)\n",
      "Belief entropy:  tensor(4.0943)\n",
      "\n",
      "\n",
      "State ((14, 11, 1), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -2.0  \n",
      "Next State:  ((13, 11, 0), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 11, 0), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -4.0  \n",
      "Next State:  ((13, 10, 3), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 10, 3), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -6.0  \n",
      "Next State:  ((14, 10, 2), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 10, 2), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -8.0  \n",
      "Next State:  ((14, 11, 1), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 11, 1), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -10.0  \n",
      "Next State:  ((13, 11, 0), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 11, 0), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -12.0  \n",
      "Next State:  ((13, 10, 3), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 10, 3), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -14.0  \n",
      "Next State:  ((14, 10, 2), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 10, 2), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -16.0  \n",
      "Next State:  ((14, 11, 1), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 11, 1), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -18.0  \n",
      "Next State:  ((13, 11, 0), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 11, 0), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -20.0  \n",
      "Next State:  ((13, 10, 3), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 10, 3), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -22.0  \n",
      "Next State:  ((14, 10, 2), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 10, 2), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -24.0  \n",
      "Next State:  ((14, 11, 1), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 11, 1), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -26.0  \n",
      "Next State:  ((13, 11, 0), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 11, 0), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -28.0  \n",
      "Next State:  ((13, 10, 3), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 10, 3), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -30.0  \n",
      "Next State:  ((14, 10, 2), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 10, 2), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -32.0  \n",
      "Next State:  ((14, 11, 1), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 11, 1), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -34.0  \n",
      "Next State:  ((13, 11, 0), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 11, 0), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -36.0  \n",
      "Next State:  ((13, 10, 3), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 10, 3), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -38.0  \n",
      "Next State:  ((14, 10, 2), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 10, 2), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -40.0  \n",
      "Next State:  ((14, 11, 1), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 11, 1), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -42.0  \n",
      "Next State:  ((13, 11, 0), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 11, 0), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -44.0  \n",
      "Next State:  ((13, 10, 3), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 10, 3), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -46.0  \n",
      "Next State:  ((14, 10, 2), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 10, 2), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -48.0  \n",
      "Next State:  ((14, 11, 1), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 11, 1), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -50.0  \n",
      "Next State:  ((13, 11, 0), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 11, 0), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -52.0  \n",
      "Next State:  ((13, 10, 3), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 10, 3), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -54.0  \n",
      "Next State:  ((14, 10, 2), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 10, 2), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -56.0  \n",
      "Next State:  ((14, 11, 1), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 11, 1), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -58.0  \n",
      "Next State:  ((13, 11, 0), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 11, 0), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -60.0  \n",
      "Next State:  ((13, 10, 3), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 10, 3), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -62.0  \n",
      "Next State:  ((14, 10, 2), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 10, 2), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -64.0  \n",
      "Next State:  ((14, 11, 1), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 11, 1), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -66.0  \n",
      "Next State:  ((13, 11, 0), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 11, 0), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -68.0  \n",
      "Next State:  ((13, 10, 3), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 10, 3), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -70.0  \n",
      "Next State:  ((14, 10, 2), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 10, 2), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -72.0  \n",
      "Next State:  ((14, 11, 1), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 11, 1), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -74.0  \n",
      "Next State:  ((13, 11, 0), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 11, 0), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -76.0  \n",
      "Next State:  ((13, 10, 3), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 10, 3), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -78.0  \n",
      "Next State:  ((14, 10, 2), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 10, 2), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -80.0  \n",
      "Next State:  ((14, 11, 1), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 11, 1), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -82.0  \n",
      "Next State:  ((13, 11, 0), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 11, 0), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -84.0  \n",
      "Next State:  ((13, 10, 3), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 10, 3), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -86.0  \n",
      "Next State:  ((14, 10, 2), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 10, 2), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -88.0  \n",
      "Next State:  ((14, 11, 1), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 11, 1), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -90.0  \n",
      "Next State:  ((13, 11, 0), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 11, 0), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -92.0  \n",
      "Next State:  ((13, 10, 3), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:04<00:07,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((13, 10, 3), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -94.0  \n",
      "Next State:  ((14, 10, 2), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 10, 2), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -96.0  \n",
      "Next State:  ((14, 11, 1), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((14, 11, 1), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -98.0  \n",
      "Next State:  ((13, 11, 0), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((13, 11, 0), (7, 4))\n",
      "Action:  3\n",
      "Reward:     -100.0  \n",
      "Next State:  ((13, 10, 3), (7, 4))\n",
      "argmax and max Belief:  (4, 4) tensor(0.1667)\n",
      "Belief entropy:  tensor(1.7918)\n",
      "\n",
      "\n",
      "State ((17, 26, 1), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -2.0  \n",
      "Next State:  ((16, 26, 0), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0132)\n",
      "Belief entropy:  tensor(4.3307)\n",
      "\n",
      "\n",
      "State ((16, 26, 0), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -4.0  \n",
      "Next State:  ((16, 25, 3), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 25, 3), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -6.0  \n",
      "Next State:  ((17, 25, 2), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((17, 25, 2), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -8.0  \n",
      "Next State:  ((17, 26, 1), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((17, 26, 1), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -10.0  \n",
      "Next State:  ((16, 26, 0), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 26, 0), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -12.0  \n",
      "Next State:  ((16, 25, 3), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 25, 3), (7, 5))\n",
      "Action:  2\n",
      "Reward:     -14.0  \n",
      "Next State:  ((16, 26, 1), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 26, 1), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -16.0  \n",
      "Next State:  ((15, 26, 0), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 26, 0), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -18.0  \n",
      "Next State:  ((15, 25, 3), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 25, 3), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -20.0  \n",
      "Next State:  ((16, 25, 2), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 25, 2), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -22.0  \n",
      "Next State:  ((16, 26, 1), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 26, 1), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -24.0  \n",
      "Next State:  ((15, 26, 0), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 26, 0), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -26.0  \n",
      "Next State:  ((15, 25, 3), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 25, 3), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -28.0  \n",
      "Next State:  ((16, 25, 2), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 25, 2), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -30.0  \n",
      "Next State:  ((16, 26, 1), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 26, 1), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -32.0  \n",
      "Next State:  ((15, 26, 0), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 26, 0), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -34.0  \n",
      "Next State:  ((15, 25, 3), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 25, 3), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -36.0  \n",
      "Next State:  ((16, 25, 2), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 25, 2), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -38.0  \n",
      "Next State:  ((16, 26, 1), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 26, 1), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -40.0  \n",
      "Next State:  ((15, 26, 0), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 26, 0), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -42.0  \n",
      "Next State:  ((15, 25, 3), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 25, 3), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -44.0  \n",
      "Next State:  ((16, 25, 2), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 25, 2), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -46.0  \n",
      "Next State:  ((16, 26, 1), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 26, 1), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -48.0  \n",
      "Next State:  ((15, 26, 0), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 26, 0), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -50.0  \n",
      "Next State:  ((15, 25, 3), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 25, 3), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -52.0  \n",
      "Next State:  ((16, 25, 2), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 25, 2), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -54.0  \n",
      "Next State:  ((16, 26, 1), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 26, 1), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -56.0  \n",
      "Next State:  ((15, 26, 0), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 26, 0), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -58.0  \n",
      "Next State:  ((15, 25, 3), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 25, 3), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -60.0  \n",
      "Next State:  ((16, 25, 2), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 25, 2), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -62.0  \n",
      "Next State:  ((16, 26, 1), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 26, 1), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -64.0  \n",
      "Next State:  ((15, 26, 0), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 26, 0), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -66.0  \n",
      "Next State:  ((15, 25, 3), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 25, 3), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -68.0  \n",
      "Next State:  ((16, 25, 2), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 25, 2), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -70.0  \n",
      "Next State:  ((16, 26, 1), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 26, 1), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -72.0  \n",
      "Next State:  ((15, 26, 0), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 26, 0), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -74.0  \n",
      "Next State:  ((15, 25, 3), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 25, 3), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -76.0  \n",
      "Next State:  ((16, 25, 2), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 25, 2), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -78.0  \n",
      "Next State:  ((16, 26, 1), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 26, 1), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -80.0  \n",
      "Next State:  ((15, 26, 0), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 26, 0), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -82.0  \n",
      "Next State:  ((15, 25, 3), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 25, 3), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -84.0  \n",
      "Next State:  ((16, 25, 2), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 25, 2), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -86.0  \n",
      "Next State:  ((16, 26, 1), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 26, 1), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -88.0  \n",
      "Next State:  ((15, 26, 0), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 26, 0), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -90.0  \n",
      "Next State:  ((15, 25, 3), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 25, 3), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -92.0  \n",
      "Next State:  ((16, 25, 2), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 25, 2), (7, 5))\n",
      "Action:  2\n",
      "Reward:     -94.0  \n",
      "Next State:  ((15, 25, 0), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((15, 25, 0), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -96.0  \n",
      "Next State:  ((15, 24, 3), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:06<00:06,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((15, 24, 3), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -98.0  \n",
      "Next State:  ((16, 24, 2), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((16, 24, 2), (7, 5))\n",
      "Action:  3\n",
      "Reward:     -100.0  \n",
      "Next State:  ((16, 25, 1), (7, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((18, 4, 1), (5, 5))\n",
      "Action:  2\n",
      "Reward:     -2.0  \n",
      "Next State:  ((18, 3, 3), (5, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0222)\n",
      "Belief entropy:  tensor(3.8067)\n",
      "\n",
      "\n",
      "State ((18, 3, 3), (5, 5))\n",
      "Action:  3\n",
      "Reward:     -4.0  \n",
      "Next State:  ((19, 3, 2), (5, 5))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0222)\n",
      "Belief entropy:  tensor(3.8067)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m GridEnvDeform(maze,l0,h0,l1,h1)\n\u001b[0;32m----> 2\u001b[0m \u001b[43meval_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOMDP\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mpomdp_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmax_episode_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/RoboSurgery/src/eval/eval.py:284\u001b[0m, in \u001b[0;36meval_agent\u001b[0;34m(observability, agent, env, num_episodes, max_episode_steps, render)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m observability \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOMDP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOMDP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 284\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meval_agent_pomdp\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_episode_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid agent type\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/RoboSurgery/src/eval/eval.py:164\u001b[0m, in \u001b[0;36meval_agent_pomdp\u001b[0;34m(agent, env, num_episodes, max_episode_steps, render)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelief entropy: \u001b[39m\u001b[38;5;124m\"\u001b[39m, agent\u001b[38;5;241m.\u001b[39mget_entropy(next_belief))\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 164\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_bis\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m s \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m    167\u001b[0m b \u001b[38;5;241m=\u001b[39m next_belief\n",
      "File \u001b[0;32m~/Desktop/RoboSurgery/src/environment/env.py:339\u001b[0m, in \u001b[0;36mGridEnvDeform.render_bis\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_state(s)\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# Clear the screen\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Draw the maze\u001b[39;00m\n\u001b[1;32m    342\u001b[0m cell_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen_height) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_shape)\n",
      "\u001b[0;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "env = GridEnvDeform(maze,l0,h0,l1,h1)\n",
    "eval_agent(\"POMDP\",pomdp_agent,env, num_episodes=10,max_episode_steps=50,render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POMDP based solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DoubleDQNAgent(84, 4, lr = 0.01, batch_size=64,target_update_freq=100, wandb=False)\n",
    "agent.load(\"agents/pretrained/POMDP/double_dqn_8000.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POMDP_DQN():\n",
    "    def __init__(self, model,env):\n",
    "        self.model = model\n",
    "        self.env = env\n",
    "\n",
    "    def get_entropy(self, belief):\n",
    "        return -torch.sum(belief * torch.log(belief + 1e-10))\n",
    "\n",
    "    def get_action(self, belief,pos):\n",
    "\n",
    "        action = self.model.choose_deterministic_action(torch.cat((torch.tensor(pos),belief)))\n",
    "        return action\n",
    "    \n",
    "    def update_belief(self, belief, pos, observation):\n",
    "        \"\"\"\"\n",
    "        perform update over theta\n",
    "        \n",
    "        $$b'_{x,a,o}(theta) = \\eta \\cdot p(o|x,theta) \\cdot b(theta)$$\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        new_belief = torch.zeros_like(belief)\n",
    "\n",
    "        for t, theta in enumerate(self.env.deformations):\n",
    "            P_o_s_theta = np.all(self.env.get_observation(s = (pos,theta)) == observation) # 0 or 1 \n",
    "\n",
    "            new_belief[t] = P_o_s_theta * belief[t]\n",
    "        \n",
    "        new_belief = new_belief / (torch.sum(new_belief) + 1e-10)\n",
    "\n",
    "        return new_belief\n",
    "\n",
    "pomdp_agent = POMDP_DQN(agent,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POMDP\n",
      "eval_agent_pomdp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -2.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -4.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -6.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -8.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -10.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -12.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -14.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -16.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -18.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -20.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -22.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -24.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -26.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -28.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -30.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -32.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -34.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -36.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -38.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -40.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -42.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -44.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -46.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -48.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -50.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -52.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -54.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -56.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -58.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -60.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -62.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -64.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -66.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -68.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -70.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -72.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -74.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -76.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -78.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -80.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -82.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -84.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -86.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -88.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -90.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -92.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -94.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -96.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:12,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((26, 25, 1), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -98.0  \n",
      "Next State:  ((26, 24, 3), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((26, 24, 3), (4, 2))\n",
      "Action:  2\n",
      "Reward:     -100.0  \n",
      "Next State:  ((26, 25, 1), (4, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0127)\n",
      "Belief entropy:  tensor(4.3694)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -2.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0164)\n",
      "Belief entropy:  tensor(4.1109)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -4.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -6.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -8.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -10.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -12.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -14.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -16.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -18.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -20.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -22.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -24.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -26.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -28.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -30.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -32.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -34.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -36.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -38.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -40.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -42.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -44.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -46.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -48.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -50.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -52.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -54.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -56.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -58.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -60.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -62.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -64.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -66.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -68.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -70.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -72.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -74.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -76.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -78.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -80.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -82.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -84.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -86.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -88.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -90.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:02<00:10,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -92.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -94.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -96.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 10, 3), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -98.0  \n",
      "Next State:  ((17, 11, 1), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((17, 11, 1), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -100.0  \n",
      "Next State:  ((17, 10, 3), (3, 6))\n",
      "argmax and max Belief:  (3, 6) tensor(0.2000)\n",
      "Belief entropy:  tensor(1.6094)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -2.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -4.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -6.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -8.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -10.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -12.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -14.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -16.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -18.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -20.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -22.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -24.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -26.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -28.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -30.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -32.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -34.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -36.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -38.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -40.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -42.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -44.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -46.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -48.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -50.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -52.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -54.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -56.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -58.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -60.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -62.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -64.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -66.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -68.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -70.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -72.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -74.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -76.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -78.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -80.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -82.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -84.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -86.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -88.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -90.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -92.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -94.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -96.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((13, 23, 0), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -98.0  \n",
      "Next State:  ((14, 23, 2), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:04<00:09,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((14, 23, 2), (3, 2))\n",
      "Action:  2\n",
      "Reward:     -100.0  \n",
      "Next State:  ((13, 23, 0), (3, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0145)\n",
      "Belief entropy:  tensor(4.2341)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -2.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0141)\n",
      "Belief entropy:  tensor(4.2627)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -4.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -6.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -8.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -10.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -12.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -14.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -16.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -18.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -20.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -22.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -24.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -26.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -28.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -30.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -32.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -34.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -36.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -38.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -40.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -42.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -44.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -46.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -48.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -50.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -52.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -54.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -56.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -58.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -60.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -62.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -64.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -66.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -68.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -70.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -72.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -74.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -76.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -78.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -80.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -82.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -84.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -86.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -88.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -90.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:05<00:08,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -92.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -94.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -96.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((21, 15, 0), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -98.0  \n",
      "Next State:  ((22, 15, 2), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((22, 15, 2), (9, 2))\n",
      "Action:  2\n",
      "Reward:     -100.0  \n",
      "Next State:  ((21, 15, 0), (9, 2))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0152)\n",
      "Belief entropy:  tensor(4.1897)\n",
      "\n",
      "\n",
      "State ((7, 24, 2), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -2.0  \n",
      "Next State:  ((6, 24, 0), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((6, 24, 0), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -4.0  \n",
      "Next State:  ((7, 24, 2), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((7, 24, 2), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -6.0  \n",
      "Next State:  ((6, 24, 0), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((6, 24, 0), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -8.0  \n",
      "Next State:  ((7, 24, 2), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((7, 24, 2), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -10.0  \n",
      "Next State:  ((6, 24, 0), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((6, 24, 0), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -12.0  \n",
      "Next State:  ((7, 24, 2), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((7, 24, 2), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -14.0  \n",
      "Next State:  ((6, 24, 0), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((6, 24, 0), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -16.0  \n",
      "Next State:  ((7, 24, 2), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((7, 24, 2), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -18.0  \n",
      "Next State:  ((6, 24, 0), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((6, 24, 0), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -20.0  \n",
      "Next State:  ((7, 24, 2), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((7, 24, 2), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -22.0  \n",
      "Next State:  ((6, 24, 0), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((6, 24, 0), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -24.0  \n",
      "Next State:  ((7, 24, 2), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((7, 24, 2), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -26.0  \n",
      "Next State:  ((6, 24, 0), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((6, 24, 0), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -28.0  \n",
      "Next State:  ((7, 24, 2), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((7, 24, 2), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -30.0  \n",
      "Next State:  ((6, 24, 0), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((6, 24, 0), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -32.0  \n",
      "Next State:  ((7, 24, 2), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((7, 24, 2), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -34.0  \n",
      "Next State:  ((6, 24, 0), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((6, 24, 0), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -36.0  \n",
      "Next State:  ((7, 24, 2), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((7, 24, 2), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -38.0  \n",
      "Next State:  ((6, 24, 0), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((6, 24, 0), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -40.0  \n",
      "Next State:  ((7, 24, 2), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((7, 24, 2), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -42.0  \n",
      "Next State:  ((6, 24, 0), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((6, 24, 0), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -44.0  \n",
      "Next State:  ((7, 24, 2), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((7, 24, 2), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -46.0  \n",
      "Next State:  ((6, 24, 0), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((6, 24, 0), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -48.0  \n",
      "Next State:  ((7, 24, 2), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((7, 24, 2), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -50.0  \n",
      "Next State:  ((6, 24, 0), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((6, 24, 0), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -52.0  \n",
      "Next State:  ((7, 24, 2), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((7, 24, 2), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -54.0  \n",
      "Next State:  ((6, 24, 0), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((6, 24, 0), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -56.0  \n",
      "Next State:  ((7, 24, 2), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((7, 24, 2), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -58.0  \n",
      "Next State:  ((6, 24, 0), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((6, 24, 0), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -60.0  \n",
      "Next State:  ((7, 24, 2), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((7, 24, 2), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -62.0  \n",
      "Next State:  ((6, 24, 0), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((6, 24, 0), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -64.0  \n",
      "Next State:  ((7, 24, 2), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((7, 24, 2), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -66.0  \n",
      "Next State:  ((6, 24, 0), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((6, 24, 0), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -68.0  \n",
      "Next State:  ((7, 24, 2), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((7, 24, 2), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -70.0  \n",
      "Next State:  ((6, 24, 0), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n",
      "State ((6, 24, 0), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -72.0  \n",
      "Next State:  ((7, 24, 2), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:06<00:10,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ((7, 24, 2), (3, 6))\n",
      "Action:  2\n",
      "Reward:     -74.0  \n",
      "Next State:  ((6, 24, 0), (3, 6))\n",
      "argmax and max Belief:  (1, 1) tensor(0.0154)\n",
      "Belief entropy:  tensor(4.1744)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43meval_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOMDP\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mpomdp_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmax_episode_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/RoboSurgery/src/eval/eval.py:285\u001b[0m, in \u001b[0;36meval_agent\u001b[0;34m(observability, agent, env, num_episodes, max_episode_steps, render)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m observability \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOMDP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOMDP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 285\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meval_agent_pomdp\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_episode_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid agent type\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/RoboSurgery/src/eval/eval.py:165\u001b[0m, in \u001b[0;36meval_agent_pomdp\u001b[0;34m(agent, env, num_episodes, max_episode_steps, render)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelief entropy: \u001b[39m\u001b[38;5;124m\"\u001b[39m, agent\u001b[38;5;241m.\u001b[39mget_entropy(next_belief))\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_bis\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m s \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m    168\u001b[0m b \u001b[38;5;241m=\u001b[39m next_belief\n",
      "File \u001b[0;32m~/Desktop/RoboSurgery/src/environment/env.py:340\u001b[0m, in \u001b[0;36mGridEnvDeform.render_bis\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_state(s)\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# Clear the screen\u001b[39;00m\n\u001b[0;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# Draw the maze\u001b[39;00m\n\u001b[1;32m    343\u001b[0m cell_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen_height) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_shape)\n",
      "\u001b[0;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "eval_agent(\"POMDP\",pomdp_agent,env, num_episodes=10,max_episode_steps=50,render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
