{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "import gymnasium as gym \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import itertools\n",
    "import torch\n",
    "from agents.dqn import DoubleDQNAgent # for typing only\n",
    "import pygame\n",
    "from gymnasium.spaces import Dict, Discrete, Box\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GYMGridEnvDeform(gym.Env):\n",
    "    \n",
    "    def __init__(self, maze, l0,h0,l1,h1,render_mode = None):\n",
    "\n",
    "        self.original_maze = maze\n",
    "        self.original_maze_shape = maze.shape\n",
    " \n",
    "        self.maze = maze\n",
    "        self.maze_shape = maze.shape\n",
    "\n",
    "        # list of possible actions\n",
    "        self.actions = [0,1,2,3]\n",
    "        # list of possible orientations\n",
    "        self.orientations = [0,1,2,3]\n",
    "        # list of possible observations\n",
    "        self.obs = list(itertools.product([0,1], repeat=5))\n",
    "        # list of possible deformations\n",
    "        self.deformations = [(i,j) for i in range(l0,h0) for j in range(l1,h1)]\n",
    "\n",
    "        \n",
    "        # space in which every maze lives (is a 2d matrix)\n",
    "        self.max_shape = self.original_maze.shape * np.array([h1-1,h0-1]) + np.array([2,2])\n",
    "        # list of states\n",
    "        self.states = [((x,y,phi),(i,j)) for x in range(1,self.max_shape[0]-1) for y in range(1,self.max_shape[1]-1) for phi in range(4) for i in range(l0,h0) for j in range(l1,h1)] \n",
    "        self.state_dict = {state : i for i, state in enumerate(self.states)}\n",
    "        \n",
    "        self.l0 = l0\n",
    "        self.h0 = h0\n",
    "        self.l1 = l1\n",
    "        self.h1 = h1\n",
    "\n",
    "        self.goal_pos = self.original_maze.shape - np.array([2,2])\n",
    "        \n",
    "        self.frames = []\n",
    "        self.reset()\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        if self.render_mode == \"human\":\n",
    "            self.set_rendering()\n",
    "\n",
    "        # gym attributes\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.observation_space =  Dict({\n",
    "                                    \"x\": Discrete(self.max_shape[0]),              # Values from 0 to 10\n",
    "                                    \"y\": Discrete(self.max_shape[1]),              # Values from 0 to 10\n",
    "                                    \"phi\": Discrete(5),             # Values from 0 to 4\n",
    "                                    \"belief\": Box(low=0.0, high=1.0, shape=(len(self.deformations),), dtype=float)  # Probability vector\n",
    "                                })\n",
    "    \n",
    "    def step(self, a):\n",
    "\n",
    "        \"\"\"take action a from state s (if given) or from actual state of the maze \n",
    "        \n",
    "        return the next state, the reward, if the episode is terminated, if the episode is truncated, info\"\"\"\n",
    "        \n",
    "        x, y = self.agent_pos\n",
    "        phi = self.agent_orientation\n",
    "        x_, y_, phi_ = x, y, phi\n",
    "\n",
    "        actual_action = (a + phi) % 4\n",
    "        \n",
    "        if actual_action == 0:  # Move up\n",
    "            new_pos = [x - 1, y]\n",
    "        elif actual_action == 2:  # Move down\n",
    "            new_pos = [x + 1, y]\n",
    "        elif actual_action == 3:  # Move left\n",
    "            new_pos = [x, y - 1]\n",
    "        elif actual_action == 1:  # Move right\n",
    "            new_pos = [x, y + 1]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Action\")\n",
    "        \n",
    "        # Check if the new position is valid (inside the maze and not a wall)\n",
    "        if 0 < new_pos[0] < self.max_shape[0]-1 and 0 < new_pos[1] < self.max_shape[1]-1:\n",
    "            x_, y_ = new_pos\n",
    "            self.agent_pos = new_pos\n",
    "\n",
    "        phi_ = (phi + a) % 4\n",
    "        \n",
    "        self.agent_orientation = phi_\n",
    "        \n",
    "        terminated = np.all((x_,y_) == self.goal_pos)\n",
    "\n",
    "        if np.all((x_,y_) == self.goal_pos):\n",
    "            # if the agent is in the goal position\n",
    "            reward =  1            \n",
    "        elif np.all((x_,y_) == (x,y)):\n",
    "            # if the agent has not moved (only at the boundary of the maze)\n",
    "            reward =  -2 # -50/(self.max_shape[0]*self.max_shape[1])\n",
    "        elif self.maze[x_, y_] == 1:\n",
    "            # if the agent has entered a wall\n",
    "            reward =  -2 # -50/(self.max_shape[0]*self.max_shape[1])\n",
    "        elif self.maze[x_, y_] == 0:\n",
    "            # if the agent has moved to a free cell\n",
    "            reward =  -0.5 # -1/(self.max_shape[0]*self.max_shape[1])\n",
    "\n",
    "        info = {}\n",
    "        truncated = False \n",
    "                \n",
    "        self.timestep += 1\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        new_beleif = self.update_belief()\n",
    "\n",
    "        self.belief = new_beleif\n",
    "\n",
    "        obs = OrderedDict({\n",
    "                            \"x\": np.int64(x_),              # Values from 0 to 10\n",
    "                            \"y\": np.int64(y_),              # Values from 0 to 10\n",
    "                            \"phi\": np.int64(phi_),             # Values from 0 to 4\n",
    "                            \"belief\": self.belief , # Probability vector\n",
    "                        })\n",
    "\n",
    "        \n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "    def set_render_mode(self, mode):\n",
    "        self.render_mode = mode\n",
    "        if self.render_mode == \"human\":\n",
    "            self.set_rendering()\n",
    "        \n",
    "    def get_observation(self, s=None):\n",
    "\n",
    "        if s is None:\n",
    "            agent_pos = self.agent_pos\n",
    "            agent_orientation = self.agent_orientation\n",
    "        else:\n",
    "            prior_state = self.get_state()\n",
    "            self.set_deformed_maze(s[1])\n",
    "            agent_pos = s[0][:2]\n",
    "            agent_orientation = s[0][2]\n",
    "\n",
    "        ind = [agent_pos + a for a in [np.array([0,-1]),\n",
    "                                            np.array([-1,-1]),\n",
    "                                            np.array([-1,0]),\n",
    "                                            np.array([-1,+1]),\n",
    "                                            np.array([0,+1]),\n",
    "                                            np.array([+1,+1]),\n",
    "                                            np.array([+1,0]),\n",
    "                                            np.array([+1,-1])]]\n",
    "\n",
    "        agent_obs = np.array([self.maze[tuple(ind[i%8])] \n",
    "                                                for i in range(2*agent_orientation, 2*agent_orientation+5)])\n",
    "        \n",
    "        if s is not None:\n",
    "            self.set_state(prior_state)\n",
    "\n",
    "        \n",
    "        return agent_obs\n",
    "\n",
    "    def set_state(self, s):\n",
    "        theta0, theta1 = s[1][0], s[1][1]\n",
    "        self.theta = (theta0, theta1)\n",
    "        self.agent_pos = np.array(s[0][:2]) \n",
    "        self.agent_orientation = s[0][2]\n",
    "        self.set_deformed_maze(s[1])\n",
    "\n",
    "    def get_state(self):\n",
    "        return (self.agent_pos[0],self.agent_pos[1], self.agent_orientation), self.theta\n",
    "\n",
    "    def set_deformed_maze(self,thetas: tuple):\n",
    "        self.theta = thetas\n",
    "        self.maze = self.stretch_maze(thetas)\n",
    "        # self.goal_pos = self.maze.shape - np.array([thetas[1],thetas[0]])\n",
    "        self.goal_pos = self.original_maze.shape * np.array([thetas[1],thetas[0]])\n",
    "\n",
    "        canva1 = np.ones(self.max_shape, dtype=int)  # Start with walls\n",
    "        # Place the original maze in the canvas\n",
    "        canva1[1:self.maze.shape[0] + 1, 1:self.maze.shape[1] + 1] = self.maze\n",
    "\n",
    "        self.maze = canva1\n",
    "   \n",
    "    def stretch_maze(self, thetas):\n",
    "        scale_x, scale_y = thetas\n",
    "        maze = self.original_maze\n",
    "\n",
    "        original_height, original_width = maze.shape\n",
    "        # Calculate new dimensions\n",
    "        new_height = original_height * scale_y\n",
    "        new_width = original_width * scale_x\n",
    "        \n",
    "        # Create a new maze with stretched dimensions\n",
    "        stretched_maze = np.ones((new_height, new_width), dtype=int)\n",
    "\n",
    "        # Fill the new maze with values from the original maze\n",
    "        for i in range(original_height):\n",
    "            for j in range(original_width):\n",
    "                if maze[i, j] == 0:  # Path cell\n",
    "                    # Fill the corresponding region in the stretched maze\n",
    "                    stretched_maze[i*scale_y:(i+1)*scale_y, j*scale_x:(j+1)*scale_x] = 0\n",
    "\n",
    "        return stretched_maze\n",
    "    \n",
    "    def update_belief(self):\n",
    "        \"\"\"\"\n",
    "        perform update over theta\n",
    "        \n",
    "        $$b'_{x,a,o}(theta) = \\eta \\cdot p(o|x,theta) \\cdot b(theta)$$\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        new_belief = np.zeros_like(self.belief)\n",
    "        observation = self.get_observation()\n",
    "        pos = (self.agent_pos[0],self.agent_pos[1],self.agent_orientation)\n",
    "\n",
    "        for t, theta in enumerate(self.deformations):\n",
    "            P_o_s_theta = np.all(self.get_observation(s = (pos,theta)) == observation) # 0 or 1 \n",
    "\n",
    "            new_belief[t] = P_o_s_theta * self.belief[t]\n",
    "        \n",
    "        new_belief = new_belief / (np.sum(new_belief) + 1e-10)\n",
    "\n",
    "        return new_belief\n",
    "\n",
    "    def set_rendering(self):\n",
    "        self.screen_width = 800\n",
    "        self.screen_height = 600\n",
    "        pygame.init()  # Initialize all pygame modules\n",
    "        self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "        pygame.display.set_caption(\"Maze Environment\")\n",
    "        \n",
    "        # Handle key events\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                # Press 'r' to reset environment\n",
    "                if event.key == pygame.K_r:\n",
    "                    self.reset()\n",
    "                # Press 'q' to quit\n",
    "                elif event.key == pygame.K_q:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "                # Press 's' to save current state\n",
    "                elif event.key == pygame.K_s:\n",
    "                    self.save_state()\n",
    "                # Press space to pause/resume\n",
    "                elif event.key == pygame.K_SPACE:\n",
    "                    self.pause()\n",
    "                # Press arrow keys for manual control\n",
    "                elif event.key == pygame.K_LEFT:\n",
    "                    self.step(3)  # Left action\n",
    "                elif event.key == pygame.K_RIGHT:\n",
    "                    self.step(1)  # Right action\n",
    "                elif event.key == pygame.K_UP:\n",
    "                    self.step(0)  # Up action\n",
    "                elif event.key == pygame.K_DOWN:\n",
    "                    self.step(2)  # Down action\n",
    "\n",
    "        # Update display\n",
    "        pygame.display.flip()\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Render the maze using Pygame\"\"\"\n",
    "        \n",
    "        # Clear the screen\n",
    "        self.screen.fill((255, 255, 255))\n",
    "\n",
    "        # Draw the maze\n",
    "        cell_size = min(self.screen_width, self.screen_height) // max(self.max_shape)\n",
    "        for x in range(self.max_shape[0]):\n",
    "            for y in range(self.max_shape[1]):\n",
    "                if (x, y) == tuple(self.agent_pos):\n",
    "                    color = (255, 0, 0)  # Red for agent\n",
    "                elif (x, y) == tuple(self.goal_pos):\n",
    "                    color = (0, 255, 0)  # Green for goal\n",
    "                elif self.maze[x, y] == 1:\n",
    "                    color = (0, 0, 0)  # Black for walls\n",
    "                else:\n",
    "                    color = (255, 255, 255)  # White for free space\n",
    "                pygame.draw.rect(self.screen, color, (y * cell_size, x * cell_size, cell_size, cell_size))\n",
    "\n",
    "        # Add text for controls\n",
    "        font = pygame.font.Font(None, 36)\n",
    "        controls = [\n",
    "            \"Controls:\",\n",
    "            \"R - Reset\",\n",
    "            \"Q - Quit\",\n",
    "            \"Space - Pause/Resume\",\n",
    "            \"Arrows - Move agent\"\n",
    "        ]\n",
    "        \n",
    "        for i, text in enumerate(controls):\n",
    "            text_surface = font.render(text, True, (0, 0, 0))\n",
    "            self.screen.blit(text_surface, (self.screen_width - 200, 20 + i * 30))\n",
    "\n",
    "        # Handle events\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_r:\n",
    "                    self.reset()\n",
    "                elif event.key == pygame.K_q:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "                elif event.key == pygame.K_SPACE:\n",
    "                    self.pause()\n",
    "                elif event.key == pygame.K_LEFT:\n",
    "                    self.step(3,execute=True)\n",
    "                elif event.key == pygame.K_RIGHT:\n",
    "                    self.step(1,execute=True)\n",
    "                elif event.key == pygame.K_UP:\n",
    "                    self.step(0,execute=True)\n",
    "                elif event.key == pygame.K_DOWN:\n",
    "                    self.step(2,execute=True)\n",
    "                \n",
    "\n",
    "        # Update the display\n",
    "        pygame.display.flip()\n",
    "\n",
    "        # Capture the current frame and add it to the list of frames\n",
    "        frame = pygame.surfarray.array3d(self.screen)\n",
    "        self.frames.append(frame)\n",
    "\n",
    "    def reset(self, seed=42):\n",
    "        randomdeformation = random.choice(self.deformations)\n",
    "        self.agent_pos = [np.random.randint(1, self.max_shape[0]-1), np.random.randint(1, self.max_shape[1]-1)]\n",
    "        self.agent_orientation = random.choice(self.orientations)\n",
    "        self.set_deformed_maze(randomdeformation)\n",
    "        self.goal_pos = self.original_maze.shape * np.array([randomdeformation[1],randomdeformation[0]])\n",
    "        self.theta = randomdeformation\n",
    "        self.timestep = 0\n",
    "        \n",
    "        self.belief = np.ones(len(self.deformations)) / len(self.deformations)\n",
    "        obs = OrderedDict({\n",
    "                            \"x\": np.int64(self.agent_pos[0]),              # Values from 0 to 10\n",
    "                            \"y\": np.int64(self.agent_pos[1]),              # Values from 0 to 10\n",
    "                            \"phi\": np.int64(self.agent_orientation),             # Values from 0 to 4\n",
    "                            \"belief\": self.belief , # Probability vector\n",
    "                        })\n",
    "\n",
    "        return obs, {}\n",
    "    \n",
    "\n",
    "class FULLGYMGridEnvDeform(gym.Env):\n",
    "    \n",
    "    def __init__(self, maze, l0,h0,l1,h1,render_mode = None):\n",
    "\n",
    "        self.original_maze = maze\n",
    "        self.original_maze_shape = maze.shape\n",
    " \n",
    "        self.maze = maze\n",
    "        self.maze_shape = maze.shape\n",
    "\n",
    "        # list of possible actions\n",
    "        self.actions = [0,1,2,3]\n",
    "        # list of possible orientations\n",
    "        self.orientations = [0,1,2,3]\n",
    "        # list of possible observations\n",
    "        self.obs = list(itertools.product([0,1], repeat=5))\n",
    "        # list of possible deformations\n",
    "        self.deformations = [(i,j) for i in range(l0,h0) for j in range(l1,h1)]\n",
    "\n",
    "        \n",
    "        # space in which every maze lives (is a 2d matrix)\n",
    "        self.max_shape = self.original_maze.shape * np.array([h1-1,h0-1]) + np.array([2,2])\n",
    "        # list of states\n",
    "        self.states = [((x,y,phi),(i,j)) for x in range(1,self.max_shape[0]-1) for y in range(1,self.max_shape[1]-1) for phi in range(4) for i in range(l0,h0) for j in range(l1,h1)] \n",
    "        self.state_dict = {state : i for i, state in enumerate(self.states)}\n",
    "        \n",
    "        self.l0 = l0\n",
    "        self.h0 = h0\n",
    "        self.l1 = l1\n",
    "        self.h1 = h1\n",
    "\n",
    "        self.goal_pos = self.original_maze.shape - np.array([2,2])\n",
    "        \n",
    "        self.frames = []\n",
    "        self.reset()\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        if self.render_mode == \"human\":\n",
    "            self.set_rendering()\n",
    "\n",
    "        # gym attributes\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.observation_space =  Dict({\n",
    "                                    \"x\": Discrete(self.max_shape[0]),              # Values from 0 to 10\n",
    "                                    \"y\": Discrete(self.max_shape[1]),              # Values from 0 to 10\n",
    "                                    \"phi\": Discrete(5),                             # Values from 0 to 4\n",
    "                                    \"theta\": Box(low=min(l0,l1), high=max(h0,h1), shape=(2,), dtype=int)  # Probability vector\n",
    "                        # Probability vector\n",
    "                                })\n",
    "    \n",
    "    def step(self, a):\n",
    "\n",
    "        \"\"\"take action a from state s (if given) or from actual state of the maze \n",
    "        \n",
    "        return the next state, the reward, if the episode is terminated, if the episode is truncated, info\"\"\"\n",
    "        \n",
    "        x, y = self.agent_pos\n",
    "        phi = self.agent_orientation\n",
    "        x_, y_, phi_ = x, y, phi\n",
    "\n",
    "        actual_action = (a + phi) % 4\n",
    "        \n",
    "        if actual_action == 0:  # Move up\n",
    "            new_pos = [x - 1, y]\n",
    "        elif actual_action == 2:  # Move down\n",
    "            new_pos = [x + 1, y]\n",
    "        elif actual_action == 3:  # Move left\n",
    "            new_pos = [x, y - 1]\n",
    "        elif actual_action == 1:  # Move right\n",
    "            new_pos = [x, y + 1]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Action\")\n",
    "        \n",
    "        # Check if the new position is valid (inside the maze and not a wall)\n",
    "        if 0 < new_pos[0] < self.max_shape[0]-1 and 0 < new_pos[1] < self.max_shape[1]-1:\n",
    "            x_, y_ = new_pos\n",
    "            self.agent_pos = new_pos\n",
    "\n",
    "        phi_ = (phi + a) % 4\n",
    "        \n",
    "        self.agent_orientation = phi_\n",
    "        \n",
    "        terminated = np.all((x_,y_) == self.goal_pos)\n",
    "\n",
    "        if np.all((x_,y_) == self.goal_pos):\n",
    "            # if the agent is in the goal position\n",
    "            reward =  1            \n",
    "        elif np.all((x_,y_) == (x,y)):\n",
    "            # if the agent has not moved (only at the boundary of the maze)\n",
    "            reward =  -2 # -50/(self.max_shape[0]*self.max_shape[1])\n",
    "        elif self.maze[x_, y_] == 1:\n",
    "            # if the agent has entered a wall\n",
    "            reward =  -2 # -50/(self.max_shape[0]*self.max_shape[1])\n",
    "        elif self.maze[x_, y_] == 0:\n",
    "            # if the agent has moved to a free cell\n",
    "            reward =  -0.5 # -1/(self.max_shape[0]*self.max_shape[1])\n",
    "\n",
    "        info = {}\n",
    "        truncated = False \n",
    "                \n",
    "        self.timestep += 1\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        new_beleif = self.update_belief()\n",
    "\n",
    "        self.belief = new_beleif\n",
    "\n",
    "        obs = OrderedDict({\n",
    "                            \"x\": np.int64(x_),              # Values from 0 to 10\n",
    "                            \"y\": np.int64(y_),              # Values from 0 to 10\n",
    "                            \"phi\": np.int64(phi_),             # Values from 0 to 4\n",
    "                            \"theta\": np.array(self.theta) , # Probability vector\n",
    "                        })\n",
    "\n",
    "        \n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "    def set_render_mode(self, mode):\n",
    "        self.render_mode = mode\n",
    "        if self.render_mode == \"human\":\n",
    "            self.set_rendering()\n",
    "        \n",
    "    def get_observation(self, s=None):\n",
    "\n",
    "        if s is None:\n",
    "            agent_pos = self.agent_pos\n",
    "            agent_orientation = self.agent_orientation\n",
    "        else:\n",
    "            prior_state = self.get_state()\n",
    "            self.set_deformed_maze(s[1])\n",
    "            agent_pos = s[0][:2]\n",
    "            agent_orientation = s[0][2]\n",
    "\n",
    "        ind = [agent_pos + a for a in [np.array([0,-1]),\n",
    "                                            np.array([-1,-1]),\n",
    "                                            np.array([-1,0]),\n",
    "                                            np.array([-1,+1]),\n",
    "                                            np.array([0,+1]),\n",
    "                                            np.array([+1,+1]),\n",
    "                                            np.array([+1,0]),\n",
    "                                            np.array([+1,-1])]]\n",
    "\n",
    "        agent_obs = np.array([self.maze[tuple(ind[i%8])] \n",
    "                                                for i in range(2*agent_orientation, 2*agent_orientation+5)])\n",
    "        \n",
    "        if s is not None:\n",
    "            self.set_state(prior_state)\n",
    "\n",
    "        \n",
    "        return agent_obs\n",
    "\n",
    "    def set_state(self, s):\n",
    "        theta0, theta1 = s[1][0], s[1][1]\n",
    "        self.theta = (theta0, theta1)\n",
    "        self.agent_pos = np.array(s[0][:2]) \n",
    "        self.agent_orientation = s[0][2]\n",
    "        self.set_deformed_maze(s[1])\n",
    "\n",
    "    def get_state(self):\n",
    "        return (self.agent_pos[0],self.agent_pos[1], self.agent_orientation), self.theta\n",
    "\n",
    "    def set_deformed_maze(self,thetas: tuple):\n",
    "        self.theta = thetas\n",
    "        self.maze = self.stretch_maze(thetas)\n",
    "        # self.goal_pos = self.maze.shape - np.array([thetas[1],thetas[0]])\n",
    "        self.goal_pos = self.original_maze.shape * np.array([thetas[1],thetas[0]])\n",
    "\n",
    "        canva1 = np.ones(self.max_shape, dtype=int)  # Start with walls\n",
    "        # Place the original maze in the canvas\n",
    "        canva1[1:self.maze.shape[0] + 1, 1:self.maze.shape[1] + 1] = self.maze\n",
    "\n",
    "        self.maze = canva1\n",
    "   \n",
    "    def stretch_maze(self, thetas):\n",
    "        scale_x, scale_y = thetas\n",
    "        maze = self.original_maze\n",
    "\n",
    "        original_height, original_width = maze.shape\n",
    "        # Calculate new dimensions\n",
    "        new_height = original_height * scale_y\n",
    "        new_width = original_width * scale_x\n",
    "        \n",
    "        # Create a new maze with stretched dimensions\n",
    "        stretched_maze = np.ones((new_height, new_width), dtype=int)\n",
    "\n",
    "        # Fill the new maze with values from the original maze\n",
    "        for i in range(original_height):\n",
    "            for j in range(original_width):\n",
    "                if maze[i, j] == 0:  # Path cell\n",
    "                    # Fill the corresponding region in the stretched maze\n",
    "                    stretched_maze[i*scale_y:(i+1)*scale_y, j*scale_x:(j+1)*scale_x] = 0\n",
    "\n",
    "        return stretched_maze\n",
    "    \n",
    "    def update_belief(self):\n",
    "        \"\"\"\"\n",
    "        perform update over theta\n",
    "        \n",
    "        $$b'_{x,a,o}(theta) = \\eta \\cdot p(o|x,theta) \\cdot b(theta)$$\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        new_belief = np.zeros_like(self.belief)\n",
    "        observation = self.get_observation()\n",
    "        pos = (self.agent_pos[0],self.agent_pos[1],self.agent_orientation)\n",
    "\n",
    "        for t, theta in enumerate(self.deformations):\n",
    "            P_o_s_theta = np.all(self.get_observation(s = (pos,theta)) == observation) # 0 or 1 \n",
    "\n",
    "            new_belief[t] = P_o_s_theta * self.belief[t]\n",
    "        \n",
    "        new_belief = new_belief / (np.sum(new_belief) + 1e-10)\n",
    "\n",
    "        return new_belief\n",
    "\n",
    "    def set_rendering(self):\n",
    "        self.screen_width = 800\n",
    "        self.screen_height = 600\n",
    "        pygame.init()  # Initialize all pygame modules\n",
    "        self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "        pygame.display.set_caption(\"Maze Environment\")\n",
    "        \n",
    "        # Handle key events\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                # Press 'r' to reset environment\n",
    "                if event.key == pygame.K_r:\n",
    "                    self.reset()\n",
    "                # Press 'q' to quit\n",
    "                elif event.key == pygame.K_q:\n",
    "                    pygame.quit()\n",
    "                    self.render_mode = None\n",
    "                    return\n",
    "                # Press 's' to save current state\n",
    "                elif event.key == pygame.K_s:\n",
    "                    self.save_state()\n",
    "                # Press space to pause/resume\n",
    "                elif event.key == pygame.K_SPACE:\n",
    "                    self.pause()\n",
    "                # Press arrow keys for manual control\n",
    "                elif event.key == pygame.K_LEFT:\n",
    "                    self.step(3)  # Left action\n",
    "                elif event.key == pygame.K_RIGHT:\n",
    "                    self.step(1)  # Right action\n",
    "                elif event.key == pygame.K_UP:\n",
    "                    self.step(0)  # Up action\n",
    "                elif event.key == pygame.K_DOWN:\n",
    "                    self.step(2)  # Down action\n",
    "\n",
    "        # Update display\n",
    "        pygame.display.flip()\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Render the maze using Pygame\"\"\"\n",
    "        \n",
    "        # Clear the screen\n",
    "        self.screen.fill((255, 255, 255))\n",
    "\n",
    "        # Draw the maze\n",
    "        cell_size = min(self.screen_width, self.screen_height) // max(self.max_shape)\n",
    "        for x in range(self.max_shape[0]):\n",
    "            for y in range(self.max_shape[1]):\n",
    "                if (x, y) == tuple(self.agent_pos):\n",
    "                    color = (255, 0, 0)  # Red for agent\n",
    "                elif (x, y) == tuple(self.goal_pos):\n",
    "                    color = (0, 255, 0)  # Green for goal\n",
    "                elif self.maze[x, y] == 1:\n",
    "                    color = (0, 0, 0)  # Black for walls\n",
    "                else:\n",
    "                    color = (255, 255, 255)  # White for free space\n",
    "                pygame.draw.rect(self.screen, color, (y * cell_size, x * cell_size, cell_size, cell_size))\n",
    "\n",
    "        # Add text for controls\n",
    "        font = pygame.font.Font(None, 36)\n",
    "        controls = [\n",
    "            \"Controls:\",\n",
    "            \"R - Reset\",\n",
    "            \"Q - Quit\",\n",
    "            \"Space - Pause/Resume\",\n",
    "            \"Arrows - Move agent\"\n",
    "        ]\n",
    "        \n",
    "        for i, text in enumerate(controls):\n",
    "            text_surface = font.render(text, True, (0, 0, 0))\n",
    "            self.screen.blit(text_surface, (self.screen_width - 200, 20 + i * 30))\n",
    "\n",
    "        # Handle events\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_r:\n",
    "                    self.reset()\n",
    "                elif event.key == pygame.K_q:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "                elif event.key == pygame.K_SPACE:\n",
    "                    self.pause()\n",
    "                elif event.key == pygame.K_LEFT:\n",
    "                    self.step(3,execute=True)\n",
    "                elif event.key == pygame.K_RIGHT:\n",
    "                    self.step(1,execute=True)\n",
    "                elif event.key == pygame.K_UP:\n",
    "                    self.step(0,execute=True)\n",
    "                elif event.key == pygame.K_DOWN:\n",
    "                    self.step(2,execute=True)\n",
    "                \n",
    "\n",
    "        # Update the display\n",
    "        pygame.display.flip()\n",
    "\n",
    "        # Capture the current frame and add it to the list of frames\n",
    "        frame = pygame.surfarray.array3d(self.screen)\n",
    "        self.frames.append(frame)\n",
    "\n",
    "    def reset(self, seed=42):\n",
    "        randomdeformation = random.choice(self.deformations)\n",
    "        self.agent_pos = [np.random.randint(1, self.max_shape[0]-1), np.random.randint(1, self.max_shape[1]-1)]\n",
    "        self.agent_orientation = random.choice(self.orientations)\n",
    "        self.set_deformed_maze(randomdeformation)\n",
    "        self.goal_pos = self.original_maze.shape * np.array([randomdeformation[1],randomdeformation[0]])\n",
    "        self.theta = randomdeformation\n",
    "        self.timestep = 0\n",
    "        \n",
    "        self.belief = np.ones(len(self.deformations)) / len(self.deformations)\n",
    "        obs = OrderedDict({\n",
    "                            \"x\": np.int64(self.agent_pos[0]),              # Values from 0 to 10\n",
    "                            \"y\": np.int64(self.agent_pos[1]),              # Values from 0 to 10\n",
    "                            \"phi\": np.int64(self.agent_orientation),             # Values from 0 to 4\n",
    "                            \"theta\": np.array(self.theta) , # Probability vector\n",
    "                        })\n",
    "\n",
    "        return obs, {}\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from agents.dqn import DoubleDQNAgent\n",
    "from eval import eval_agent, all_data\n",
    "# maze size\n",
    "N = 2\n",
    "\n",
    "# thetas deformations (range(a,b),range(c,d))\n",
    "l0 = 1\n",
    "h0 = 10\n",
    "l1 = 1\n",
    "h1 = 10\n",
    "\n",
    "maze = np.load(f\"maze/maze_{N}.npy\")\n",
    "env = FULLGYMGridEnvDeform(maze,l0,h0,l1,h1, render_mode=\"human\")\n",
    "\n",
    "states = [((x,y,phi),(i,j)) for x in range(1,env.max_shape[0]-1) for y in range(1,env.max_shape[1]-1) for phi in range(4) for i in range(l0,h0) for j in range(l1,h1)] \n",
    "positions = [(x,y,phi) for x in range(1,env.max_shape[0]-1) for y in range(1,env.max_shape[1]-1) for phi in range(4)]\n",
    "actions = [0,1,2,3]\n",
    "obs = list(itertools.product([0,1], repeat=5))\n",
    "thetas = [(i,j) for i in range(l0,h0) for j in range(l1,h1)]\n",
    "\n",
    "state_dict = {state: i for i, state in enumerate(states)}\n",
    "position_dict = {position: i for i, position in enumerate(positions)}\n",
    "obs_dict = {obs : i for i, obs in enumerate(obs)}\n",
    "\n",
    "# Actions are: 0-listen, 1-open-left, 2-open-right\n",
    "lenS = len(states)\n",
    "lenP = len(positions)\n",
    "lenA = len(actions)\n",
    "lenO = len(obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatteo-nunziante\u001b[0m (\u001b[33madv_topics\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/flaccagora/Desktop/RoboSurgery/src/wandb/run-20241113_173521-pvxv9r1l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adv_topics/PPO/runs/pvxv9r1l' target=\"_blank\">confused-haze-7</a></strong> to <a href='https://wandb.ai/adv_topics/PPO' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adv_topics/PPO' target=\"_blank\">https://wandb.ai/adv_topics/PPO</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adv_topics/PPO/runs/pvxv9r1l' target=\"_blank\">https://wandb.ai/adv_topics/PPO/runs/pvxv9r1l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "import wandb\n",
    "\n",
    "class My_callback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(My_callback, self).__init__(verbose)\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.num_timesteps % 200 == 0:\n",
    "            self.training_env.reset()\n",
    "        return True\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        print(f\"Rollout end: {self.num_timesteps}\")\n",
    "        return True\n",
    "    \n",
    "\n",
    "total_timesteps = 100000\n",
    "batch_size = 2000\n",
    "n_steps = 2000\n",
    "\n",
    "config = {\n",
    "    \"policy_type\": \"MultiInputPolicy\",\n",
    "    \"env_name\": \"FULLGYMGridEnvDeform\",\n",
    "    \"defo_range\": (l0,h0,l1,h1),\n",
    "    \"maze_size\": N,\n",
    "    \"total_timesteps\": total_timesteps,\n",
    "    \"Batch_Size\": batch_size,\n",
    "    \"PPO n_steps\": n_steps\n",
    "}\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"PPO\",\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "    monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
    "    save_code=True,  # optional\n",
    ")\n",
    "\n",
    "callbacks = [My_callback(0), \n",
    "             WandbCallback(gradient_save_freq=100,\n",
    "                            model_save_path=f\"models/{run.id}\",\n",
    "                            verbose=2,\n",
    "                            ),\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to runs/pvxv9r1l/PPO_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b2035954e04bbaa0e3b97cda0bbb37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rollout end: 2000\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Rollout end: 2000\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 33       |\n",
      "|    ep_rew_mean     | -30      |\n",
      "| time/              |          |\n",
      "|    fps             | 47       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m env \u001b[38;5;241m=\u001b[39m DummyVecEnv([make_env])\n\u001b[1;32m     24\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiInputPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,env,n_steps\u001b[38;5;241m=\u001b[39mn_steps,batch_size\u001b[38;5;241m=\u001b[39mbatch_size,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/PPO_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.miniconda3/envs/rob/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/envs/rob/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda3/envs/rob/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:195\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    193\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 195\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda3/envs/rob/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/envs/rob/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/.miniconda3/envs/rob/lib/python3.9/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "Cell \u001b[0;32mIn[2], line 443\u001b[0m, in \u001b[0;36mFULLGYMGridEnvDeform.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m new_beleif \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_belief()\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbelief \u001b[38;5;241m=\u001b[39m new_beleif\n",
      "Cell \u001b[0;32mIn[2], line 601\u001b[0m, in \u001b[0;36mFULLGYMGridEnvDeform.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Render the maze using Pygame\"\"\"\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# Clear the screen\u001b[39;00m\n\u001b[0;32m--> 601\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;66;03m# Draw the maze\u001b[39;00m\n\u001b[1;32m    604\u001b[0m cell_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen_height) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_shape)\n",
      "\u001b[0;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "# n_steps (int)  The number of steps to run for each environment per update \n",
    "# (i.e. rollout buffer size is n_steps * n_envs\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
    "\n",
    "def make_env():\n",
    "    N = 2\n",
    "\n",
    "    # thetas deformations (range(a,b),range(c,d))\n",
    "    l0 = 1\n",
    "    h0 = 10\n",
    "    l1 = 1\n",
    "    h1 = 10\n",
    "    \n",
    "    maze = np.load(f\"maze/maze_{N}.npy\")\n",
    "    env = FULLGYMGridEnvDeform(maze,l0,h0,l1,h1, render_mode=\"human\")\n",
    "\n",
    "    env = Monitor(env)  # record stats such as returns\n",
    "    return env\n",
    "\n",
    "env = DummyVecEnv([make_env])\n",
    "\n",
    "\n",
    "model = PPO(\"MultiInputPolicy\",env,n_steps=n_steps,batch_size=batch_size,verbose=1,tensorboard_log=f\"runs/{run.id}\", device=\"cpu\")\n",
    "model.learn(total_timesteps,progress_bar=True, callback=callbacks)\n",
    "model.save(f\"models/PPO_{run.id}\")\n",
    "env.close()\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_MDP_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m     obs, reward, done, _, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m      8\u001b[0m         obs, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "Cell \u001b[0;32mIn[2], line 443\u001b[0m, in \u001b[0;36mFULLGYMGridEnvDeform.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m new_beleif \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_belief()\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbelief \u001b[38;5;241m=\u001b[39m new_beleif\n",
      "Cell \u001b[0;32mIn[2], line 601\u001b[0m, in \u001b[0;36mFULLGYMGridEnvDeform.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Render the maze using Pygame\"\"\"\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# Clear the screen\u001b[39;00m\n\u001b[0;32m--> 601\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;66;03m# Draw the maze\u001b[39;00m\n\u001b[1;32m    604\u001b[0m cell_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen_height) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_shape)\n",
      "\u001b[0;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "env = FULLGYMGridEnvDeform(maze,l0,h0,l1,h1,render_mode=\"human\")\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "while True:\n",
    "    action, _states = model.predict(obs, deterministic=False)\n",
    "    obs, reward, done, _, info = env.step(action)\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10_000,progress_bar=True)\n",
    "\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    vec_env.render()\n",
    "    # VecEnv resets automatically\n",
    "    # if done:\n",
    "    #   obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval value function MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "model.load(\"ppo_MDP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FULLGYMGridEnvDeform(maze,l0,h0,l1,h1,render_mode=\"human\")\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "while True:\n",
    "    action, _states = model.predict(obs, deterministic=False)\n",
    "    obs, reward, done, _, info = env.step(action)\n",
    "    if done:\n",
    "        OBS, _ = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fix orientation and deformation\n",
    "deformation = (2, 1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for orientation in range(4):\n",
    "    Value_matrix_plot = np.zeros(env.maze.shape) - np.inf\n",
    "    for s, state in enumerate(states):\n",
    "        if state[1] == deformation and state[0][2] == orientation:\n",
    "            Value_matrix_plot[state[0][0], state[0][1]] = state_value[s]\n",
    "    \n",
    "    ax = axes[orientation]\n",
    "    ax.imshow(Value_matrix_plot)\n",
    "    ax.set_title(f\"Orientation: {orientation}\")\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "\n",
    "plt.suptitle(\"Value Function Matrices for Different Orientations\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
