{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2]) torch.Size([2])\n",
      "Epoch 100, ELBO: -11546.2998\n",
      "Variational Mean: [ 0.8998552 -0.7438144]\n",
      "Variational Log Std: [-0.65705997 -0.69207567]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# Define the model and likelihood\n",
    "def likelihood(y, y_pred, noise_std):\n",
    "    return Normal(y_pred, noise_std).log_prob(y).sum()\n",
    "\n",
    "# Define the prior\n",
    "def prior(theta):\n",
    "    return Normal(0, 1).log_prob(theta).sum()\n",
    "\n",
    "# Variational distribution (q_lambda)\n",
    "class VariationalDistribution(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(VariationalDistribution, self).__init__()\n",
    "        self.mu = nn.Parameter(torch.zeros(dim))\n",
    "        self.log_sigma = nn.Parameter(torch.zeros(dim))\n",
    "    \n",
    "    def sample(self, num_samples=1):\n",
    "        epsilon = torch.randn(num_samples, self.mu.size(0), device=self.mu.device)\n",
    "        sigma = torch.exp(self.log_sigma)\n",
    "        return self.mu + sigma * epsilon  # Reparameterization trick\n",
    "    \n",
    "    def log_prob(self, theta):\n",
    "        sigma = torch.exp(self.log_sigma)\n",
    "        return Normal(self.mu, sigma).log_prob(theta).sum()\n",
    "\n",
    "# BBVI Training Loop\n",
    "def bbvi(X, y, variational_dist, num_epochs=5000, num_samples=10, lr=0.01):\n",
    "    optimizer = optim.Adam(variational_dist.parameters(), lr=lr)\n",
    "    noise_std = 0.1  # Observation noise\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Monte Carlo approximation of ELBO\n",
    "        elbo = 0\n",
    "        for _ in range(num_samples):\n",
    "            # Sample theta ~ q_lambda\n",
    "            theta = variational_dist.sample().squeeze() \n",
    "            \n",
    "            # Predicted y\n",
    "            y_pred = X @ theta\n",
    "            \n",
    "            # Log likelihood\n",
    "            log_likelihood = likelihood(y, y_pred, noise_std)\n",
    "            \n",
    "            # Log prior\n",
    "            log_prior = prior(theta)\n",
    "            \n",
    "            # Log variational density\n",
    "            log_q = variational_dist.log_prob(theta)\n",
    "            \n",
    "            # ELBO term\n",
    "            elbo += log_likelihood + log_prior - log_q\n",
    "        \n",
    "        elbo /= num_samples  # Average over samples\n",
    "        \n",
    "        # Maximize ELBO (minimize negative ELBO)\n",
    "        loss = -elbo\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, ELBO: {elbo.item():.4f}\")\n",
    "\n",
    "    return variational_dist\n",
    "\n",
    "# Synthetic data for Bayesian linear regression\n",
    "torch.manual_seed(42)\n",
    "X = torch.randn(100, 2)  # 100 samples, 2 features\n",
    "true_theta = torch.tensor([2.0, -1.0])\n",
    "y = X @ true_theta + 0.1 * torch.randn(100)\n",
    "print(X.shape, true_theta.shape)\n",
    "\n",
    "# Initialize variational distribution\n",
    "variational_dist = VariationalDistribution(dim=2)\n",
    "\n",
    "# Train with BBVI\n",
    "trained_dist = bbvi(X, y, variational_dist, num_epochs=100, num_samples=10, lr=0.01)\n",
    "\n",
    "# Output trained variational parameters\n",
    "print(f\"Variational Mean: {trained_dist.mu.detach().numpy()}\")\n",
    "print(f\"Variational Log Std: {trained_dist.log_sigma.detach().numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 71\u001b[0m\n\u001b[1;32m     56\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# # Parameters for prior and variational distribution\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# input_dim = 2\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# prior_mu = torch.zeros(input_dim)  # Prior mean\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Plot the prior and variational distribution\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[43mplot_distributions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariational_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_theta\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_theta\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m, in \u001b[0;36mplot_distributions\u001b[0;34m(variational_dist, prior_mu, prior_sigma)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_distributions\u001b[39m(variational_dist, prior_mu, prior_sigma):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    Plots the prior and variational distributions in 2D.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    - prior_sigma: Tensor (2,) representing the diagonal standard deviation of the prior.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mprior_mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis function only works for input_dim=2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Convert prior mean and std to NumPy arrays\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     prior_mu \u001b[38;5;241m=\u001b[39m prior_mu\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "def plot_distributions(variational_dist, prior_mu, prior_sigma):\n",
    "    \"\"\"\n",
    "    Plots the prior and variational distributions in 2D.\n",
    "\n",
    "    Parameters:\n",
    "    - variational_dist: VariationalDistribution instance.\n",
    "    - prior_mu: Tensor (2,) representing the mean of the prior.\n",
    "    - prior_sigma: Tensor (2,) representing the diagonal standard deviation of the prior.\n",
    "    \"\"\"\n",
    "    assert prior_mu.shape[0] == 2, \"This function only works for input_dim=2\"\n",
    "    \n",
    "    # Convert prior mean and std to NumPy arrays\n",
    "    prior_mu = prior_mu.detach().cpu().numpy()\n",
    "    prior_sigma = prior_sigma.detach().cpu().numpy()\n",
    "    prior_cov = np.diag(prior_sigma**2)\n",
    "    \n",
    "    # Define the variational mean and covariance\n",
    "    variational_mu = variational_dist.mu.detach().cpu().numpy()\n",
    "    variational_sigma = torch.exp(variational_dist.log_sigma).detach().cpu().numpy()\n",
    "    variational_cov = np.diag(variational_sigma**2)\n",
    "    \n",
    "    # Define grid for plotting\n",
    "    x, y = np.linspace(-3, 3, 100), np.linspace(-3, 3, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    pos = np.stack([X, Y], axis=-1)\n",
    "    \n",
    "    # Compute prior distribution pdf\n",
    "    prior_pdf = np.exp(\n",
    "        -0.5 * np.einsum(\"...i,ij,...j->...\", pos - prior_mu, np.linalg.inv(prior_cov), pos - prior_mu)\n",
    "    )\n",
    "    prior_pdf /= np.sqrt((2 * np.pi)**2 * np.linalg.det(prior_cov))  # Normalize\n",
    "    \n",
    "    # Compute variational distribution pdf\n",
    "    var_pdf = np.exp(\n",
    "        -0.5 * np.einsum(\"...i,ij,...j->...\", pos - variational_mu, np.linalg.inv(variational_cov), pos - variational_mu)\n",
    "    )\n",
    "    var_pdf /= np.sqrt((2 * np.pi)**2 * np.linalg.det(variational_cov))  # Normalize\n",
    "    \n",
    "    # Plot the contours\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contour(X, Y, prior_pdf, levels=8, cmap='Blues', alpha=0.7)\n",
    "    plt.contour(X, Y, var_pdf, levels=8, cmap='Reds', alpha=0.7)\n",
    "    plt.scatter(prior_mu[0], prior_mu[1], c='blue', marker='x', label='Prior Mean')\n",
    "    plt.scatter(variational_mu[0], variational_mu[1], c='red', marker='x', label='Variational Mean')\n",
    "    \n",
    "    # Add labels\n",
    "    plt.title(\"Prior and Variational Distribution (2D)\")\n",
    "    plt.xlabel(\"Theta_1\")\n",
    "    plt.ylabel(\"Theta_2\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# # Parameters for prior and variational distribution\n",
    "# input_dim = 2\n",
    "# prior_mu = torch.zeros(input_dim)  # Prior mean\n",
    "# prior_sigma = torch.ones(input_dim)  # Prior standard deviation\n",
    "\n",
    "# # Initialize the variational distribution\n",
    "# variational_dist = VariationalDistribution(input_dim)\n",
    "\n",
    "# # Update variational distribution with some arbitrary values (e.g., after training)\n",
    "# variational_dist.mu.data = torch.tensor([1.0, -0.5])  # Example mean\n",
    "# variational_dist.log_sigma.data = torch.log(torch.tensor([0.7, 0.4]))  # Example log std dev\n",
    "\n",
    "# Plot the prior and variational distribution\n",
    "plot_distributions(variational_dist, true_theta[0], true_theta[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'environment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menvironment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Grid\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'environment'"
     ]
    }
   ],
   "source": [
    "from environment.env import Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# my VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# Define the model and likelihood\n",
    "def likelihood(y, y_pred, noise_std):\n",
    "    return Normal(y_pred, noise_std).log_prob(y).sum()\n",
    "\n",
    "# Define the prior\n",
    "def prior(theta):\n",
    "    return Normal(0, 1).log_prob(theta).sum()\n",
    "\n",
    "# Variational distribution (q_lambda)\n",
    "class VariationalDistribution(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(VariationalDistribution, self).__init__()\n",
    "        self.mu = nn.Parameter(torch.zeros(dim))\n",
    "        self.log_sigma = nn.Parameter(torch.zeros(dim))\n",
    "    \n",
    "    def sample(self, num_samples=1):\n",
    "        epsilon = torch.randn(num_samples, self.mu.size(0), device=self.mu.device)\n",
    "        sigma = torch.exp(self.log_sigma)\n",
    "        return self.mu + sigma * epsilon  # Reparameterization trick\n",
    "    \n",
    "    def log_prob(self, theta):\n",
    "        sigma = torch.exp(self.log_sigma)\n",
    "        return Normal(self.mu, sigma).log_prob(theta).sum()\n",
    "\n",
    "# BBVI Training Loop\n",
    "def bbvi(X, y, variational_dist, num_epochs=5000, num_samples=10, lr=0.01):\n",
    "    optimizer = optim.Adam(variational_dist.parameters(), lr=lr)\n",
    "    noise_std = 0.1  # Observation noise\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Monte Carlo approximation of ELBO\n",
    "        elbo = 0\n",
    "        for _ in range(num_samples):\n",
    "            # Sample theta ~ q_lambda\n",
    "            theta = variational_dist.sample().squeeze() \n",
    "                        \n",
    "            # Log likelihood p(x|theta)\n",
    "            log_likelihood = obs_model(x,theta,y)\n",
    "            \n",
    "            # Log prior\n",
    "            log_prior = prior(theta)\n",
    "            \n",
    "            # Log variational density\n",
    "            log_q = variational_dist.log_prob(theta)\n",
    "            \n",
    "            # ELBO term\n",
    "            elbo += log_likelihood + log_prior - log_q\n",
    "        \n",
    "        elbo /= num_samples  # Average over samples\n",
    "        \n",
    "        # Maximize ELBO (minimize negative ELBO)\n",
    "        loss = -elbo\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, ELBO: {elbo.item():.4f}\")\n",
    "\n",
    "    return variational_dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vae inference style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, obs_model):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        \n",
    "        self.obs_model = obs_model\n",
    "        self.obs_model.eval()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        # this should be replaced by the observation model p(x|z)\n",
    "        return self.obs_model(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.BCELoss(recon_x, x, reduction='sum') #\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) # replace with KL for arbitrary prior\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_model = nn.Sequential(\n",
    "    nn.Linear(20, 400),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(400, 784),\n",
    "    nn.Sigmoid()\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(obs_model=obs_model).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
