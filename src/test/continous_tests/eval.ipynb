{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.append(os.path.abspath('../..'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flaccagora/.miniconda3/envs/robogym/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-119.95 118.70361199222202\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import DQN\n",
    "from environment.env import Grid\n",
    "from utils.checkpoints import find_last_checkpoint\n",
    "\n",
    "obstacles = [((0.14625, 0.3325), (0.565, 0.55625)), \n",
    "             ((0.52875, 0.5375), (0.7375, 0.84125)), \n",
    "             ((0.0, 0.00125), (0.01625, 0.99125)), \n",
    "             ((0.0075, 0.00125), (0.99875, 0.04)), \n",
    "             ((0.98875, 0.0075), (0.99875, 1.0)), \n",
    "             ((0.00125, 0.9825), (0.99875, 1.0))]\n",
    "\n",
    "env = Grid(\n",
    "    obstacles=obstacles, \n",
    "    shear_range=(-.2, .2),\n",
    "    stretch_range=(.4,1),\n",
    "    render_mode=\"human\"\n",
    ")\n",
    "\n",
    "last_checkpoint = find_last_checkpoint(\"../../agents/pretrained/MDP/DQN_continous_1vbmjd2a\")\n",
    "model = DQN.load(f\"../../agents/pretrained/MDP/DQN_continous_1vbmjd2a/{last_checkpoint}\", env=env)\n",
    "\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20, deterministic=True)\n",
    "print(mean_reward, std_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flaccagora/.miniconda3/envs/robogym/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-65.3 215.1961547054222\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO\n",
    "from environment.env import Grid\n",
    "from utils.checkpoints import find_last_checkpoint\n",
    "\n",
    "obstacles = [((0.14625, 0.3325), (0.565, 0.55625)), \n",
    "             ((0.52875, 0.5375), (0.7375, 0.84125)), \n",
    "             ((0.0, 0.00125), (0.01625, 0.99125)), \n",
    "             ((0.0075, 0.00125), (0.99875, 0.04)), \n",
    "             ((0.98875, 0.0075), (0.99875, 1.0)), \n",
    "             ((0.00125, 0.9825), (0.99875, 1.0))]\n",
    "\n",
    "env = Grid(\n",
    "    obstacles=obstacles, \n",
    "    shear_range=(-.2, .2),\n",
    "    stretch_range=(.4,1),\n",
    "    render_mode=\"human\"\n",
    ")\n",
    "\n",
    "last_checkpoint = find_last_checkpoint(\"../../agents/pretrained/MDP/PPO_continous_38pvesev\")\n",
    "model = PPO.load(f\"../../agents/pretrained/MDP/PPO_continous_38pvesev/{last_checkpoint}\", env=env)\n",
    "\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20, deterministic=True)\n",
    "print(mean_reward, std_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 50\u001b[0m\n\u001b[1;32m     40\u001b[0m             time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     42\u001b[0m env \u001b[38;5;241m=\u001b[39m Grid(\n\u001b[1;32m     43\u001b[0m     obstacles\u001b[38;5;241m=\u001b[39mobstacles, \n\u001b[1;32m     44\u001b[0m     shear_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m.2\u001b[39m, \u001b[38;5;241m.2\u001b[39m),\n\u001b[1;32m     45\u001b[0m     stretch_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m.4\u001b[39m,\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     46\u001b[0m     render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m )\n\u001b[0;32m---> 50\u001b[0m \u001b[43meval_agent_mdp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 36\u001b[0m, in \u001b[0;36meval_agent_mdp\u001b[0;34m(agent, env, num_episodes, max_episode_steps, render)\u001b[0m\n\u001b[1;32m     27\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m render:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# print(\"State\", s)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# print(\"Action: \", best_action)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# print(\"Reward:     \" + str(totalReward) + \"  \")\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# print(\"Next State: \", next_state)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# print(\"\\n\")\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m s \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     39\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/RoboSurgery/src/environment/env.py:1604\u001b[0m, in \u001b[0;36mGrid.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mset_caption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeformed and Original Gridworld\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1603\u001b[0m     \u001b[38;5;66;03m# Fill background with white\u001b[39;00m\n\u001b[0;32m-> 1604\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWHITE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[38;5;66;03m# Compute the bounding box of the deformed grid\u001b[39;00m\n\u001b[1;32m   1607\u001b[0m     corners \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1608\u001b[0m         np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m   1609\u001b[0m         np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_size[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m   1610\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_size,\n\u001b[1;32m   1611\u001b[0m         np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_size[\u001b[38;5;241m1\u001b[39m]]),\n\u001b[1;32m   1612\u001b[0m     ]\n",
      "\u001b[0;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "def eval_agent_mdp(agent,env,num_episodes,max_episode_steps,render):\n",
    "    \"\"\"Returns\n",
    "        - episode_transition: list of list of tuples (s,a,r,s',done), t[i] is the ith episode\n",
    "        - beliefs: list of beliefs at each time step \n",
    "    \"\"\"\n",
    "    \n",
    "    # if render:\n",
    "    #     env.set_rendering()\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "\n",
    "        s, _ = env.reset()\n",
    "\n",
    "        totalReward = 0.0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        episode_transitions = []\n",
    "\n",
    "        while not done and steps < max_episode_steps:\n",
    "\n",
    "            best_action, _ = agent.predict(s,deterministic=True)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(best_action)\n",
    "\n",
    "            totalReward += reward            \n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "            if render:\n",
    "                # print(\"State\", s)\n",
    "                # print(\"Action: \", best_action)\n",
    "                # print(\"Reward:     \" + str(totalReward) + \"  \")\n",
    "                # print(\"Next State: \", next_state)\n",
    "                # print(\"\\n\")\n",
    "            \n",
    "                env.render()\n",
    "\n",
    "            s = next_state\n",
    "            steps += 1\n",
    "            time.sleep(0.1)\n",
    "\n",
    "env = Grid(\n",
    "    obstacles=obstacles, \n",
    "    shear_range=(-.2, .2),\n",
    "    stretch_range=(.4,1),\n",
    "    render_mode=\"human\"\n",
    ")\n",
    "\n",
    "\n",
    "eval_agent_mdp(model,env,10,100,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robogym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
