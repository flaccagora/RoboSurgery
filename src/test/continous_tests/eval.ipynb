{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.append(os.path.abspath('../..'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import DQN\n",
    "from environment.env import Grid\n",
    "from utils.checkpoints import find_last_checkpoint\n",
    "\n",
    "obstacles = [((0.14625, 0.3325), (0.565, 0.55625)), \n",
    "             ((0.52875, 0.5375), (0.7375, 0.84125)), \n",
    "             ((0.0, 0.00125), (0.01625, 0.99125)), \n",
    "             ((0.0075, 0.00125), (0.99875, 0.04)), \n",
    "             ((0.98875, 0.0075), (0.99875, 1.0)), \n",
    "             ((0.00125, 0.9825), (0.99875, 1.0))]\n",
    "\n",
    "env = Grid(\n",
    "    obstacles=obstacles, \n",
    "    shear_range=(-.2, .2),\n",
    "    stretch_range=(.4,1),\n",
    "    render_mode=\"human\"\n",
    ")\n",
    "\n",
    "last_checkpoint = find_last_checkpoint(\"../../agents/pretrained/MDP/DQN_continous_1vbmjd2a\")\n",
    "model = DQN.load(f\"../../agents/pretrained/MDP/DQN_continous_1vbmjd2a/{last_checkpoint}\", env=env)\n",
    "\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20, deterministic=True)\n",
    "print(mean_reward, std_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = \"PPO_continous_qmqiws99\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flaccagora/.miniconda3/envs/robogym/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-87.41599998734891 177.39701484049272\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO\n",
    "from environment.env import Grid\n",
    "from utils.checkpoints import find_last_checkpoint\n",
    "\n",
    "obstacles = [((0.14625, 0.3325), (0.565, 0.55625)), \n",
    "             ((0.52875, 0.5375), (0.7375, 0.84125)), \n",
    "             ((0.0, 0.00125), (0.01625, 0.99125)), \n",
    "             ((0.0075, 0.00125), (0.99875, 0.04)), \n",
    "             ((0.98875, 0.0075), (0.99875, 1.0)), \n",
    "             ((0.00125, 0.9825), (0.99875, 1.0))]\n",
    "\n",
    "env = Grid(\n",
    "    obstacles=obstacles, \n",
    "    shear_range=(-.2, .2),\n",
    "    stretch_range=(.4,1),\n",
    "    render_mode=\"human\"\n",
    ")\n",
    "\n",
    "last_checkpoint = find_last_checkpoint(f\"../../agents/pretrained/MDP/{run}\")\n",
    "model = PPO.load(f\"../../agents/pretrained/MDP/{run}/{last_checkpoint}\", env=env)\n",
    "\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20, deterministic=True)\n",
    "print(mean_reward, std_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Grid' object has no attribute 'save_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 50\u001b[0m\n\u001b[1;32m     40\u001b[0m             time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     42\u001b[0m env \u001b[38;5;241m=\u001b[39m Grid(\n\u001b[1;32m     43\u001b[0m     obstacles\u001b[38;5;241m=\u001b[39mobstacles, \n\u001b[1;32m     44\u001b[0m     shear_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m.2\u001b[39m, \u001b[38;5;241m.2\u001b[39m),\n\u001b[1;32m     45\u001b[0m     stretch_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m.4\u001b[39m,\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     46\u001b[0m     render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m )\n\u001b[0;32m---> 50\u001b[0m \u001b[43meval_agent_mdp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 23\u001b[0m, in \u001b[0;36meval_agent_mdp\u001b[0;34m(agent, env, num_episodes, max_episode_steps, render)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m steps \u001b[38;5;241m<\u001b[39m max_episode_steps:\n\u001b[1;32m     21\u001b[0m     best_action, _ \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mpredict(s,deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 23\u001b[0m     next_state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     totalReward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward            \n\u001b[1;32m     27\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/Desktop/RoboSurgery/src/environment/env.py:1746\u001b[0m, in \u001b[0;36mGrid.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1746\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1748\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/Desktop/RoboSurgery/src/environment/env.py:1716\u001b[0m, in \u001b[0;36mGrid.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;66;03m# Press 's' to save current state\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m event\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m==\u001b[39m pygame\u001b[38;5;241m.\u001b[39mK_s:\n\u001b[0;32m-> 1716\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_state\u001b[49m()\n\u001b[1;32m   1717\u001b[0m \u001b[38;5;66;03m# Press space to pause/resume\u001b[39;00m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m event\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m==\u001b[39m pygame\u001b[38;5;241m.\u001b[39mK_SPACE:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Grid' object has no attribute 'save_state'"
     ]
    }
   ],
   "source": [
    "def eval_agent_mdp(agent,env,num_episodes,max_episode_steps,render):\n",
    "    \"\"\"Returns\n",
    "        - episode_transition: list of list of tuples (s,a,r,s',done), t[i] is the ith episode\n",
    "        - beliefs: list of beliefs at each time step \n",
    "    \"\"\"\n",
    "    \n",
    "    # if render:\n",
    "    #     env.set_rendering()\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "\n",
    "        s, _ = env.reset()\n",
    "\n",
    "        totalReward = 0.0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        episode_transitions = []\n",
    "\n",
    "        while not done and steps < max_episode_steps:\n",
    "\n",
    "            best_action, _ = agent.predict(s,deterministic=True)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(best_action)\n",
    "\n",
    "            totalReward += reward            \n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "            if render:\n",
    "                # print(\"State\", s)\n",
    "                # print(\"Action: \", best_action)\n",
    "                # print(\"Reward:     \" + str(totalReward) + \"  \")\n",
    "                # print(\"Next State: \", next_state)\n",
    "                # print(\"\\n\")\n",
    "            \n",
    "                env.render()\n",
    "\n",
    "            s = next_state\n",
    "            steps += 1\n",
    "            time.sleep(0.1)\n",
    "\n",
    "env = Grid(\n",
    "    obstacles=obstacles, \n",
    "    shear_range=(-.2, .2),\n",
    "    stretch_range=(.4,1),\n",
    "    render_mode=\"human\"\n",
    ")\n",
    "\n",
    "\n",
    "eval_agent_mdp(model,env,10,100,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robogym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
