{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath('../..'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import DQN\n",
    "from environment.env import Grid\n",
    "from utils.checkpoints import find_last_checkpoint\n",
    "\n",
    "obstacles = [((0.14625, 0.3325), (0.565, 0.55625)), \n",
    "             ((0.52875, 0.5375), (0.7375, 0.84125)), \n",
    "             ((0.0, 0.00125), (0.01625, 0.99125)), \n",
    "             ((0.0075, 0.00125), (0.99875, 0.04)), \n",
    "             ((0.98875, 0.0075), (0.99875, 1.0)), \n",
    "             ((0.00125, 0.9825), (0.99875, 1.0))]\n",
    "\n",
    "env = Grid(\n",
    "    obstacles=obstacles, \n",
    "    shear_range=(-.2, .2),\n",
    "    stretch_range=(.4,1),\n",
    "    render_mode=\"human\"\n",
    ")\n",
    "\n",
    "last_checkpoint = find_last_checkpoint(\"../../agents/pretrained/MDP/DQN_continous_1vbmjd2a\")\n",
    "model = DQN.load(f\"../../agents/pretrained/MDP/DQN_continous_1vbmjd2a/{last_checkpoint}\", env=env)\n",
    "\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20, deterministic=True)\n",
    "print(mean_reward, std_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = \"PPO_continous_\" + \"o6hdii7n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flaccagora/.miniconda3/envs/robogym/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-58.6 206.59415831528247\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO\n",
    "from environment.env import Grid\n",
    "from utils.checkpoints import find_last_checkpoint\n",
    "\n",
    "obstacles = [((0.14625, 0.3325), (0.565, 0.55625)), \n",
    "             ((0.52875, 0.5375), (0.7375, 0.84125)), \n",
    "             ((0.0, 0.00125), (0.01625, 0.99125)), \n",
    "             ((0.0075, 0.00125), (0.99875, 0.04)), \n",
    "             ((0.98875, 0.0075), (0.99875, 1.0)), \n",
    "             ((0.00125, 0.9825), (0.99875, 1.0))]\n",
    "\n",
    "env = Grid(\n",
    "    obstacles=obstacles, \n",
    "    shear_range=(-.2, .2),\n",
    "    stretch_range=(.4,1),\n",
    "    render_mode=\"human\"\n",
    ")\n",
    "\n",
    "last_checkpoint = find_last_checkpoint(f\"../../agents/pretrained/MDP/{run}\")\n",
    "model = PPO.load(f\"../../agents/pretrained/MDP/{run}/{last_checkpoint}\", env=env)\n",
    "\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20, deterministic=True)\n",
    "print(mean_reward, std_reward)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def eval_agent_mdp(agent,env,num_episodes,max_episode_steps,render):\n",
    "    \"\"\"Returns\n",
    "        - episode_transition: list of list of tuples (s,a,r,s',done), t[i] is the ith episode\n",
    "        - beliefs: list of beliefs at each time step \n",
    "    \"\"\"\n",
    "    for i in range(num_episodes):\n",
    "        s, _ = env.reset()\n",
    "\n",
    "        totalReward = 0.0\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        while not done and steps < max_episode_steps:\n",
    "\n",
    "            best_action, _ = agent.predict(s,deterministic=True)\n",
    "            next_state, reward, terminated, truncated, info = env.step(best_action)\n",
    "            totalReward += reward            \n",
    "\n",
    "            done = terminated or truncated\n",
    "            if render:\n",
    "                # print(\"State\", s)\n",
    "                # print(\"Action: \", best_action)\n",
    "                # print(\"Reward:     \" + str(totalReward) + \"  \")\n",
    "                # print(\"Next State: \", next_state)\n",
    "                # print(\"\\n\")\n",
    "                env.render()\n",
    "\n",
    "            s = next_state\n",
    "            steps += 1\n",
    "            time.sleep(0.02)\n",
    "\n",
    "env = Grid(\n",
    "    render_mode=\"human\"\n",
    ")\n",
    "\n",
    "eval_agent_mdp(model,env,10,100,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, None, None, None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Grid(\n",
    "    render_mode=\"human\"\n",
    ")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, None, None, None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset(4256)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(2)\n",
    "np.random.randint(0, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robogym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
