{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Dict, Discrete, Box\n",
    "import torch\n",
    "\n",
    "import os \n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from environment.env import ObservableDeformedGridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POMDPDeformedGridworld(ObservableDeformedGridworld):\n",
    "    def __init__(self):\n",
    "        super(POMDPDeformedGridworld, self).__init__(render_mode='human')\n",
    "        \n",
    "        self.observation_space = Dict({\n",
    "            'obs': Discrete(2),\n",
    "            'pos': Box(low=-np.inf, high=np.inf, shape=(2,), dtype=np.float32)\n",
    "        })\n",
    "    \n",
    "    def reset(self):\n",
    "        state, _ = super().reset()\n",
    "        pomdp_state = {\n",
    "            'obs': torch.tensor([1], dtype=torch.float32) if super().is_in_obstacle(state['pos']) else torch.tensor([0], dtype=torch.float32),\n",
    "            'pos': torch.tensor(state['pos'], dtype=torch.float32)\n",
    "            }\n",
    "        return pomdp_state, {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        state, reward, terminated, truncated, info = super().step(action)\n",
    "        pomdp_state = {\n",
    "            'obs': torch.tensor([1], dtype=torch.float32) if super().is_in_obstacle(state['pos']) else torch.tensor([0], dtype=torch.float32),\n",
    "            'pos': torch.tensor(state['pos'], dtype=torch.float32)\n",
    "            }\n",
    "        return pomdp_state, reward, terminated,truncated, info\n",
    "    \n",
    "    def get_state(self):\n",
    "        pomdp_state = {\n",
    "            'obs': torch.tensor([1], dtype=torch.float32) if super().is_in_obstacle(self.state) else torch.tensor([0], dtype=torch.float32),\n",
    "            'pos': torch.tensor(self.state, dtype=torch.float32)\n",
    "            }\n",
    "        return pomdp_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'obs': tensor([0.]), 'pos': tensor([0.0786, 0.6706])}, {})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pomdp_env = POMDPDeformedGridworld()\n",
    "pomdp_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Probability of observation o in position p and deformation theta\n",
    "# P(o|p,theta) = p(\\phi(o,p,theta))\n",
    "# \n",
    "# but i only have a belief of the deformation theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POMDPAgent():\n",
    "    def __init__(self, env: POMDPDeformedGridworld, obs_model):\n",
    "        self.env = env\n",
    "        self.obs_model = obs_model\n",
    "        \n",
    "        # assuming gaussian belief over 4 parameter deformation matrix\n",
    "        self.belief = {\n",
    "            'mean': torch.tensor([0.5, 0.0, 0.0, 0.5], dtype=torch.float32, requires_grad=True),\n",
    "            'cov': torch.eye(4) * 0.1\n",
    "        }\n",
    "    \n",
    "    def act(self):\n",
    "        action = input('Enter action: ')\n",
    "        pomdp_state, reward, terminated,truncated, info = self.env.step(int(action))\n",
    "        print(pomdp_state)\n",
    "        self.belief_update(pomdp_state)\n",
    "    \n",
    "\n",
    "    def render_act(self):\n",
    "        \"\"\"For testing belief convergence\"\"\"\n",
    "        pomdp_state = self.env.get_state()\n",
    "        # self.belief_update(pomdp_state)\n",
    "        self.env.render()\n",
    "        \n",
    "\n",
    "    def belief_update(self, state):\n",
    "        # assuming gaussian belief over 4 parameter deformation matrix\n",
    "        # belief update using kalman filter\n",
    "        mu = self.belief['mean']\n",
    "        P = self.belief['cov']\n",
    "        # predicted observation using current obs_model\n",
    "        actual_obs = state['obs']\n",
    "        print(state['pos'].unsqueeze(0),actual_obs.unsqueeze(0),mu.unsqueeze(0))\n",
    "        rescale_obs = self.obs_model(state['pos'].unsqueeze(0),actual_obs.unsqueeze(0),mu.unsqueeze(0))\n",
    "        p_o = torch.distributions.Bernoulli(rescale_obs)\n",
    "\n",
    "        rescale_obs.backward()\n",
    "        H = mu.grad.clone()\n",
    "        # mu.grad.zero_()\n",
    "\n",
    "        R = torch.eye(1) * 0.01\n",
    "\n",
    "        mu_new, P_new = self.kalman_update(mu, P, actual_obs, H, R)\n",
    "        # mu_new.grad.zero_()\n",
    "        \n",
    "        self.belief['mean'] = mu_new\n",
    "        self.belief['cov'] = P_new\n",
    "        \n",
    "\n",
    "    def kalman_update(self, mu, P, z, H, R):\n",
    "        \"\"\"\n",
    "        Perform the Kalman update step for the belief.\n",
    "        - mu: Current belief mean\n",
    "        - P: Current belief covariance\n",
    "        - z: Observation (0 or 1)\n",
    "        - H: Jacobian of the observation model\n",
    "        - R: Observation noise covariance\n",
    "        \"\"\"\n",
    "        # Compute the Kalman gain\n",
    "        K = torch.matmul(P, H.T) * torch.inverse(H @ P @ H.T + R).squeeze(0)\n",
    "        \n",
    "        # Update the belief\n",
    "        mu_new = mu + K * (z - H @ mu)\n",
    "        P_new = P - K @ H * P\n",
    "        \n",
    "        mu_new.retain_grad()\n",
    "        mu_new.grad.zero_()\n",
    "        print('mu_new:', mu_new)\n",
    "        return mu_new, P_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7683/38765658.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  obs_model.load_state_dict(torch.load('obs_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "        self.f1 = nn.Linear(7, 128)\n",
    "        self.f2 = nn.Linear(128, 128)\n",
    "        self.f3 = nn.Linear(128, 128)\n",
    "        self.f4 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, pos,deform_obs,theta):\n",
    "        x = torch.cat([pos,deform_obs,theta], dim=1)\n",
    "        x = F.relu(self.f1(x))\n",
    "        x = F.relu(self.f2(x))\n",
    "        x = F.relu(self.f3(x))\n",
    "        x = F.sigmoid(self.f4(x))\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "obs_model = NN()\n",
    "\n",
    "# Load the model\n",
    "obs_model.load_state_dict(torch.load('obs_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9985]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_model.eval()\n",
    "obs_model(torch.tensor([0.5, 0.5]).unsqueeze(0), torch.tensor([1.0]).unsqueeze(0), torch.tensor([0.5, 0.0, 0.0, 0.5]).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = POMDPAgent(pomdp_env, obs_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_act\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[61], line 22\u001b[0m, in \u001b[0;36mPOMDPAgent.render_act\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender_act\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     21\u001b[0m     pomdp_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mget_state()\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/RoboSurgery/src/environment/env.py:1435\u001b[0m, in \u001b[0;36mObservableDeformedGridworld.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1432\u001b[0m         pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mset_caption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeformed and Original Gridworld\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;66;03m# Fill background with white\u001b[39;00m\n\u001b[0;32m-> 1435\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWHITE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[38;5;66;03m# Compute the bounding box of the deformed grid\u001b[39;00m\n\u001b[1;32m   1438\u001b[0m     corners \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1439\u001b[0m         np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m   1440\u001b[0m         np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_size[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m   1441\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_size,\n\u001b[1;32m   1442\u001b[0m         np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_size[\u001b[38;5;241m1\u001b[39m]]),\n\u001b[1;32m   1443\u001b[0m     ]\n",
      "\u001b[0;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        agent.render_act()\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': tensor([ 0.5422, -0.0016,  0.0369,  0.2813], grad_fn=<AddBackward0>),\n",
       " 'cov': tensor([[0.0423, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0423, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0423, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0423]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.belief\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Belief Update\n",
    "\n",
    "If b(M)b(M) is approximately Gaussian:\n",
    "b(M)∼N(μ,Σ),\n",
    "b(M)∼N(μ,Σ),\n",
    "\n",
    "where μμ is the mean vector and ΣΣ is the covariance matrix of the parameters, the update can be performed analytically or approximately using a filtering approach.\n",
    "Steps:\n",
    "\n",
    "    Observation Model Approximation: Linearize O(o∣M)O(o∣M) around the current mean μμ using a first-order Taylor expansion:\n",
    "    O(o∣M)≈O(o∣μ)+∇MO(o∣μ)⊤(M−μ),\n",
    "    O(o∣M)≈O(o∣μ)+∇M​O(o∣μ)⊤(M−μ),\n",
    "\n",
    "    where ∇MO(o∣μ)∇M​O(o∣μ) is the Jacobian of O(o∣M)O(o∣M) at μμ.\n",
    "\n",
    "    Update Mean and Covariance: Using an Extended Kalman Filter (EKF) approach:\n",
    "        Predict step: Keep μμ and ΣΣ unchanged before incorporating the observation.\n",
    "        Update step:\n",
    "        K=ΣH⊤(HΣH⊤+R)−1,\n",
    "        K=ΣH⊤(HΣH⊤+R)−1,\n",
    "        μ′=μ+K(o−h(μ)),\n",
    "        μ′=μ+K(o−h(μ)),\n",
    "        Σ′=Σ−KHΣ,\n",
    "        Σ′=Σ−KHΣ, where:\n",
    "            H=∇Mh(M)∣M=μH=∇M​h(M)∣M=μ​ is the Jacobian of the observation function h(M)h(M),\n",
    "            RR is the observation noise covariance,\n",
    "            KK is the Kalman gain.\n",
    "\n",
    "    Update Belief: Replace (μ,Σ)(μ,Σ) with (μ′,Σ′)(μ′,Σ′)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Vector Outputs (Multi-class or Multi-dimensional)\n",
    "\n",
    "If h(M)h(M) outputs a vector instead of a scalar, compute the Jacobian as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for vector-valued outputs (e.g., multi-class observation model)\n",
    "mu = torch.tensor([0.5, 0.5, 0.1, 0.1], requires_grad=True)\n",
    "obs_pred = model(mu)  # Assume model now outputs a vector, e.g., shape (3,)\n",
    "jacobian = torch.zeros(obs_pred.shape[0], mu.shape[0])  # Preallocate Jacobian (3x4)\n",
    "\n",
    "# Compute Jacobian row by row\n",
    "for i in range(obs_pred.shape[0]):\n",
    "    model.zero_grad()  # Clear gradients\n",
    "    obs_pred[i].backward(retain_graph=True)  # Backprop for the i-th output\n",
    "    jacobian[i] = mu.grad  # Store the gradient\n",
    "    mu.grad.zero_()  # Reset gradients for next iteration\n",
    "\n",
    "print(\"Jacobian (multi-dimensional output):\")\n",
    "print(jacobian)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated belief mean: tensor([-1.6752, -0.7079,  0.0612, -2.2841], grad_fn=<AddBackward0>)\n",
      "Updated belief covariance: tensor([[0.8676, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8676, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8676, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.8676]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the observation model (neural network)\n",
    "class ObservationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ObservationModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 16)  # Input: 4D vector, hidden: 16 neurons\n",
    "        self.fc2 = nn.Linear(16, 1)  # Output: scalar (0 or 1)\n",
    "        self.activation = nn.Sigmoid()  # Sigmoid output (probability-like)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Kalman filter update step\n",
    "def kalman_update(mu, P, z, H, R):\n",
    "    \"\"\"\n",
    "    Perform the Kalman update step for the belief.\n",
    "    - mu: Current belief mean\n",
    "    - P: Current belief covariance\n",
    "    - z: Observation (0 or 1)\n",
    "    - H: Jacobian of the observation model\n",
    "    - R: Observation noise covariance\n",
    "    \"\"\"\n",
    "    # Compute the Kalman gain\n",
    "    K = torch.matmul(P, H.T) * torch.inverse(H @ P @ H.T + R).squeeze(0)\n",
    "    \n",
    "    # Update the belief\n",
    "    mu_new = mu + K * (z - H @ mu)\n",
    "    P_new = P - K @ H * P\n",
    "    \n",
    "    return mu_new, P_new\n",
    "\n",
    "# Initialize the neural network\n",
    "model = ObservationModel()\n",
    "\n",
    "# Current belief (Gaussian) parameters: mean (mu) and covariance (P)\n",
    "mu = torch.tensor([0.5, 0.56, 0.1, 0.1], dtype=torch.float32, requires_grad=True)  # Example mean of the belief\n",
    "P = torch.eye(4)  # Example covariance (identity for simplicity)\n",
    "\n",
    "# The observation model is a neural network, so let's predict the observation\n",
    "obs_pred = model(mu)  # Predicted observation for the current belief\n",
    "\n",
    "# For simplicity, assume we have an observation (0 or 1)\n",
    "z = torch.tensor([1.0])  # Example observed value\n",
    "\n",
    "# Compute the Jacobian (derivative of the observation model w.r.t. mu)\n",
    "obs_pred.backward()  # Compute the gradients of the prediction wrt mu\n",
    "H = mu.grad.clone()   # The Jacobian is just the gradient of the output with respect to the input (mu)\n",
    "\n",
    "# Reset gradients for the next iteration\n",
    "mu.grad.zero_()\n",
    "\n",
    "# Assume a simple observation noise model (R)\n",
    "R = torch.tensor([[0.01]])  # Small observation noise\n",
    "\n",
    "# Perform Kalman update\n",
    "mu_new, P_new = kalman_update(mu, P, z, H, R)\n",
    "\n",
    "print(\"Updated belief mean:\", mu_new)\n",
    "print(\"Updated belief covariance:\", P_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.8000, 3.3302, 0.7704, 1.9053], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from a Gaussian distribution with mean mu and covariance P using pytorch\n",
    "def sample_gaussian(mu, P):\n",
    "    \"\"\"\n",
    "    Sample from a Gaussian distribution with mean mu and covariance P.\n",
    "    \n",
    "    Parameters:\n",
    "        mu (torch.Tensor): Mean of the Gaussian (vector).\n",
    "        P (torch.Tensor): Covariance matrix of the Gaussian.\n",
    "    \n",
    "    Returns:\n",
    "        sample (torch.Tensor): Sampled value from the Gaussian.\n",
    "    \"\"\"\n",
    "    # Generate a sample from a standard normal distribution\n",
    "    sample = torch.randn_like(mu)\n",
    "    \n",
    "    # Perform Cholesky decomposition of the covariance matrix\n",
    "    L = torch.linalg.cholesky(P)\n",
    "    \n",
    "    # Transform the sample to match the desired covariance\n",
    "    sample = mu + L @ sample\n",
    "    \n",
    "    return sample\n",
    "\n",
    "# Example usage\n",
    "sample_gaussian(mu_new, P_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape P torch.Size([4, 4])\n",
      "shape H torch.Size([4])\n",
      "shape R torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Compute the Kalman gain\n",
    "print('shape P', P.shape)\n",
    "print('shape H', H.shape)\n",
    "print('shape R', R.shape)\n",
    "K = torch.matmul(P, H.T) * torch.inverse(H @ P @ H.T + R).squeeze(0)\n",
    "\n",
    "# Update the belief\n",
    "mu_new = mu + K * (z - H @ mu)\n",
    "P_new = P - K @ H * P\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1259, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.1259, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1259, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.1259]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K@ H * P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "both arguments to matmul need to be at least 1D, but they are 0D and 2D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m P_new \u001b[38;5;241m=\u001b[39m P \u001b[38;5;241m-\u001b[39m \u001b[43mK\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: both arguments to matmul need to be at least 1D, but they are 0D and 2D"
     ]
    }
   ],
   "source": [
    "P_new = P - K @ H @ P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0114, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([87.7421])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.inverse(H @ P @ H.T + R).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0451, -0.1558,  0.2192,  0.0721])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_pred = model(mu)  # Predicted observation for the current belief\n",
    "\n",
    "# For simplicity, assume we have an observation (0 or 1)\n",
    "z = torch.tensor([1.0])  # Example observed value\n",
    "\n",
    "# Compute the Jacobian (derivative of the observation model w.r.t. mu)\n",
    "obs_pred.backward()  # Compute the gradients of the prediction wrt mu\n",
    "H = mu.grad.clone()  # The Jacobian is\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  6.,  9., 12.],\n",
       "        [12.,  9.,  6.,  3.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[1.0, 2.0, 3.0, 4.0],[4,3,2,1]], dtype=torch.float32, requires_grad=True)\n",
    "t\n",
    "a = 3\n",
    "\n",
    "t*a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robogym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
