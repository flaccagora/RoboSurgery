{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "import os \n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "\n",
    "from environment.env import POMDPDeformedGridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBSERVATION_TYPE = 'cardinal' # 'cardinal' or  'single'\n",
    "BELIEF_UPDATE = 'particlefilters' # 'discrete', 'variational' or 'particlefilters'\n",
    "DISCRETIZATION = 2000 # \n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MDP solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flaccagora/.miniconda3/envs/robogym/lib/python3.11/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run = \"PPO_continous_\" + \"enh53x0u\"\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO\n",
    "from environment.env import Grid\n",
    "from utils.checkpoints import find_last_checkpoint\n",
    "\n",
    "env = Grid(\n",
    "    shear_range=(-.2, .2),\n",
    "    stretch_range=(.4,1),\n",
    "    render_mode=\"human\"\n",
    ")\n",
    "\n",
    "last_checkpoint = find_last_checkpoint(f\"../../agents/pretrained/MDP/{run}\")\n",
    "model = PPO.load(f\"../../agents/pretrained/MDP/{run}/{last_checkpoint}\", env=env)\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Obs Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obs_model(obs_type):\n",
    "    from observation_model.obs_model import singleNN, cardinalNN\n",
    "\n",
    "    if obs_type == 'single':\n",
    "        obs_model = singleNN()\n",
    "        obs_model.load_state_dict(torch.load(\"obs_model_4.pth\", weights_only=True,map_location=torch.device('cpu')))\n",
    "    elif obs_type == 'cardinal':\n",
    "        obs_model = cardinalNN()\n",
    "        obs_model.load_state_dict(torch.load(\"obs_model_cardinal_4.pth\", weights_only=True))\n",
    "    else:\n",
    "        raise ValueError(\"Observation type not recognized\")\n",
    "    \n",
    "    return obs_model\n",
    "obs_model = load_obs_model(OBSERVATION_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pomdp agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "\n",
    "class POMDPAgent():\n",
    "    \n",
    "    def __init__(self,mdp_agent:PPO, pomdp_env: POMDPDeformedGridworld, discretization=10, update='discrete', obs_model=None, debug=False):\n",
    "        assert isinstance(pomdp_env, POMDPDeformedGridworld), f'Invalid environment type {type(pomdp_env)}'\n",
    "        self.pomdp_env = pomdp_env\n",
    "        self.mdp_agent = mdp_agent\n",
    "        self.update = update\n",
    "\n",
    "        if update == 'discrete' or update == 'discrete_exact':\n",
    "            stretch = np.linspace(.4, 1, discretization)\n",
    "            shear = np.linspace(-.2,.2, discretization)\n",
    "            xa,ya,yb,xb = np.meshgrid(stretch, shear,shear,stretch) # , shear, shear\n",
    "            positions = np.column_stack([xa.ravel(),ya.ravel(),yb.ravel(),xb.ravel()]),\n",
    "            positions = torch.tensor(np.array(positions), dtype=torch.float32)\n",
    "            self.belief_points = positions.squeeze()\n",
    "            self.belief_values = torch.ones(self.belief_points.shape[0], dtype=torch.float32, requires_grad=False) / len(positions)\n",
    "            \n",
    "            self.original_def = env.transformation_matrix[0][0], env.transformation_matrix[1][1]\n",
    "\n",
    "        if update == 'discrete': \n",
    "            assert obs_model is not None, f'Need an observation model for discrete belief update, given {obs_model}'\n",
    "            self.obs_model = obs_model\n",
    "            self.belief_update = self.discrete_belief_update\n",
    "            print('Discrete belief update with observation model - SHEAR allowed')\n",
    "        elif update == 'discrete_exact':\n",
    "            self.belief_update = self.exact_belief_update\n",
    "            raise NotImplementedError('Exact belief update not implemented here')\n",
    "        elif update == 'variational':\n",
    "            from utils.belief import BetaVariationalBayesianInference\n",
    "            assert obs_model is not None, f'Need an observation model for variational belief update, given {obs_model}'\n",
    "            self.VI = BetaVariationalBayesianInference(obs_model, input_dim=2, latent_dim=4, debug=debug)\n",
    "            self.obs_model = obs_model\n",
    "\n",
    "            self.belief_update = self.variational_belief_update\n",
    "            self.X_history = [self.pomdp_env.get_state()['pos']]\n",
    "            self.y_history = [self.pomdp_env.get_state()['obs']]\n",
    "        elif update == 'particlefilters':\n",
    "            from utils.belief import BayesianParticleFilter\n",
    "            self.obs_model = obs_model\n",
    "            self.n_particles = discretization\n",
    "            self.PF = BayesianParticleFilter(f = obs_model, n_particles=self.n_particles, theta_dim=4)\n",
    "            self.PF.initialize_particles()\n",
    "            self.belief_update = self.particle_filter_belief_update\n",
    "            \n",
    "            self.X_history = [self.pomdp_env.get_state()['pos']]\n",
    "            self.y_history = [self.pomdp_env.get_state()['obs']]\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Invalid belief update method')\n",
    "        \n",
    "        self.debug = debug\n",
    "        if self.debug:\n",
    "            print(f'Using {update} belief update method')\n",
    "    \n",
    "    def predict(self, s, deterministic=True):\n",
    "        \n",
    "        self.belief_update(s)\n",
    "        pos = s['pos']\n",
    "        if self.update == 'discrete_exact' or self.update == 'discrete':\n",
    "            # theta = self.belief_points[self.belief_values.argmax()] # QMDP\n",
    "            theta = self.belief_points[torch.multinomial(self.belief_values, 1).item()] # Thompson sampling\n",
    "        elif self.update == 'variational':\n",
    "            theta = self.VI.sample_latent(1).squeeze().clone().detach().numpy() # variational Thompson sampling\n",
    "        elif self.update == 'particlefilters':\n",
    "            mean, var = self.PF.estimate_posterior()\n",
    "            theta = torch.distributions.Normal(torch.tensor(mean), torch.tensor(var).sqrt()+1e-6).sample().squeeze()\n",
    "            # theta = torch.tensor(mean, dtype=torch.float32)\n",
    "\n",
    "        s = OrderedDict({'pos': pos, 'theta': theta})\n",
    "        action = self.mdp_agent.predict(s, deterministic=deterministic)\n",
    "\n",
    "        self.on_precidt_callback()\n",
    "        return action\n",
    "    \n",
    "    def discrete_belief_update(self, pomdp_state):\n",
    "        \"\"\"discrete belief update\"\"\"\n",
    "        pos = pomdp_state['pos']\n",
    "        obs = pomdp_state['obs']\n",
    "\n",
    "        batch_pos = pos.repeat(len(self.belief_points), 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = self.obs_model(batch_pos,self.belief_points)\n",
    "\n",
    "        likelihood = torch.exp(torch.distributions.Bernoulli(predictions).log_prob(obs))\n",
    "        if len(likelihood.shape) == 2:\n",
    "            likelihood = likelihood.sum(dim=1)\n",
    "\n",
    "        tmp = likelihood.squeeze() * self.belief_values\n",
    "        self.belief_values = tmp  / tmp.sum()\n",
    "\n",
    "    def exact_belief_update(self, pomdp_state):\n",
    "        \"\"\"discrete belief update\"\"\"\n",
    "        obs = pomdp_state['obs']\n",
    "        pos = pomdp_state['pos']\n",
    "\n",
    "        def f():\n",
    "            likelihood = []\n",
    "            for x in self.belief_points:\n",
    "                try:\n",
    "                    self.pomdp_env.set_deformation([x[0], x[1]],[0,0]) # stretch, shear format\n",
    "                    likelihood.append(torch.all(torch.tensor(self.pomdp_env.observe(list(pos))) == obs))\n",
    "                except:\n",
    "                    raise ValueError('Invalid belief point x', x)\n",
    "            self.pomdp_env.set_deformation(self.original_def, [0,0])\n",
    "            return torch.tensor(likelihood, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        likelihood = f()\n",
    "        self.belief_values =  likelihood * self.belief_values\n",
    "        self.belief_values = self.belief_values / self.belief_values.sum()\n",
    "\n",
    "    def variational_belief_update(self, pomdp_state):\n",
    "        self.X_history.append(pomdp_state['pos'])\n",
    "        self.y_history.append(pomdp_state['obs'])\n",
    "\n",
    "        # X = posizione dell'agente (x,y)\n",
    "        X = torch.stack(self.X_history)\n",
    "\n",
    "        # ossevrazioni dell'agente negli stati pos=(x,y)\n",
    "        y = torch.stack(self.y_history)\n",
    "\n",
    "        # Create and fit the model\n",
    "        self.VI.fit(X, y, n_epochs=10, lr=0.05)\n",
    "\n",
    "    def particle_filter_belief_update(self, pomdp_state):\n",
    "        self.X_history.append(pomdp_state['pos'])\n",
    "        self.y_history.append(pomdp_state['obs'])\n",
    "\n",
    "        # X = posizione dell'agente (x,y)\n",
    "        X = torch.stack(self.X_history[-1:])\n",
    "\n",
    "        # ossevrazioni dell'agente negli stati pos=(x,y)\n",
    "        y = torch.stack(self.y_history[-1:])\n",
    "\n",
    "        # X, y = pomdp_state['pos'].unsqueeze(0), pomdp_state['obs'].unsqueeze(0)\n",
    "\n",
    "        # Create and fit the model\n",
    "        self.PF.update(X, y)\n",
    "\n",
    "    def on_precidt_callback(self):\n",
    "        if self.debug:\n",
    "            self.print_stats()\n",
    "        \n",
    "    def print_stats(self):\n",
    "        if self.update == 'discrete':\n",
    "            # print(f'Belief shape: {self.belief_values.shape}')\n",
    "            # print(f'Belief points shape: {self.belief_points.shape}')\n",
    "            # print(f'Belief max: {self.belief_points[self.belief_values.argmax()]}')\n",
    "            # print(f'Belief sum: {self.belief_values.sum()}')\n",
    "            # print(f'Belief entropy: {torch.distributions.Categorical(probs=self.belief_values).entropy()}')\n",
    "            # print(\"\\n\")\n",
    "            self.entropy = torch.distributions.Categorical(probs=self.belief_values).entropy()\n",
    "        elif self.update == 'variational':\n",
    "            # print(f'Variational inference: {self.VI.entropy()}')\n",
    "            # print(self.VI.get_posterior_params())\n",
    "            # print(\"\\n\")\n",
    "            self.entropy = self.VI.entropy()\n",
    "        elif self.update == 'particlefilters':\n",
    "            # print(f'Particle filter: {self.PF.estimate_posterior()[1]}')\n",
    "            # print(\"\\n\")\n",
    "            self.entropy = None# self.PF.entropy()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.X_history = [self.pomdp_env.get_state()['pos']]\n",
    "        self.y_history = [self.pomdp_env.get_state()['obs']]\n",
    "        self.entropy = None\n",
    "\n",
    "        if self.update == 'discrete':\n",
    "            self.belief_values = torch.ones(self.belief_points.shape[0], dtype=torch.float32, requires_grad=False) / len(self.belief_points)\n",
    "        elif self.update == 'variational':\n",
    "            del self.VI\n",
    "            from utils.belief import BetaVariationalBayesianInference\n",
    "            self.VI = BetaVariationalBayesianInference(self.obs_model, input_dim=2, latent_dim=4, debug=self.debug)\n",
    "        elif self.update == 'particlefilters':\n",
    "            del self.PF\n",
    "            from utils.belief import BayesianParticleFilter\n",
    "            self.PF = BayesianParticleFilter(f = self.obs_model, n_particles=self.n_particles, theta_dim=4)\n",
    "            self.PF.initialize_particles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminated\n",
      "[[0.7250188149317318, 0.1756523169150862], [-0.17704976369063757, 0.5876180343994203]]\n"
     ]
    }
   ],
   "source": [
    "pomdp_env = POMDPDeformedGridworld(obs_type=OBSERVATION_TYPE)\n",
    "pomdp_env.reset()\n",
    "\n",
    "agent = POMDPAgent(model, pomdp_env, update=BELIEF_UPDATE, obs_model=obs_model,discretization=DISCRETIZATION, debug=DEBUG)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        action, _  = agent.predict(pomdp_env.get_state(), deterministic=False)\n",
    "        _, _ , terminated, truncated, _ = pomdp_env.step(action)\n",
    "        pomdp_env.render()\n",
    "        if terminated or truncated:\n",
    "            print('Terminated' if terminated else 'Truncated')\n",
    "            print(pomdp_env.transformation_matrix)\n",
    "            break\n",
    "    except:\n",
    "        print('Terminated' if terminated else 'Truncated')\n",
    "\n",
    "        break\n",
    "pomdp_env.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:26<00:00, 38.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange \n",
    "def eval_agent_mdp(agent,env,num_episodes):\n",
    "    \"\"\"Returns\n",
    "        - episode_transition: list of list of tuples (s,a,r,s',done), t[i] is the ith episode\n",
    "        - beliefs: list of beliefs at each time step \n",
    "    \"\"\"\n",
    "    transitions = []\n",
    "    for i in trange(num_episodes):\n",
    "        s, _ = env.reset()\n",
    "\n",
    "        totalReward = 0.0\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        ep_transitions = []\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            best_action, _ = agent.predict(s,deterministic=True)\n",
    "            next_state, reward, terminated, truncated, info = env.step(best_action)\n",
    "            totalReward += reward            \n",
    "\n",
    "            done = terminated or truncated\n",
    "            s = next_state\n",
    "            steps += 1\n",
    "\n",
    "            ep_transitions.append((s, best_action, reward, next_state, terminated, truncated))\n",
    "    \n",
    "        transitions.append(ep_transitions)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return transitions\n",
    "\n",
    "env = Grid(\n",
    "    render_mode=\"rgb_array\"\n",
    ")\n",
    "\n",
    "transitions = eval_agent_mdp(model,env,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target reached 992 out of 1000 episodes\n",
      "Mean episode Reward:  -20.6195\n",
      "Mean number of steps:  30.148\n"
     ]
    }
   ],
   "source": [
    "# chek how many times the agent has reached the goal\n",
    "reached_goal = 0\n",
    "for ep in transitions:\n",
    "    if ep[-1][-2] == True:\n",
    "        reached_goal += 1\n",
    "print(\"Target reached\", reached_goal, \"out of\", len(transitions), \"episodes\")    \n",
    "\n",
    "# check the mean reward\n",
    "mean_reward = 0\n",
    "for ep in transitions:\n",
    "    mean_reward += sum([t[2] for t in ep])\n",
    "mean_reward /= len(transitions)\n",
    "print(\"Mean episode Reward: \", mean_reward)\n",
    "\n",
    "# check the mean number of steps\n",
    "mean_steps = 0\n",
    "for ep in transitions:\n",
    "    mean_steps += len(ep)\n",
    "mean_steps /= len(transitions)\n",
    "print(\"Mean number of steps: \", mean_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def eval_agent_pomdp(agent:POMDPAgent,env: POMDPDeformedGridworld,num_episodes):\n",
    "    \"\"\"Returns\n",
    "        - episode_transition: list of list of tuples (s,a,r,s',done), t[i] is the ith episode\n",
    "        - beliefs: list of beliefs at each time step \n",
    "    \"\"\"\n",
    "\n",
    "    assert agent.debug, 'Agent must be in debug mode to evaluate'\n",
    "\n",
    "    transitions = []\n",
    "    entropy = []\n",
    "\n",
    "    for i in trange(num_episodes):\n",
    "\n",
    "        agent.reset()\n",
    "        s, _ = env.reset()\n",
    "\n",
    "        totalReward = 0.0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        episode_transitions = []\n",
    "        episode_entropy = []\n",
    "        \n",
    "        while not done:\n",
    "\n",
    "            best_action, _ = agent.predict(s, deterministic=True)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(best_action)\n",
    "            \n",
    "            done = terminated or truncated\n",
    "            s = next_state\n",
    "\n",
    "            steps += 1\n",
    "            totalReward += reward\n",
    "            episode_transitions.append((s, best_action, reward, next_state, terminated, truncated))\n",
    "            # episode_entropy.append(agent.entropy.item())\n",
    "\n",
    "        transitions.append(episode_transitions)\n",
    "        # entropy.append(episode_entropy)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return transitions, entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBSERVATION_TYPE = 'cardinal' # 'single' or 'cardinal' \n",
    "RENDER_MODE = 'rgb_array' # 'rgb_array' or 'human'\n",
    "BELIEF_UPDATE = 'particlefilters' # 'variational' 'discrete' or 'particlefilters'\n",
    "DISCRETIZATION = 7000\n",
    "DEBUG = True\n",
    "\n",
    "obs_model = load_obs_model(OBSERVATION_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using particlefilters belief update method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:35<00:00,  2.84it/s]\n"
     ]
    }
   ],
   "source": [
    "pomdp_env = POMDPDeformedGridworld(obs_type=OBSERVATION_TYPE, render_mode=RENDER_MODE)\n",
    "agent = POMDPAgent(model, pomdp_env, update=BELIEF_UPDATE, obs_model=obs_model,discretization=DISCRETIZATION, debug=DEBUG)\n",
    "transitions, entropy = eval_agent_pomdp(agent,pomdp_env, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target reached 96 out of 100 episodes\n",
      "Mean episode Reward:  -45.405\n",
      "Mean number of steps:  58.74\n"
     ]
    }
   ],
   "source": [
    "def explain_transitions(transitions):\n",
    "    # chek how many times the agent has reached the goal\n",
    "    reached_goal = 0\n",
    "    for ep in transitions:\n",
    "        if ep[-1][-2] == True:\n",
    "            reached_goal += 1\n",
    "    print(\"Target reached\", reached_goal, \"out of\", len(transitions), \"episodes\")    \n",
    "\n",
    "    # check the mean reward\n",
    "    mean_reward = 0\n",
    "    for ep in transitions:\n",
    "        mean_reward += sum([t[2] for t in ep])\n",
    "    mean_reward /= len(transitions)\n",
    "    print(\"Mean episode Reward: \", mean_reward)\n",
    "\n",
    "    # check the mean number of steps\n",
    "    mean_steps = 0\n",
    "    for ep in transitions:\n",
    "        mean_steps += len(ep)\n",
    "    mean_steps /= len(transitions)\n",
    "    print(\"Mean number of steps: \", mean_steps)\n",
    "\n",
    "explain_transitions(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_entropy(entropy):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for i in range(len(entropy)):\n",
    "        plt.plot(entropy[i])\n",
    "\n",
    "    plt.xlabel('Time step')\n",
    "    plt.ylabel('Entropy')\n",
    "    plt.title('Entropy over time')\n",
    "\n",
    "plot_entropy(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target reached 96 out of 100 episodes\n",
      "Mean episode Reward:  -20.865\n",
      "Mean number of steps:  38.7\n"
     ]
    }
   ],
   "source": [
    "reached_goal = 0\n",
    "truncated_index = []\n",
    "for i, ep in enumerate(transitions):\n",
    "    if ep[-1][-2] == True:\n",
    "        reached_goal += 1\n",
    "    else:\n",
    "        truncated_index.append(i)\n",
    "print(\"Target reached\", reached_goal, \"out of\", len(transitions), \"episodes\")    \n",
    "\n",
    "# check the mean reward\n",
    "mean_reward = 0\n",
    "for i, ep in enumerate(transitions):\n",
    "    if i in truncated_index:\n",
    "        continue\n",
    "    mean_reward += sum([t[2] for t in ep])\n",
    "mean_reward /= len(transitions)\n",
    "print(\"Mean episode Reward: \", mean_reward)\n",
    "\n",
    "# check the mean number of steps\n",
    "mean_steps = 0\n",
    "for i, ep in enumerate(transitions):\n",
    "    if i in truncated_index:\n",
    "        continue\n",
    "    mean_steps += len(ep)\n",
    "mean_steps /= len(transitions)\n",
    "print(\"Mean number of steps: \", mean_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete vs VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBSERVATION_TYPE = 'single' # OR 'cardinal'\n",
    "RENDER_MODE = 'human' # OR 'rgb_array'\n",
    "BELIEF_UPDATE = 'discrete' # OR 'variational'\n",
    "DISCRETIZATION = 20\n",
    "DEBUG = True\n",
    "\n",
    "obs_model = load_obs_model(OBSERVATION_TYPE)\n",
    "\n",
    "pomdp_env = POMDPDeformedGridworld(obs_type=OBSERVATION_TYPE, render_mode=RENDER_MODE)\n",
    "agent = POMDPAgent(model, pomdp_env, update=BELIEF_UPDATE, obs_model=obs_model,discretization=DISCRETIZATION, debug=DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pomdp_env.reset()\n",
    "\n",
    "stretch, shear = [pomdp_env.transformation_matrix[0][0], pomdp_env.transformation_matrix[1][1]],[pomdp_env.transformation_matrix[0][1], pomdp_env.transformation_matrix[1][0]]\n",
    "starting_pos = pomdp_env.get_state()['pos']\n",
    "\n",
    "print(\"Starting position: \", starting_pos)\n",
    "print(\"Deformation: \", pomdp_env.transformation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        action, _  = agent.predict(pomdp_env.get_state(), deterministic=False)\n",
    "        _, _ , terminated, truncated, _ = pomdp_env.step(action)\n",
    "        pomdp_env.render()\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    except:\n",
    "        break\n",
    "pomdp_env.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBSERVATION_TYPE = 'single' # OR 'cardinal'\n",
    "RENDER_MODE = 'human' # OR 'rgb_array'\n",
    "BELIEF_UPDATE = 'variational' # OR 'variational'\n",
    "DISCRETIZATION = 20\n",
    "DEBUG = True\n",
    "\n",
    "obs_model = load_obs_model(OBSERVATION_TYPE)\n",
    "\n",
    "pomdp_env = POMDPDeformedGridworld(obs_type=OBSERVATION_TYPE, render_mode=RENDER_MODE)\n",
    "agent = POMDPAgent(model, pomdp_env, update=BELIEF_UPDATE, obs_model=obs_model,discretization=DISCRETIZATION, debug=DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "pomdp_env.set_deformation(stretch, shear)\n",
    "pomdp_env.set_position(starting_pos.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        action, _  = agent.predict(pomdp_env.get_state(), deterministic=False)\n",
    "        _, _ , terminated, truncated, _ = pomdp_env.step(action)\n",
    "        pomdp_env.render()\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    except:\n",
    "        break\n",
    "pomdp_env.close()   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robogym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
