{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4451]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "        self.f1 = nn.Linear(6, 128)\n",
    "        self.f2 = nn.Linear(128, 128)\n",
    "        self.f3 = nn.Linear(128, 128)\n",
    "        self.f4 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, pos,theta):\n",
    "        x = torch.cat([pos,theta], dim=1)\n",
    "        x = F.relu(self.f1(x))\n",
    "        x = F.relu(self.f2(x))\n",
    "        x = F.relu(self.f3(x))\n",
    "        x = F.sigmoid(self.f4(x))\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "obs_model = NN()\n",
    "\n",
    "# Load the model\n",
    "obs_model.load_state_dict(torch.load('obs_model_0.pth',map_location=torch.device('cpu') ,weights_only=True))\n",
    "\n",
    "obs_model.eval()\n",
    "obs_model(torch.tensor([0.5, 0.5]).unsqueeze(0), torch.tensor([0.5, 0.0, 0.0, 0.5]).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'obs': tensor(0.), 'pos': tensor([0.5750, 0.3591])}, {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from environment.env import POMDPDeformedGridworld\n",
    "pomdp_env = POMDPDeformedGridworld()\n",
    "pomdp_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# # Define the model and likelihood\n",
    "# def likelihood(y, y_pred, noise_std):\n",
    "#     return Normal(y_pred, noise_std).log_prob(y).sum()\n",
    "\n",
    "# Define the likelihood: Bernoulli(f(theta)), where f(theta) = sigmoid(theta)\n",
    "def likelihood_exact(y,X, theta, ):\n",
    "    \"\"\"\n",
    "    y: observation\n",
    "    X: position of the agent\n",
    "    theta: deformation parameter (sampled from the variational distribution)\n",
    "    \n",
    "    Returns the log likelihood of the observation y given the position X and the deformation parameter theta\n",
    "    \"\"\"\n",
    "    if isinstance(X, list) or (isinstance(X, torch.Tensor) and X.shape[0] > 1):\n",
    "        likelihood = 0\n",
    "        for x,y,t in zip(X,y,theta):\n",
    "            f_theta = torch.tensor([pomdp_env.batchedobserve(x.tolist(),t.reshape(2,2).tolist())]) #function of  X=(x,y) and theta sampled from variational distribution q_lambda\n",
    "            likelihood += (y * torch.log(f_theta + 10e-9) + (1 - y) * torch.log(1 - f_theta + 10e-9))\n",
    "        return likelihood\n",
    "\n",
    "    f_theta = torch.tensor([pomdp_env.batchedobserve(X.squeeze().tolist(),theta.reshape(2,2).tolist())]) #function of  X=(x,y) and theta sampled from variational distribution q_lambda\n",
    "    return y * torch.log(f_theta + 10e-9) + (1 - y) * torch.log(1 - f_theta + 10e-9)\n",
    "\n",
    "def likelihood_model(y,X, theta, ):\n",
    "    \"\"\"\n",
    "    y: observation\n",
    "    X: position of the agent\n",
    "    theta: deformation parameter (sampled from the variational distribution)\n",
    "    \n",
    "    Returns the log likelihood of the observation y given the position X and the deformation parameter theta\n",
    "    \"\"\"\n",
    "    # print(X.shape, theta.shape)\n",
    "    f_theta = obs_model(X, theta)\n",
    "    return torch.distributions.Bernoulli(f_theta).log_prob(y).sum()\n",
    "    # return (y * torch.log(f_theta + 10e-9) + (1 - y) * torch.log(1 - f_theta + 10e-9)).sum()\n",
    "\n",
    "# Define the prior\n",
    "def prior(theta):\n",
    "    mu = torch.tensor([torch.mean(torch.tensor(pomdp_env.stretch_range)),\n",
    "                    torch.mean(torch.tensor(pomdp_env.shear_range)),\n",
    "                    torch.mean(torch.tensor(pomdp_env.shear_range)),\n",
    "                    torch.mean(torch.tensor(pomdp_env.stretch_range))])\n",
    "    sigma = torch.tensor([.5,.5,.5,.5])\n",
    "    return Normal(mu, sigma).log_prob(theta).sum()\n",
    "\n",
    "# Variational distribution (q_lambda)\n",
    "class VariationalDistribution(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(VariationalDistribution, self).__init__()\n",
    "        self.mu = nn.Parameter(torch.zeros(dim))\n",
    "        self.log_sigma = nn.Parameter(torch.zeros(dim))\n",
    "    \n",
    "    def sample(self, num_samples=1):\n",
    "        epsilon = torch.randn(num_samples, self.mu.size(0), device=self.mu.device)\n",
    "        sigma = torch.exp(self.log_sigma)\n",
    "        return self.mu + sigma * epsilon  # Reparameterization trick\n",
    "    \n",
    "    def log_prob(self, theta):\n",
    "        sigma = torch.exp(self.log_sigma)\n",
    "        return Normal(self.mu, sigma).log_prob(theta).sum()\n",
    "    \n",
    "    def entropy(self):\n",
    "        sigma = torch.exp(self.log_sigma)\n",
    "        return Normal(self.mu, sigma).entropy().sum()\n",
    "\n",
    "# BBVI Training Loop\n",
    "def bbvi(X, y, variational_dist, num_epochs=5000, num_samples=10, lr=0.01):\n",
    "    optimizer = optim.Adam(variational_dist.parameters(), lr=lr)\n",
    "    noise_std = 0.1  # Observation noise\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Monte Carlo approximation of ELBO\n",
    "        elbo = 0\n",
    "        for i in range(num_samples):\n",
    "            # Sample theta ~ q_lambda\n",
    "            theta = variational_dist.sample()#(X.shape[0]) \n",
    "            theta = theta.repeat(X.shape[0],1)\n",
    "            # Log likelihood p(x|theta) theta is sampled from the variational distribution\n",
    "            log_likelihood = likelihood_exact(y,X,theta)\n",
    "            \n",
    "            # Log prior\n",
    "            log_prior = prior(theta)\n",
    "            \n",
    "            # Log variational density\n",
    "            log_q = variational_dist.log_prob(theta)\n",
    "            \n",
    "            # ELBO term\n",
    "            elbo += log_likelihood + log_prior - log_q\n",
    "        \n",
    "        elbo /= num_samples  # Average over samples\n",
    "        \n",
    "        # Maximize ELBO (minimize negative ELBO)\n",
    "        loss = -elbo\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, ELBO: {elbo.item():.4f}\")\n",
    "            print(f\"Variational entropy: {variational_dist.entropy().item():.4f}\")\n",
    "    return variational_dist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46624/2130527995.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mu = torch.tensor(torch.tensor([torch.mean(torch.tensor(pomdp_env.stretch_range)),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, ELBO: -1.4618\n",
      "Variational entropy: 3.8229\n",
      "Epoch 20, ELBO: -2.0629\n",
      "Variational entropy: 2.6913\n",
      "Epoch 30, ELBO: -3.2017\n",
      "Variational entropy: 2.3513\n",
      "Epoch 40, ELBO: -1.8546\n",
      "Variational entropy: 2.5077\n",
      "Epoch 50, ELBO: -2.0608\n",
      "Variational entropy: 2.8103\n",
      "Epoch 60, ELBO: -2.5817\n",
      "Variational entropy: 2.9983\n",
      "Epoch 70, ELBO: -2.3840\n",
      "Variational entropy: 2.9963\n",
      "Epoch 80, ELBO: -2.2114\n",
      "Variational entropy: 2.8595\n",
      "Epoch 90, ELBO: -0.9313\n",
      "Variational entropy: 2.7943\n",
      "Epoch 100, ELBO: -2.0220\n",
      "Variational entropy: 2.8118\n",
      "Variational Mean: [0.69867116 0.00935323 0.02087384 0.72142774]\n",
      "Variational Log Std: [-0.68619794 -0.7461779  -0.6965455  -0.7350372 ]\n",
      "True Theta: [[0.9150712259141278, -0.026402282565070212], [-0.08528106287796525, 0.8148674518812786]]\n"
     ]
    }
   ],
   "source": [
    "# Synthetic data for Bayesian linear regression\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# X va sostituito con gli stati del pomdp (x,y)\n",
    "pos = pomdp_env.get_state()['pos']\n",
    "X = torch.tensor([pos[0], pos[1]]).unsqueeze(0) # one sample only\n",
    "# X = torch.randn(100, 2)  # 100 samples, 2 features\n",
    "\n",
    "# y va sostituito con le osservazioni del pomdp negli stati pos=(x.y)\n",
    "y = pomdp_env.get_state()['obs']\n",
    "\n",
    "# true_theta va sostituito con i parametri reali del pomdp\n",
    "true_theta = pomdp_env.transformation_matrix #torch.tensor([1.5, 0.5])\n",
    "\n",
    "\n",
    "# print(X.shape, true_theta.shape, y.shape)\n",
    "\n",
    "# Initialize variational distribution\n",
    "variational_dist = VariationalDistribution(dim=4)\n",
    "\n",
    "# Train with BBVI\n",
    "trained_dist = bbvi(X, y, variational_dist, num_epochs=100, num_samples=100, lr=0.05)\n",
    "\n",
    "# Output trained variational parameters\n",
    "print(f\"Variational Mean: {trained_dist.mu.detach().numpy()}\")\n",
    "print(f\"Variational Log Std: {trained_dist.log_sigma.detach().numpy()}\")\n",
    "\n",
    "print(f\"True Theta: {true_theta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trajectory generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8109401012886684, 0.06654548799716531], [-0.020748753038606266, 0.8358242639626121]]\n"
     ]
    }
   ],
   "source": [
    "pomdp_env.reset()\n",
    "print(pomdp_env.transformation_matrix)\n",
    "s = pomdp_env.get_state()\n",
    "pos = [s['pos']]\n",
    "obs = [s['obs']]\n",
    "while True:\n",
    "    try:\n",
    "        state, _, _, _, _ = pomdp_env.step(int(input()))\n",
    "        pos.append(state['pos'])\n",
    "        obs.append(state['obs'])\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions, observations = pos, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46624/2130527995.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mu = torch.tensor(torch.tensor([torch.mean(torch.tensor(pomdp_env.stretch_range)),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, ELBO: -201.7300\n",
      "Variational entropy: 2.5280\n",
      "Epoch 20, ELBO: -203.0858\n",
      "Variational entropy: 1.8568\n",
      "Epoch 30, ELBO: -133.7054\n",
      "Variational entropy: 2.6228\n",
      "Epoch 40, ELBO: -174.8132\n",
      "Variational entropy: 3.2287\n",
      "Epoch 50, ELBO: -132.3981\n",
      "Variational entropy: 2.9931\n",
      "Epoch 60, ELBO: -147.2686\n",
      "Variational entropy: 2.7454\n",
      "Epoch 70, ELBO: -151.7767\n",
      "Variational entropy: 2.8417\n",
      "Epoch 80, ELBO: -149.1945\n",
      "Variational entropy: 2.9733\n",
      "Epoch 90, ELBO: -158.0813\n",
      "Variational entropy: 2.9115\n",
      "Epoch 100, ELBO: -156.1136\n",
      "Variational entropy: 2.8544\n",
      "Variational Mean: [ 0.6744745   0.01219801 -0.05089802  0.71386325]\n",
      "Variational Log Std: [-0.72823495 -0.73788625 -0.6774991  -0.6777137 ]\n",
      "True Theta: [[0.8109401012886684, 0.06654548799716531], [-0.020748753038606266, 0.8358242639626121]]\n"
     ]
    }
   ],
   "source": [
    "# X va sostituito con gli stati del pomdp (x,y)\n",
    "X = torch.stack(positions)\n",
    "\n",
    "# y va sostituito con le osservazioni del pomdp negli stati pos=(x.y)\n",
    "y = torch.stack(observations)\n",
    "\n",
    "# true_theta va sostituito con i parametri reali del pomdp\n",
    "true_theta = pomdp_env.transformation_matrix \n",
    "\n",
    "# print(X.shape, true_theta.shape, y.shape)\n",
    "\n",
    "# Initialize variational distribution\n",
    "variational_dist = VariationalDistribution(dim=4)\n",
    "\n",
    "# Train with BBVI\n",
    "trained_dist = bbvi(X, y, variational_dist, num_epochs=100, num_samples=200, lr=0.1)\n",
    "\n",
    "# Output trained variational parameters\n",
    "print(f\"Variational Mean: {trained_dist.mu.detach().numpy()}\")\n",
    "print(f\"Variational Log Std: {trained_dist.log_sigma.detach().numpy()}\")\n",
    "\n",
    "print(f\"True Theta: {true_theta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "class VariationalBayesianInference:\n",
    "    def __init__(self, f, input_dim, latent_dim=1, hidden_dim=32):\n",
    "        \"\"\"\n",
    "        Initialize the variational Bayesian inference model.\n",
    "        \n",
    "        Args:\n",
    "            f: callable, the known function linking X and y through theta\n",
    "            input_dim: int, dimension of input X\n",
    "            latent_dim: int, dimension of latent parameter theta\n",
    "            hidden_dim: int, dimension of hidden layers in the neural network\n",
    "        \"\"\"\n",
    "        self.f = f\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Variational parameters (mean and log variance of q(theta))\n",
    "        self.q_mu = nn.Parameter(torch.randn(latent_dim))\n",
    "        self.q_logvar = nn.Parameter(torch.zeros(latent_dim))\n",
    "        \n",
    "        # Prior parameters (assumed to be standard normal)\n",
    "        # self.prior = dist.Normal(torch.zeros(latent_dim), torch.ones(latent_dim))\n",
    "        prior_mu = torch.tensor([torch.mean(torch.tensor(pomdp_env.stretch_range)),\n",
    "                    torch.mean(torch.tensor(pomdp_env.shear_range)),\n",
    "                    torch.mean(torch.tensor(pomdp_env.shear_range)),\n",
    "                    torch.mean(torch.tensor(pomdp_env.stretch_range))])\n",
    "        prior_sigma = torch.tensor([.5,.5,.5,.5])\n",
    "        self.prior = dist.Normal(prior_mu, prior_sigma)\n",
    "\n",
    "        \n",
    "    def sample_latent(self, n_samples=1):\n",
    "        \"\"\"Sample from the variational distribution q(theta)\"\"\"\n",
    "        eps = torch.randn(n_samples, self.latent_dim)\n",
    "        std = torch.exp(0.5 * self.q_logvar)\n",
    "        return self.q_mu + eps * std\n",
    "    \n",
    "    def elbo(self, X, y, n_samples=10):\n",
    "        \"\"\"\n",
    "        Compute the evidence lower bound (ELBO)\n",
    "        \n",
    "        Args:\n",
    "            X: torch.Tensor, input data (batch_size, input_dim)\n",
    "            y: torch.Tensor, observations (batch_size,)\n",
    "            n_samples: int, number of Monte Carlo samples\n",
    "        \"\"\"\n",
    "        batch_size = X.shape[0]\n",
    "        \n",
    "        # Sample from variational distribution\n",
    "        theta_samples = self.sample_latent(n_samples)  # (n_samples, latent_dim)\n",
    "        \n",
    "        # Compute log likelihood for each sample\n",
    "        log_likelihood = torch.zeros(n_samples, batch_size)\n",
    "        for i in range(n_samples):\n",
    "            theta = theta_samples[i]\n",
    "            y_pred = self.f(X, theta.repeat(batch_size, 1))\n",
    "            # Assuming Bernoulli observation model\n",
    "            log_likelihood[i] = dist.Bernoulli(y_pred.squeeze()).log_prob(y)\n",
    "        \n",
    "        # Average over samples\n",
    "        expected_log_likelihood = torch.mean(log_likelihood, dim=0).sum()\n",
    "\n",
    "        # if expected_log_likelihood == dist.Bernoulli(self.f(X,theta_samples)).log_prob(y).sum():\n",
    "        #     print('ok should substitute the for loop with the above line')\n",
    "        \n",
    "        # Compute KL divergence\n",
    "        q_dist = dist.Normal(self.q_mu, torch.exp(0.5 * self.q_logvar))\n",
    "        kl_div = dist.kl_divergence(q_dist, self.prior).sum()\n",
    "        \n",
    "        return expected_log_likelihood - kl_div\n",
    "    \n",
    "    def fit(self, X, y, n_epochs=100, batch_size=64, lr=0.1):\n",
    "        \"\"\"\n",
    "        Fit the model using variational inference\n",
    "        \n",
    "        Args:\n",
    "            X: torch.Tensor, input data\n",
    "            y: torch.Tensor, observations\n",
    "            n_epochs: int, number of training epochs\n",
    "            batch_size: int, batch size for stochastic optimization\n",
    "            lr: float, learning rate\n",
    "        \"\"\"\n",
    "        optimizer = Adam([self.q_mu, self.q_logvar], lr=lr)\n",
    "        \n",
    "        dataset = torch.utils.data.TensorDataset(X, y)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "            for batch_X, batch_y in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                loss = -self.elbo(batch_X, batch_y,100)  # Negative because we minimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    def get_posterior_params(self):\n",
    "        \"\"\"Return the learned posterior parameters\"\"\"\n",
    "        return {\n",
    "            'mean': self.q_mu.detach(),\n",
    "            'std': torch.exp(0.5 * self.q_logvar).detach()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 173.8288\n",
      "Epoch 200/1000, Loss: 46.3040\n",
      "Epoch 300/1000, Loss: 28.8903\n",
      "Epoch 400/1000, Loss: 122.4874\n",
      "Epoch 500/1000, Loss: 28.5610\n",
      "Epoch 600/1000, Loss: 29.0916\n",
      "Epoch 700/1000, Loss: 29.2610\n",
      "Epoch 800/1000, Loss: 28.3988\n",
      "Epoch 900/1000, Loss: 28.7424\n",
      "Epoch 1000/1000, Loss: 28.7606\n",
      "Estimated theta (mean): tensor([ 3.0437, -1.1561, -0.1792,  0.7987])\n",
      "Posterior standard deviation: tensor([0.0325, 0.0114, 0.0087, 0.0062])\n",
      "True theta: [[0.5079403171435333, 0.17129979562961556], [-0.10487444801468786, 0.8962749936302412]]\n"
     ]
    }
   ],
   "source": [
    "# X va sostituito con gli stati del pomdp (x,y)\n",
    "X = torch.stack(positions)\n",
    "\n",
    "# y va sostituito con le osservazioni del pomdp negli stati pos=(x.y)\n",
    "y = torch.stack(observations)\n",
    "\n",
    "# true_theta va sostituito con i parametri reali del pomdp\n",
    "true_theta = pomdp_env.transformation_matrix \n",
    "\n",
    "# Create and fit the model\n",
    "model = VariationalBayesianInference(obs_model, input_dim=2, latent_dim=4)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get the learned posterior parameters\n",
    "posterior = model.get_posterior_params()\n",
    "print(\"Estimated theta (mean):\", posterior['mean'])\n",
    "print(\"Posterior standard deviation:\", posterior['std'])\n",
    "\n",
    "print(\"True theta:\", true_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.7534, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check everything is working\n",
    "theta = torch.tensor(true_theta).flatten()\n",
    "theta = torch.tensor([[.5,0.2,-0.1,.9]])\n",
    "y_pred = obs_model(X, theta.repeat(X.shape[0], 1))\n",
    "# # Assuming Bernoulli observation model\n",
    "log_likelihood = dist.Bernoulli(y_pred.squeeze()).log_prob(y).sum()\n",
    "log_likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, None, None, None)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pomdp_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1250, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metropolis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5124/741569895.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mu = torch.tensor(torch.tensor([torch.mean(torch.tensor(pomdp_env.stretch_range)),\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m sigma \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m4\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.2\u001b[39m  \u001b[38;5;66;03m# Identity covariance matrix\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Run Metropolis-Hastings\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mmetropolis_hastings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mburn_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposal_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Analyze and visualize results\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 62\u001b[0m, in \u001b[0;36mmetropolis_hastings\u001b[0;34m(X, y, num_samples, burn_in, mu, sigma, proposal_std)\u001b[0m\n\u001b[1;32m     59\u001b[0m proposal \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mMultivariateNormal(theta, proposal_cov)\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Compute the log-posterior for the proposal\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m proposed_log_posterior \u001b[38;5;241m=\u001b[39m \u001b[43mlog_posterior\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mproposal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Compute acceptance probability\u001b[39;00m\n\u001b[1;32m     65\u001b[0m acceptance_prob \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(proposed_log_posterior \u001b[38;5;241m-\u001b[39m current_log_posterior)\n",
      "Cell \u001b[0;32mIn[35], line 32\u001b[0m, in \u001b[0;36mlog_posterior\u001b[0;34m(X, theta, y, mu, sigma)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_posterior\u001b[39m(X,theta, y, mu, sigma):\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    theta: Tensor of shape (2,) representing [theta1, theta2].\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlikelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m prior(theta, mu, sigma)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.distributions as dist\n",
    "\n",
    "# Define the multivariate Gaussian prior: N(mu, Sigma)\n",
    "def prior(theta, mu, sigma):\n",
    "    \"\"\"\n",
    "    theta: Tensor of shape (2,) representing [theta1, theta2].\n",
    "    mu: Tensor of shape (2,) representing [mu1, mu2].\n",
    "    sigma: Tensor of shape (2, 2), covariance matrix.\n",
    "    \"\"\"\n",
    "    mvn = dist.MultivariateNormal(mu, sigma)\n",
    "    return mvn.log_prob(theta)\n",
    "\n",
    "def likelihood(X,theta,y):\n",
    "    \"\"\"\n",
    "    y: observation\n",
    "    X: position of the agent\n",
    "    theta: deformation parameter (sampled from the variational distribution)\n",
    "    \n",
    "    Returns the log likelihood of the observation y given the position X and the deformation parameter theta\n",
    "    \"\"\"\n",
    "    # print(X.shape, theta.shape)\n",
    "    f_theta = obs_model(X, theta.repeat(X.shape[0],1))\n",
    "    return y * torch.log(f_theta + 10e-9) + (1 - y) * torch.log(1 - f_theta + 10e-9)\n",
    "\n",
    "\n",
    "# Define the posterior (up to proportionality)\n",
    "def log_posterior(X,theta, y, mu, sigma):\n",
    "    \"\"\"\n",
    "    theta: Tensor of shape (2,) representing [theta1, theta2].\n",
    "    \"\"\"\n",
    "    return likelihood(X,theta, y).sum() + prior(theta, mu, sigma)\n",
    "\n",
    "# Metropolis-Hastings algorithm\n",
    "def metropolis_hastings(X,y, num_samples=10000, burn_in=1000, mu=None, sigma=None, proposal_std=0.5):\n",
    "    \"\"\"\n",
    "    y: Observed binary outcomes (0 or 1).\n",
    "    num_samples: Number of samples to draw from posterior.\n",
    "    burn_in: Number of initial samples to discard.\n",
    "    mu: Tensor of shape (2,) representing the prior mean vector.\n",
    "    sigma: Tensor of shape (2, 2), the prior covariance matrix.\n",
    "    proposal_std: Standard deviation for the proposal distribution.\n",
    "    \"\"\"\n",
    "    if mu is None:\n",
    "        raise ValueError(\"Prior mean (mu) must be specified.\")\n",
    "        mu = torch.tensor([0.0, 0.0])\n",
    "    if sigma is None:\n",
    "        sigma = torch.eye(2)  # Identity covariance matrix by default\n",
    "\n",
    "    samples = []\n",
    "    theta = torch.zeros(4)  # Initial value for [theta1, theta2]\n",
    "    current_log_posterior = log_posterior(X,theta, y, mu, sigma)\n",
    "\n",
    "    # Define proposal distribution\n",
    "    proposal_cov = torch.eye(4) * proposal_std**2\n",
    "\n",
    "    for i in range(num_samples + burn_in):\n",
    "        # Propose new theta' using a multivariate normal proposal\n",
    "        proposal = dist.MultivariateNormal(theta, proposal_cov).sample()\n",
    "\n",
    "        # Compute the log-posterior for the proposal\n",
    "        proposed_log_posterior = log_posterior(X,proposal, y, mu, sigma)\n",
    "\n",
    "        # Compute acceptance probability\n",
    "        acceptance_prob = torch.exp(proposed_log_posterior - current_log_posterior)\n",
    "\n",
    "        # Accept or reject the proposal\n",
    "        if torch.rand(1).item() < acceptance_prob:\n",
    "            theta = proposal\n",
    "            current_log_posterior = proposed_log_posterior\n",
    "\n",
    "        # Store the sample after burn-in\n",
    "        if i >= burn_in:\n",
    "            samples.append(theta.numpy())\n",
    "\n",
    "    return torch.tensor(samples)\n",
    "\n",
    "# Example usage\n",
    "X = torch.stack(positions)\n",
    "# Observations (y): binary data from a Bernoulli distribution\n",
    "y = torch.stack(observations)\n",
    "\n",
    "# Define prior parameters\n",
    "mu = torch.tensor(torch.tensor([torch.mean(torch.tensor(pomdp_env.stretch_range)),\n",
    "                torch.mean(torch.tensor(pomdp_env.shear_range)),\n",
    "                torch.mean(torch.tensor(pomdp_env.shear_range)),\n",
    "                torch.mean(torch.tensor(pomdp_env.stretch_range))]))\n",
    "sigma = torch.eye(4) * 0.2  # Identity covariance matrix\n",
    "\n",
    "# Run Metropolis-Hastings\n",
    "samples = metropolis_hastings(X,y, num_samples=5000, burn_in=1000, mu=mu, sigma=sigma, proposal_std=0.5)\n",
    "\n",
    "# Analyze and visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract theta1 and theta2 samples\n",
    "theta1_samples = samples[:, 0].numpy()\n",
    "theta2_samples = samples[:, 1].numpy()\n",
    "theta3_samples = samples[:, 2].numpy()\n",
    "theta4_samples = samples[:, 3].numpy()\n",
    "\n",
    "# Plot the posterior samples\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Theta1 histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(theta1_samples, bins=50, density=True, alpha=0.7, label='Theta1 Samples')\n",
    "plt.title(\"Posterior Distribution of Theta1\")\n",
    "plt.xlabel(\"Theta1\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "\n",
    "# Theta2 histogram\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(theta2_samples, bins=50, density=True, alpha=0.7, label='Theta2 Samples')\n",
    "plt.title(\"Posterior Distribution of Theta2\")\n",
    "plt.xlabel(\"Theta2\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.hist(theta3_samples, bins=50, density=True, alpha=0.7, label='Theta3 Samples')\n",
    "# plt.title(\"Posterior Distribution of Theta3\")\n",
    "# plt.xlabel(\"Theta3\")\n",
    "# plt.ylabel(\"Density\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.hist(theta4_samples, bins=50, density=True, alpha=0.7, label='Theta4 Samples')\n",
    "# plt.title(\"Posterior Distribution of Theta4\")\n",
    "# plt.xlabel(\"Theta4\")\n",
    "# plt.ylabel(\"Density\")\n",
    "# plt.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.435245612516588, -0.19779374326553678],\n",
       " [-0.19766264973518238, 0.5614838793112605]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pomdp_env.transformation_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robogym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
