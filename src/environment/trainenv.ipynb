{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.9.19)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pygame\n",
    "import time\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Dict, Discrete, Box\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def point_inside_prlgm(point,poly):\n",
    "    x, y = point\n",
    "    inside = False\n",
    "    xb = poly[0][0] - poly[1][0]\n",
    "    yb = poly[0][1] - poly[1][1]\n",
    "    xc = poly[2][0] - poly[1][0]\n",
    "    yc = poly[2][1] - poly[1][1]\n",
    "    xp = x - poly[1][0]\n",
    "    yp = y - poly[1][1]\n",
    "    d = xb * yc - yb * xc\n",
    "    if (d != 0):\n",
    "        oned = 1.0 / d\n",
    "        bb = (xp * yc - xc * yp) * oned\n",
    "        cc = (xb * yp - xp * yb) * oned\n",
    "        inside = (bb >= 0) & (cc >= 0) & (bb <= 1) & (cc <= 1)\n",
    "    return inside\n",
    "\n",
    "\n",
    "# Check if point is inside the quadrilateral\n",
    "def is_point_inside_quad(p, box):\n",
    "    # Divide the quadrilateral into two triangles and check if the point is inside either one\n",
    "    p2,p1, p3, p4 = box\n",
    "    return point_inside_prlgm(p, [p1, p2, p3]) or point_inside_prlgm(p, [p1, p3, p4])\n",
    "\n",
    "quadri = env.transformed_corners\n",
    "point = [-0.5, 1]\n",
    "print(is_point_inside_quad(point, quadri))\n",
    "\n",
    "# plot the poligon\n",
    "fig, ax = plt.subplots()\n",
    "ax.add_patch(patches.Polygon(quadri, closed=True, fill=False))\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "# plot point of coordinates point[0], point[1]\n",
    "plt.plot(point[0], point[1], 'ro')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservableDeformedGridworld(gym.Env):\n",
    "\n",
    "    def __init__(self, grid_size=(1.0, 1.0), step_size=0.02, goal=(0.9, 0.9), \n",
    "                 obstacles=None, stretch=(1.0, 1.0), shear=(0.0, 0.0), observation_radius=0.05, render_mode=None):\n",
    "        \"\"\"\n",
    "        Initialize the observable deformed continuous gridworld.\n",
    "        :param grid_size: Size of the grid (width, height).\n",
    "        :param step_size: Step size for the agent's movement.\n",
    "        :param goal: Coordinates of the goal position.\n",
    "        :param obstacles: List of obstacles as rectangles [(x_min, y_min), (x_max, y_max)].\n",
    "        :param stretch: Tuple (s_x, s_y) for stretching the grid in x and y directions.\n",
    "        :param shear: Tuple (sh_x, sh_y) for shearing the grid.\n",
    "        :param observation_radius: Radius within which the agent can observe its surroundings.\n",
    "        \"\"\"\n",
    "        self.grid_size = np.array(grid_size)\n",
    "        self.step_size = step_size\n",
    "        self.goal = np.array(goal)\n",
    "        self.state = np.array([0.1, 0.1])  # Start at the origin\n",
    "        self.obstacles = obstacles if obstacles else []\n",
    "        self.observation_radius = observation_radius\n",
    "\n",
    "        # Transformation matrix\n",
    "        self.transformation_matrix = np.array([\n",
    "            [stretch[0], shear[0]],\n",
    "            [shear[1], stretch[1]]\n",
    "        ])\n",
    "        self.inverse_transformation_matrix = np.linalg.inv(self.transformation_matrix)\n",
    "\n",
    "        # Rendering mode\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # gymnasium compatibility\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.observation_space =  Dict({\n",
    "            \"pos\": gym.spaces.Box(low=.0, high=1.0, shape=(2,),dtype=float),\n",
    "            \"theta\": gym.spaces.Box(low=.0, high=1.0, shape=(4,),dtype=float), # deformation is a 2x2 tensor\n",
    "            \"obs\": gym.spaces.Box(low=0, high=1, shape=(4,),dtype=int),\n",
    "        })\n",
    "\n",
    "        self.stretch_range = np.array([0.4, 1])\n",
    "        self.shear_range = np.array([-0.2, 0.2])\n",
    "\n",
    "        self.timestep = 0\n",
    "\n",
    "        self.corners = [\n",
    "            np.array([0, 0]),\n",
    "            np.array([self.grid_size[0], 0]),\n",
    "            self.grid_size,\n",
    "            np.array([0, self.grid_size[1]]),\n",
    "        ]\n",
    "        self.transformed_corners = [self.transform(corner) for corner in self.corners]\n",
    "        \n",
    "    def reset(self,seed=55):\n",
    "        \"\"\"\n",
    "        Reset the environment to the initial state.\n",
    "        :return: Initial state and observation.\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        self.set_deformation(self.sample(2,self.stretch_range), self.sample(2,self.shear_range))  # Reset deformation to random\n",
    "        self.state = np.array([0.1, 0.1])  # Start at the origin\n",
    "        #self.state = np.random.rand(2) * self.transform(self.grid_size) # Random start position in the deformable grid\n",
    "        \n",
    "        self.transformed_corners = [self.transform(corner) for corner in self.corners]\n",
    "\n",
    "        state = OrderedDict({\n",
    "            \"pos\": self.state,\n",
    "            \"theta\": self.transformation_matrix.flatten(),\n",
    "            \"obs\": self.observe_obstacle()\n",
    "        }) \n",
    "        \n",
    "        self.timestep = 0\n",
    "\n",
    "        # print(f\"Initial agent position: {self.state}\",\n",
    "        #       f\"Initial goal position: {self.goal}\",\n",
    "        #       f\"Initial deformation: {self.transformation_matrix}\",\n",
    "        #       f\"Initial observation: {self.observe_obstacle()}\",\n",
    "        #       sep=\"\\n\")\n",
    "        \n",
    "        return state, {}\n",
    "    \n",
    "    def set_deformation(self, stretch, shear):\n",
    "        \"\"\"\n",
    "        Set the deformation transformation matrix based on stretch and shear parameters.\n",
    "        \n",
    "        This function creates a transformation matrix to apply grid deformations, including \n",
    "        stretching and shearing, to the grid coordinates. It also computes the inverse of \n",
    "        this transformation for reversing the deformation.\n",
    "\n",
    "        :param stretch: A tuple (s_x, s_y) for stretching the grid in the x and y directions.\n",
    "        :param shear: A tuple (sh_x, sh_y) for shearing the grid in the x and y directions.\n",
    "        \"\"\"\n",
    "        # Create the transformation matrix based on stretch and shear\n",
    "        self.transformation_matrix = np.array([\n",
    "            [stretch[0], shear[0]],  # First row: stretch in x and shear in x direction\n",
    "            [shear[1], stretch[1]]   # Second row: shear in y and stretch in y direction\n",
    "        ])\n",
    "\n",
    "        # Calculate the inverse transformation matrix for reversing the deformation\n",
    "        self.inverse_transformation_matrix = np.linalg.inv(self.transformation_matrix)\n",
    "\n",
    "        # Optionally, print the transformation matrices for debugging\n",
    "        # print(f\"Transformation Matrix:\\n{self.transformation_matrix}\")\n",
    "        # print(f\"Inverse Transformation Matrix:\\n{self.inverse_transformation_matrix}\")\n",
    "\n",
    "    def set_pos(self, pos):\n",
    "        \"\"\"\n",
    "        Set the agent's state to a new position.\n",
    "        \n",
    "        This function directly updates the agent's position (state) to the provided coordinates.\n",
    "\n",
    "        :param pos: A tuple or array representing the new position of the agent in the grid.\n",
    "        \"\"\"\n",
    "        # Update the state (agent's position)\n",
    "        self.state = np.array(pos)\n",
    "\n",
    "        # Optionally, print the new state for debugging\n",
    "        # print(f\"New agent position: {self.state}\")\n",
    "\n",
    "    def transform(self, position):\n",
    "        \"\"\"\n",
    "        Apply the grid deformation to a given position.\n",
    "        :param position: (x, y) in original space.\n",
    "        :return: Transformed position in the deformed grid.\n",
    "        \"\"\"\n",
    "        return np.dot(self.transformation_matrix, position)\n",
    "\n",
    "    def inverse_transform(self, position):\n",
    "        \"\"\"\n",
    "        Map a position from the deformed grid back to the original space.\n",
    "        :param position: (x, y) in the deformed grid.\n",
    "        :return: Original position.\n",
    "        \"\"\"\n",
    "        return np.dot(self.inverse_transformation_matrix, position)\n",
    "\n",
    "    def is_in_obstacle(self, position):\n",
    "        \"\"\"\n",
    "        Check if a given position is inside any obstacle.\n",
    "        :param position: The (x, y) coordinates to check in the original space.\n",
    "        :return: True if the position is inside an obstacle, False otherwise.\n",
    "        \"\"\"\n",
    "        for obs in self.obstacles:\n",
    "            (x_min, y_min), (x_max, y_max) = obs\n",
    "            x_min_transformed, y_min_transformed = self.transform([x_min, y_min])\n",
    "            x_max_transformed, y_max_transformed = self.transform([x_max, y_max])\n",
    "            if x_min_transformed <= position[0] <= x_max_transformed and y_min_transformed <= position[1] <= y_max_transformed:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def observe_single_obstacle(self):\n",
    "        \"\"\"\n",
    "        Check if any part of an obstacle is within the observation radius of the agent.\n",
    "        :return: True if any part of an obstacle is within the observation radius, False otherwise.\n",
    "        \"\"\"\n",
    "        for obs in self.obstacles:\n",
    "            (x_min, y_min), (x_max, y_max) = obs\n",
    "            \n",
    "            # Clamp the agent's position to the obstacle's boundaries to find the closest point\n",
    "            closest_x = np.clip(self.state[0], x_min, x_max)\n",
    "            closest_y = np.clip(self.state[1], y_min, y_max)\n",
    "            \n",
    "            # Compute the distance from the agent to this closest point\n",
    "            closest_point = np.array([closest_x, closest_y])\n",
    "            distance_to_obstacle = np.linalg.norm(self.state - closest_point)\n",
    "            \n",
    "            # Check if this distance is within the observation radius\n",
    "            if distance_to_obstacle <= self.observation_radius:\n",
    "                return 1\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def observe_obstacle(self):\n",
    "        \"\"\"\n",
    "        Efficiently and precisely check for obstacles in the four cardinal directions (N, E, S, W).\n",
    "        Each direction checks for obstacles in a quarter-circle arc within the observation radius.\n",
    "        :return: A numpy array of shape (4,), where each entry indicates the presence of obstacles \n",
    "                in the respective direction (North, East, South, West).\n",
    "        \"\"\"\n",
    "        directions = [\"N\", \"E\", \"S\", \"W\"]\n",
    "        obstacle_presence = np.zeros(4)  # Default: no obstacles in any direction\n",
    "\n",
    "        # Precompute direction boundaries in radians\n",
    "        direction_ranges = [\n",
    "            (315, 45),   # North: [-45°, +45°]\n",
    "            (45, 135),   # East: [+45°, +135°]\n",
    "            (135, 225),  # South: [+135°, +225°]\n",
    "            (225, 315)   # West: [+225°, +315°]\n",
    "        ]\n",
    "        direction_ranges_rad = [(np.deg2rad(a1), np.deg2rad(a2)) for a1, a2 in direction_ranges]\n",
    "\n",
    "        for obs in self.obstacles:\n",
    "            (x_min, y_min), (x_max, y_max) = obs\n",
    "            x_min, y_min = self.transform([x_min, y_min])\n",
    "            x_max, y_max = self.transform([x_max, y_max])\n",
    "\n",
    "            # Generate sampled points along the edges of the obstacle\n",
    "            num_samples = 5  # Increase for more precision\n",
    "            edge_points = np.concatenate([\n",
    "                np.linspace([x_min, y_min], [x_max, y_min], num_samples),  # Bottom edge\n",
    "                np.linspace([x_max, y_min], [x_max, y_max], num_samples),  # Right edge\n",
    "                np.linspace([x_max, y_max], [x_min, y_max], num_samples),  # Top edge\n",
    "                np.linspace([x_min, y_max], [x_min, y_min], num_samples)   # Left edge\n",
    "            ])\n",
    "\n",
    "            # Compute vectors from agent to sampled points\n",
    "            vectors = edge_points - self.state\n",
    "            distances = np.linalg.norm(vectors, axis=1)\n",
    "\n",
    "            # Filter points that are outside the observation radius\n",
    "            within_radius = distances <= self.observation_radius\n",
    "            if not np.any(within_radius):\n",
    "                continue  # Skip obstacles entirely outside the radius\n",
    "\n",
    "            # Compute angles relative to positive Y-axis\n",
    "            angles = np.arctan2(vectors[:, 1], vectors[:, 0])  # Radians\n",
    "            angles = (angles + 2 * np.pi) % (2 * np.pi)  # Normalize to [0, 2π)\n",
    "\n",
    "            # Check which direction each point falls into\n",
    "            for i, (angle_min, angle_max) in enumerate(direction_ranges_rad):\n",
    "                if obstacle_presence[i] == 1:\n",
    "                    continue  # Early exit if the direction is already flagged\n",
    "                for angle in angles[within_radius]:\n",
    "                    if (angle_min <= angle < angle_max) or (\n",
    "                        angle_max < angle_min and (angle >= angle_min or angle < angle_max)\n",
    "                    ):\n",
    "                        obstacle_presence[i] = 1\n",
    "                        break  # No need to check further points for this direction\n",
    "\n",
    "        return obstacle_presence\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take a step in the environment, interpreting the action in the deformed space.\n",
    "        :param action: One of ['N', 'S', 'E', 'W'].\n",
    "        :return: Tuple (next_state, observation, reward, done, info).\n",
    "        \"\"\"\n",
    "        # Map actions to movements in the deformed space\n",
    "        moves = [np.array([0, self.step_size]),   # Move up in deformed space\n",
    "            np.array([0, -self.step_size]),  # Move down in deformed space\n",
    "            np.array([self.step_size, 0]),   # Move right in deformed space\n",
    "            np.array([-self.step_size, 0])   # Move left in deformed space\n",
    "        ]\n",
    "\n",
    "        # Get the movement vector in the deformed space\n",
    "        move = moves[action]\n",
    "\n",
    "        # Map the movement to the original space using the inverse transformation\n",
    "        # move_original = np.dot(self.inverse_transformation_matrix, move)\n",
    "\n",
    "        # Update state in the original grid space\n",
    "        next_state = self.state + move\n",
    "\n",
    "        num_samples = 50  # Number of points to sample along the path\n",
    "        path = np.linspace(self.state, next_state, num_samples)\n",
    "\n",
    "        # Check for collisions along the path\n",
    "        collision = any(self.is_in_obstacle(point) for point in path)\n",
    "\n",
    "        # Check if the new state is in an obstacle\n",
    "        if collision:\n",
    "            reward = -1.0  # Penalty for hitting an obstacle\n",
    "            info = {\"collision\": True}\n",
    "            terminated = False\n",
    "        # Check if the is inside the deformed grid boundaries\n",
    "        # elif not is_point_inside_quad(next_state, self.transformed_corners):\n",
    "        #     reward = -2.0\n",
    "        #     info = {\"out\": True}\n",
    "        #     next_state = self.state\n",
    "        #     terminated = True\n",
    "        else:   \n",
    "            transformed_goal = self.transform(self.goal)\n",
    "            transformed_state = self.transform(self.state)\n",
    "            terminated = np.linalg.norm(transformed_state - transformed_goal) < self.step_size\n",
    "            reward = 1.0 if terminated else -0.01\n",
    "            info = {\"collision\": False, \"out\": False}\n",
    "    \n",
    "        self.state = next_state\n",
    "        self.timestep += 1\n",
    "        truncated = self.timestep >= 200 \n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        state = OrderedDict({\n",
    "                    \"pos\": self.state,\n",
    "                    \"theta\": self.transformation_matrix.flatten(),\n",
    "                    \"obs\": self.observe_obstacle()\n",
    "                })\n",
    "\n",
    "        # Return the transformed state, reward, and terminated truncated flag\n",
    "        return state, reward, terminated, truncated, info\n",
    "\n",
    "    def render_old(self):\n",
    "        \"\"\"\n",
    "        Render the deformed gridworld environment using Pygame, ensuring the entire deformed grid fits within the screen.\n",
    "        \"\"\"\n",
    "        import pygame  # Ensure Pygame is imported\n",
    "\n",
    "        # Define colors\n",
    "        WHITE = (255, 255, 255)\n",
    "        BLUE = (0, 0, 255)\n",
    "        GREEN = (0, 255, 0)\n",
    "        RED = (255, 0, 0)\n",
    "        YELLOW = (255, 255, 0)\n",
    "        BLACK = (0, 0, 0)\n",
    "\n",
    "        # Initialize the screen\n",
    "        if not hasattr(self, \"screen\"):\n",
    "            self.screen_width = 800\n",
    "            self.screen_height = 600\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "            pygame.display.set_caption(\"Deformed Gridworld Environment\")\n",
    "\n",
    "        # Fill background with white\n",
    "        self.screen.fill(WHITE)\n",
    "\n",
    "        # Compute the bounding box of the deformed grid\n",
    "        corners = [\n",
    "            self.transform(np.array([0, 0])),\n",
    "            self.transform(np.array([self.grid_size[0], 0])),\n",
    "            self.transform(self.grid_size),\n",
    "            self.transform(np.array([0, self.grid_size[1]])),\n",
    "        ]\n",
    "        x_coords, y_coords = zip(*corners)\n",
    "        min_x, max_x = min(x_coords), max(x_coords)\n",
    "        min_y, max_y = min(y_coords), max(y_coords)\n",
    "\n",
    "        # Define scaling factors to fit the deformed grid within the screen\n",
    "        scale_x = self.screen_width / (max_x - min_x)\n",
    "        scale_y = self.screen_height / (max_y - min_y)\n",
    "        scale = min(scale_x, scale_y)  # Uniform scaling to maintain aspect ratio\n",
    "\n",
    "        # Transform helper for rendering\n",
    "        def to_screen_coords(pos):\n",
    "            \"\"\"\n",
    "            Map transformed coordinates to screen coordinates, scaled and shifted to fit the screen.\n",
    "            \"\"\"\n",
    "            x, y = pos\n",
    "            x_screen = int((x - min_x) * scale)\n",
    "            y_screen = int((max_y - y) * scale)  # Flip y-axis for screen rendering\n",
    "            return x_screen, y_screen\n",
    "\n",
    "        # Draw the deformed grid boundaries\n",
    "        pygame.draw.polygon(self.screen, BLACK, [to_screen_coords(corner) for corner in corners], width=3)\n",
    "\n",
    "        # Draw the obstacles\n",
    "        for obs in self.obstacles:\n",
    "            (x_min, y_min), (x_max, y_max) = obs\n",
    "            bottom_left = self.transform(np.array([x_min, y_min]))\n",
    "            bottom_right = self.transform(np.array([x_max, y_min]))\n",
    "            top_left = self.transform(np.array([x_min, y_max]))\n",
    "            top_right = self.transform(np.array([x_max, y_max]))\n",
    "            \n",
    "            # Draw each obstacle as a polygon in the deformed space\n",
    "            pygame.draw.polygon(self.screen, RED, [\n",
    "                to_screen_coords(bottom_left),\n",
    "                to_screen_coords(bottom_right),\n",
    "                to_screen_coords(top_right),\n",
    "                to_screen_coords(top_left)\n",
    "            ])\n",
    "\n",
    "        # Draw the agent\n",
    "        agent_position = self.transform(self.state)\n",
    "        pygame.draw.circle(self\n",
    "        .screen, BLUE, to_screen_coords(agent_position), 10)\n",
    "\n",
    "        # Draw the goal\n",
    "        goal_position = self.transform(self.goal)\n",
    "        pygame.draw.circle(self.screen, GREEN, to_screen_coords(goal_position), 12)\n",
    "\n",
    "        # Draw observation radius as a dashed circle around the agent\n",
    "        observation_radius = self.observation_radius * max(self.transformation_matrix.diagonal())\n",
    "        pygame.draw.circle(self.screen, YELLOW, to_screen_coords(agent_position), \n",
    "                        int(observation_radius * scale), 1)\n",
    "\n",
    "        # Update the display\n",
    "        pygame.display.flip()\n",
    "\n",
    "        # Handle key events\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                # Press 'r' to reset environment\n",
    "                if event.key == pygame.K_r:\n",
    "                    self.reset()\n",
    "                # Press 'w' to quit\n",
    "                elif event.key == pygame.K_w:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "                # Press 's' to save current state\n",
    "                elif event.key == pygame.K_s:\n",
    "                    self.save_state()\n",
    "                # Press space to pause/resume\n",
    "                elif event.key == pygame.K_SPACE:\n",
    "                    self.pause()\n",
    "                # Press arrow keys for manual control\n",
    "                elif event.key == pygame.K_LEFT:\n",
    "                    return self.step(3)  # Left action\n",
    "                elif event.key == pygame.K_RIGHT:\n",
    "                    return self.step(2)  # Right action\n",
    "                elif event.key == pygame.K_UP:\n",
    "                    return self.step(0)  # Up action\n",
    "                elif event.key == pygame.K_DOWN:\n",
    "                    return self.step(1)  # Down action\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    def close(self):\n",
    "        self.render_mode = None\n",
    "        pygame.quit()\n",
    "    \n",
    "    def sample(self,num,limit):\n",
    "        low,high = limit\n",
    "        return low + np.random.rand(num)*(high-low)\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Render the deformed gridworld environment along with the original gridworld.\n",
    "        The original gridworld serves as a reference background.\n",
    "        \"\"\"\n",
    "        import pygame  # Ensure Pygame is imported\n",
    "\n",
    "        # Define colors\n",
    "        WHITE = (255, 255, 255)\n",
    "        LIGHT_GRAY = (200, 200, 200)\n",
    "        BLUE = (0, 0, 255)\n",
    "        GREEN = (0, 255, 0)\n",
    "        RED = (255, 0, 0)\n",
    "        PINK = (255, 105, 180)  \n",
    "        YELLOW = (255, 255, 0)\n",
    "        BLACK = (0, 0, 0)\n",
    "\n",
    "        # Initialize the screen\n",
    "        if not hasattr(self, \"screen\"):\n",
    "            self.screen_width = 1000\n",
    "            self.screen_height = 1000\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "            pygame.display.set_caption(\"Deformed and Original Gridworld\")\n",
    "\n",
    "        # Fill background with white\n",
    "        self.screen.fill(WHITE)\n",
    "\n",
    "    # Compute the bounding box of the deformed grid\n",
    "        corners = [\n",
    "            np.array([0, 0]),\n",
    "            np.array([self.grid_size[0], 0]),\n",
    "            self.grid_size,\n",
    "            np.array([0, self.grid_size[1]]),\n",
    "        ]\n",
    "        transformed_corners = [self.transform(corner) for corner in corners]\n",
    "        x_coords, y_coords = zip(*transformed_corners)\n",
    "        min_x, max_x = min(x_coords), max(x_coords)\n",
    "        min_y, max_y = min(y_coords), max(y_coords)\n",
    "\n",
    "        # Define scaling factors to fit the deformed grid within the screen\n",
    "        scale_x = self.screen_width / (max_x - min_x)\n",
    "        scale_y = self.screen_height / (max_y - min_y)\n",
    "        scale = min(scale_x, scale_y)  # Uniform scaling to maintain aspect ratio\n",
    "\n",
    "        # Add upward translation offset\n",
    "        y_translation = max(0, -min_y * scale)\n",
    "\n",
    "        # Transform helper for rendering\n",
    "        def to_screen_coords(pos):\n",
    "            \"\"\"\n",
    "            Map transformed coordinates to screen coordinates, scaled and shifted to fit the screen.\n",
    "            \"\"\"\n",
    "            x, y = pos\n",
    "            x_screen = int((x - min_x) * scale)\n",
    "            y_screen = int((max_y - y) * scale + y_translation)  # Flip y-axis and add upward translation\n",
    "            return x_screen, y_screen\n",
    "        \n",
    "        # Draw the un-deformed grid (background)\n",
    "        for i in range(int(self.grid_size[0]) + 1):\n",
    "            pygame.draw.line(self.screen, LIGHT_GRAY,\n",
    "                            to_screen_coords((i, 0)),\n",
    "                            to_screen_coords((i, self.grid_size[1])), width=1)\n",
    "        for j in range(int(self.grid_size[1]) + 1):\n",
    "            pygame.draw.line(self.screen, LIGHT_GRAY,\n",
    "                            to_screen_coords((0, j)),\n",
    "                            to_screen_coords((self.grid_size[0], j)), width=1)\n",
    "\n",
    "        # Draw the deformed grid boundaries\n",
    "        corners = [\n",
    "            np.array([0, 0]),\n",
    "            np.array([self.grid_size[0], 0]),\n",
    "            self.grid_size,\n",
    "            np.array([0, self.grid_size[1]]),\n",
    "        ]\n",
    "        transformed_corners = [self.transform(corner) for corner in corners]\n",
    "        pygame.draw.polygon(self.screen, BLACK, [to_screen_coords(corner) for corner in transformed_corners], width=3)\n",
    "\n",
    "        # Draw the obstacles in both grids\n",
    "        for obs in self.obstacles:\n",
    "            (x_min, y_min), (x_max, y_max) = obs\n",
    "            # Original obstacle\n",
    "            pygame.draw.rect(self.screen, PINK,\n",
    "                            (*to_screen_coords((x_min, y_max)),  # Top-left corner\n",
    "                            int((x_max - x_min) * scale),      # Width\n",
    "                            int((y_max - y_min) * scale)),    # Height\n",
    "                            width=0)\n",
    "\n",
    "            # Transformed obstacle\n",
    "            bottom_left = self.transform(np.array([x_min, y_min]))\n",
    "            bottom_right = self.transform(np.array([x_max, y_min]))\n",
    "            top_left = self.transform(np.array([x_min, y_max]))\n",
    "            top_right = self.transform(np.array([x_max, y_max]))\n",
    "            pygame.draw.polygon(self.screen, RED, [\n",
    "                to_screen_coords(bottom_left),\n",
    "                to_screen_coords(bottom_right),\n",
    "                to_screen_coords(top_right),\n",
    "                to_screen_coords(top_left)\n",
    "            ])\n",
    "\n",
    "        # Draw the agent in both grids\n",
    "        agent_position = self.state\n",
    "        transformed_agent_position = agent_position\n",
    "        pygame.draw.circle(self.screen, BLUE, to_screen_coords(agent_position), 10)  # Original\n",
    "        pygame.draw.circle(self.screen, GREEN, to_screen_coords(transformed_agent_position), 10)  # Transformed\n",
    "\n",
    "        # Draw the goal in both grids\n",
    "        goal_position = self.goal\n",
    "        transformed_goal_position = self.transform(goal_position)\n",
    "        pygame.draw.circle(self.screen, GREEN, to_screen_coords(goal_position), 12)  # Original\n",
    "        pygame.draw.circle(self.screen, YELLOW, to_screen_coords(transformed_goal_position), 12)  # Transformed\n",
    "\n",
    "        # Draw observation radius as a dashed circle around the agent\n",
    "        observation_radius = self.observation_radius # stays the same in both grids\n",
    "        pygame.draw.circle(self.screen, YELLOW, to_screen_coords(agent_position), \n",
    "                        int(self.observation_radius * scale), 1)  # Original\n",
    "        pygame.draw.circle(self.screen, YELLOW, to_screen_coords(transformed_agent_position), \n",
    "                        int(observation_radius * scale), 1)  # Transformed\n",
    "\n",
    "        # Update the display\n",
    "        pygame.display.flip()\n",
    "\n",
    "        # Handle key events\n",
    "        # Handle key events\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                # Press 'r' to reset environment\n",
    "                if event.key == pygame.K_r:\n",
    "                    self.reset()\n",
    "                # Press 'w' to quit\n",
    "                elif event.key == pygame.K_w:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "                # Press 's' to save current state\n",
    "                elif event.key == pygame.K_s:\n",
    "                    self.save_state()\n",
    "                # Press space to pause/resume\n",
    "                elif event.key == pygame.K_SPACE:\n",
    "                    self.pause()\n",
    "                # Press arrow keys for manual control\n",
    "                elif event.key == pygame.K_LEFT:\n",
    "                    return self.step(3)  # Left action\n",
    "                elif event.key == pygame.K_RIGHT:\n",
    "                    return self.step(2)  # Right action\n",
    "                elif event.key == pygame.K_UP:\n",
    "                    return self.step(0)  # Up action\n",
    "                elif event.key == pygame.K_DOWN:\n",
    "                    return self.step(1)  # Down action\n",
    "        return None, None, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "obstacles = [((0.14625, 0.3325), (0.565, 0.55625)), \n",
    "             ((0.52875, 0.5375), (0.7375, 0.84125)), ((0.0, 0.00125), (0.01625, 0.99125)), ((0.0075, 0.00125), (0.99875, 0.04)), ((0.98875, 0.0075), (0.99875, 1.0)), ((0.00125, 0.9825), (0.99875, 1.0))]\n",
    "\n",
    "# Example Usage\n",
    "obstacles = [\n",
    "    [(0.2, 0.2), (0.6, 0.6)],  # Obstacle 1\n",
    "    [(0.6, 0.6), (0.8, 0.8)]   # Obstacle 2\n",
    "]\n",
    "\n",
    "env = ObservableDeformedGridworld(\n",
    "    obstacles=obstacles, \n",
    "    stretch=(.5, .5),\n",
    "    shear=(.0, .0),\n",
    "    render_mode=\"human\"\n",
    ")\n",
    "\n",
    "# state, observation = env.reset(np.random.randint(100))\n",
    "env.reset(seed=np.random.randint(100))\n",
    "done = False\n",
    "while not done:\n",
    "    try:\n",
    "        _, _, terminated, truncated, _ = env.render()\n",
    "        if terminated or truncated:\n",
    "            env.close()\n",
    "            break\n",
    "    except:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# net_arch=[128, 128, 128]\n",
    "# model = DQN(\"MultiInputPolicy\",env, policy_kwargs=dict(net_arch=net_arch), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatteo-nunziante\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/flaccagora/Desktop/RoboSurgery/src/environment/wandb/run-20241209_165113-lt9zflyf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matteo-nunziante/DQNsb3%20-%20MDP%20-%20ObservableDeformedGridworld/runs/lt9zflyf' target=\"_blank\">azure-water-29</a></strong> to <a href='https://wandb.ai/matteo-nunziante/DQNsb3%20-%20MDP%20-%20ObservableDeformedGridworld' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matteo-nunziante/DQNsb3%20-%20MDP%20-%20ObservableDeformedGridworld' target=\"_blank\">https://wandb.ai/matteo-nunziante/DQNsb3%20-%20MDP%20-%20ObservableDeformedGridworld</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matteo-nunziante/DQNsb3%20-%20MDP%20-%20ObservableDeformedGridworld/runs/lt9zflyf' target=\"_blank\">https://wandb.ai/matteo-nunziante/DQNsb3%20-%20MDP%20-%20ObservableDeformedGridworld/runs/lt9zflyf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to runs/lt9zflyf/DQN_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44a42014d7e496f93706bc01019c14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -21.8    |\n",
      "|    exploration_rate | 0.998    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1        |\n",
      "|    fps              | 157      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 200      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -83.7    |\n",
      "|    exploration_rate | 0.996    |\n",
      "| time/               |          |\n",
      "|    episodes         | 2        |\n",
      "|    fps              | 190      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 400      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0457   |\n",
      "|    n_updates        | 1        |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -70.3    |\n",
      "|    exploration_rate | 0.994    |\n",
      "| time/               |          |\n",
      "|    episodes         | 3        |\n",
      "|    fps              | 194      |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 600      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.186    |\n",
      "|    n_updates        | 2        |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -68.3    |\n",
      "|    exploration_rate | 0.992    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 201      |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 800      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.19     |\n",
      "|    n_updates        | 3        |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -56.1    |\n",
      "|    exploration_rate | 0.991    |\n",
      "| time/               |          |\n",
      "|    episodes         | 5        |\n",
      "|    fps              | 204      |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.173    |\n",
      "|    n_updates        | 4        |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -60.4    |\n",
      "|    exploration_rate | 0.989    |\n",
      "| time/               |          |\n",
      "|    episodes         | 6        |\n",
      "|    fps              | 211      |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 1200     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.138    |\n",
      "|    n_updates        | 5        |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -66.5    |\n",
      "|    exploration_rate | 0.987    |\n",
      "| time/               |          |\n",
      "|    episodes         | 7        |\n",
      "|    fps              | 215      |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 1400     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.143    |\n",
      "|    n_updates        | 6        |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -58.4    |\n",
      "|    exploration_rate | 0.985    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 217      |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 1600     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.154    |\n",
      "|    n_updates        | 7        |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -66.4    |\n",
      "|    exploration_rate | 0.983    |\n",
      "| time/               |          |\n",
      "|    episodes         | 9        |\n",
      "|    fps              | 221      |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 1800     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.123    |\n",
      "|    n_updates        | 8        |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -63      |\n",
      "|    exploration_rate | 0.981    |\n",
      "| time/               |          |\n",
      "|    episodes         | 10       |\n",
      "|    fps              | 222      |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 2000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.187    |\n",
      "|    n_updates        | 9        |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -61      |\n",
      "|    exploration_rate | 0.979    |\n",
      "| time/               |          |\n",
      "|    episodes         | 11       |\n",
      "|    fps              | 223      |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 2200     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.162    |\n",
      "|    n_updates        | 10       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -63.8    |\n",
      "|    exploration_rate | 0.977    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 226      |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2400     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.115    |\n",
      "|    n_updates        | 11       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -63.5    |\n",
      "|    exploration_rate | 0.975    |\n",
      "| time/               |          |\n",
      "|    episodes         | 13       |\n",
      "|    fps              | 226      |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2600     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.138    |\n",
      "|    n_updates        | 12       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -68.8    |\n",
      "|    exploration_rate | 0.973    |\n",
      "| time/               |          |\n",
      "|    episodes         | 14       |\n",
      "|    fps              | 229      |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 2800     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.128    |\n",
      "|    n_updates        | 13       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -65.4    |\n",
      "|    exploration_rate | 0.972    |\n",
      "| time/               |          |\n",
      "|    episodes         | 15       |\n",
      "|    fps              | 228      |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 3000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.148    |\n",
      "|    n_updates        | 14       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -62.5    |\n",
      "|    exploration_rate | 0.97     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 228      |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 3200     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.114    |\n",
      "|    n_updates        | 15       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -61.4    |\n",
      "|    exploration_rate | 0.968    |\n",
      "| time/               |          |\n",
      "|    episodes         | 17       |\n",
      "|    fps              | 228      |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 3400     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0912   |\n",
      "|    n_updates        | 16       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -59      |\n",
      "|    exploration_rate | 0.966    |\n",
      "| time/               |          |\n",
      "|    episodes         | 18       |\n",
      "|    fps              | 228      |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 3600     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.116    |\n",
      "|    n_updates        | 17       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -56      |\n",
      "|    exploration_rate | 0.964    |\n",
      "| time/               |          |\n",
      "|    episodes         | 19       |\n",
      "|    fps              | 228      |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3800     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0854   |\n",
      "|    n_updates        | 18       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -55.3    |\n",
      "|    exploration_rate | 0.962    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 228      |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 4000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0741   |\n",
      "|    n_updates        | 19       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -60.5    |\n",
      "|    exploration_rate | 0.96     |\n",
      "| time/               |          |\n",
      "|    episodes         | 21       |\n",
      "|    fps              | 231      |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 4200     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.114    |\n",
      "|    n_updates        | 20       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -60.6    |\n",
      "|    exploration_rate | 0.958    |\n",
      "| time/               |          |\n",
      "|    episodes         | 22       |\n",
      "|    fps              | 231      |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 4400     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.122    |\n",
      "|    n_updates        | 21       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -59.2    |\n",
      "|    exploration_rate | 0.956    |\n",
      "| time/               |          |\n",
      "|    episodes         | 23       |\n",
      "|    fps              | 231      |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 4600     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.109    |\n",
      "|    n_updates        | 22       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -60.5    |\n",
      "|    exploration_rate | 0.954    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 232      |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 4800     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.143    |\n",
      "|    n_updates        | 23       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -58.9    |\n",
      "|    exploration_rate | 0.952    |\n",
      "| time/               |          |\n",
      "|    episodes         | 25       |\n",
      "|    fps              | 232      |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 5000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.114    |\n",
      "|    n_updates        | 24       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -63.1    |\n",
      "|    exploration_rate | 0.951    |\n",
      "| time/               |          |\n",
      "|    episodes         | 26       |\n",
      "|    fps              | 233      |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 5200     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.138    |\n",
      "|    n_updates        | 25       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -62.5    |\n",
      "|    exploration_rate | 0.949    |\n",
      "| time/               |          |\n",
      "|    episodes         | 27       |\n",
      "|    fps              | 234      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 5400     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.114    |\n",
      "|    n_updates        | 26       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -66.1    |\n",
      "|    exploration_rate | 0.947    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 235      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 5600     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.119    |\n",
      "|    n_updates        | 27       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -68.4    |\n",
      "|    exploration_rate | 0.945    |\n",
      "| time/               |          |\n",
      "|    episodes         | 29       |\n",
      "|    fps              | 236      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 5800     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.166    |\n",
      "|    n_updates        | 28       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -66.9    |\n",
      "|    exploration_rate | 0.943    |\n",
      "| time/               |          |\n",
      "|    episodes         | 30       |\n",
      "|    fps              | 236      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 6000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.144    |\n",
      "|    n_updates        | 29       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -64.8    |\n",
      "|    exploration_rate | 0.941    |\n",
      "| time/               |          |\n",
      "|    episodes         | 31       |\n",
      "|    fps              | 235      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 6200     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.139    |\n",
      "|    n_updates        | 30       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -63.2    |\n",
      "|    exploration_rate | 0.939    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 234      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 6400     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.169    |\n",
      "|    n_updates        | 31       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -62      |\n",
      "|    exploration_rate | 0.937    |\n",
      "| time/               |          |\n",
      "|    episodes         | 33       |\n",
      "|    fps              | 234      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 6600     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.155    |\n",
      "|    n_updates        | 32       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -61.7    |\n",
      "|    exploration_rate | 0.935    |\n",
      "| time/               |          |\n",
      "|    episodes         | 34       |\n",
      "|    fps              | 234      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 6800     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.144    |\n",
      "|    n_updates        | 33       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -60      |\n",
      "|    exploration_rate | 0.933    |\n",
      "| time/               |          |\n",
      "|    episodes         | 35       |\n",
      "|    fps              | 234      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 7000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.192    |\n",
      "|    n_updates        | 34       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -63.8    |\n",
      "|    exploration_rate | 0.932    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 235      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 7200     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.162    |\n",
      "|    n_updates        | 35       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -65      |\n",
      "|    exploration_rate | 0.93     |\n",
      "| time/               |          |\n",
      "|    episodes         | 37       |\n",
      "|    fps              | 236      |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 7400     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.157    |\n",
      "|    n_updates        | 36       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -65.4    |\n",
      "|    exploration_rate | 0.928    |\n",
      "| time/               |          |\n",
      "|    episodes         | 38       |\n",
      "|    fps              | 236      |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 7600     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.199    |\n",
      "|    n_updates        | 37       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -66      |\n",
      "|    exploration_rate | 0.926    |\n",
      "| time/               |          |\n",
      "|    episodes         | 39       |\n",
      "|    fps              | 237      |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 7800     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.191    |\n",
      "|    n_updates        | 38       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -68.3    |\n",
      "|    exploration_rate | 0.924    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 238      |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 8000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.217    |\n",
      "|    n_updates        | 39       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -66.7    |\n",
      "|    exploration_rate | 0.922    |\n",
      "| time/               |          |\n",
      "|    episodes         | 41       |\n",
      "|    fps              | 237      |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 8200     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.203    |\n",
      "|    n_updates        | 40       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -68.9    |\n",
      "|    exploration_rate | 0.92     |\n",
      "| time/               |          |\n",
      "|    episodes         | 42       |\n",
      "|    fps              | 238      |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 8400     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.282    |\n",
      "|    n_updates        | 41       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -69      |\n",
      "|    exploration_rate | 0.918    |\n",
      "| time/               |          |\n",
      "|    episodes         | 43       |\n",
      "|    fps              | 238      |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 8600     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.262    |\n",
      "|    n_updates        | 42       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -69.3    |\n",
      "|    exploration_rate | 0.916    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 239      |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 8800     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.257    |\n",
      "|    n_updates        | 43       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -69.1    |\n",
      "|    exploration_rate | 0.914    |\n",
      "| time/               |          |\n",
      "|    episodes         | 45       |\n",
      "|    fps              | 239      |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 9000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.252    |\n",
      "|    n_updates        | 44       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 98\u001b[0m\n\u001b[1;32m     94\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--f\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     96\u001b[0m args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args()\n\u001b[0;32m---> 98\u001b[0m \u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 80\u001b[0m, in \u001b[0;36mtrain_dqn\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     75\u001b[0m net_arch\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m]\n\u001b[1;32m     76\u001b[0m model \u001b[38;5;241m=\u001b[39m DQN(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiInputPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,env,batch_size\u001b[38;5;241m=\u001b[39mbatch_size,gamma\u001b[38;5;241m=\u001b[39mgamma, \n\u001b[1;32m     77\u001b[0m             target_update_interval\u001b[38;5;241m=\u001b[39mtarget_update, policy_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(net_arch\u001b[38;5;241m=\u001b[39mnet_arch), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     78\u001b[0m             tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m     79\u001b[0m             train_freq\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode\u001b[39m\u001b[38;5;124m\"\u001b[39m), gradient_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magents/pretrained/MDP/DQNsb3_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.miniconda3/envs/rob/lib/python3.9/site-packages/stable_baselines3/dqn/dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[1;32m    260\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDQN:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/envs/rob/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda3/envs/rob/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py:560\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    557\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    563\u001b[0m num_collected_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.miniconda3/envs/rob/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/envs/rob/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/.miniconda3/envs/rob/lib/python3.9/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "Cell \u001b[0;32mIn[3], line 281\u001b[0m, in \u001b[0;36mObservableDeformedGridworld.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    278\u001b[0m truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestep \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m \n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m state \u001b[38;5;241m=\u001b[39m OrderedDict({\n\u001b[1;32m    284\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate,\n\u001b[1;32m    285\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtheta\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformation_matrix\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[1;32m    286\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobserve_obstacle()\n\u001b[1;32m    287\u001b[0m         })\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# Return the transformed state, reward, and terminated truncated flag\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 443\u001b[0m, in \u001b[0;36mObservableDeformedGridworld.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m         pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mset_caption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeformed and Original Gridworld\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# Fill background with white\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWHITE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Compute the bounding box of the deformed grid\u001b[39;00m\n\u001b[1;32m    446\u001b[0m     corners \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    447\u001b[0m         np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m    448\u001b[0m         np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_size[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m    449\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_size,\n\u001b[1;32m    450\u001b[0m         np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_size[\u001b[38;5;241m1\u001b[39m]]),\n\u001b[1;32m    451\u001b[0m     ]\n",
      "\u001b[0;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "import numpy as np\n",
    "\n",
    "obstacles = [((0.14625, 0.3325), (0.565, 0.55625)), \n",
    "             ((0.52875, 0.5375), (0.7375, 0.84125)), \n",
    "             ((0.0, 0.00125), (0.01625, 0.99125)), \n",
    "             ((0.0075, 0.00125), (0.99875, 0.04)), \n",
    "             ((0.98875, 0.0075), (0.99875, 1.0)), \n",
    "             ((0.00125, 0.9825), (0.99875, 1.0))]\n",
    "\n",
    "\n",
    "def train_dqn(args):\n",
    "    from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "    from wandb.integration.sb3 import WandbCallback\n",
    "    import wandb\n",
    "\n",
    "    total_timesteps = args.total_timesteps\n",
    "    batch_size = args.batch_size\n",
    "    lr = args.learning_rate\n",
    "    target_update = args.target_update\n",
    "    gamma = args.gamma\n",
    "\n",
    "    config = {\n",
    "        \"policy_type\": \"MultiInputPolicy\",\n",
    "        \"env_name\": \"ObservableDeformedGridworld\",\n",
    "        \"total_timesteps\": total_timesteps,\n",
    "        \"Batch_Size\": batch_size,\n",
    "        'grid_size': (1.0,1.0),\n",
    "        'step_size': 0.1,\n",
    "        'obstacles':obstacles,\n",
    "        'observation_radius':0.2,\n",
    "    }\n",
    "    run = wandb.init(\n",
    "        project=\"DQNsb3 - MDP - ObservableDeformedGridworld\",\n",
    "        config=config,\n",
    "        sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "        monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
    "        save_code=True,  # optional\n",
    "    )\n",
    "\n",
    "\n",
    "    # Save a checkpoint every 10000 steps\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "                            save_freq=10000,\n",
    "                            save_path=f\"DQNsb3_{run.id}\",\n",
    "                            name_prefix=\"rl_model\",\n",
    "                            save_replay_buffer=False,\n",
    "                            save_vecnormalize=True,\n",
    "                        )\n",
    "\n",
    "    callbacks = [ WandbCallback(\n",
    "                                verbose=2,\n",
    "                                log=\"parameters\",\n",
    "                                ),\n",
    "                checkpoint_callback,\n",
    "                ]\n",
    "\n",
    "\n",
    "    from stable_baselines3.common.monitor import Monitor\n",
    "    from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
    "\n",
    "    def make_env():\n",
    "        env = ObservableDeformedGridworld(\n",
    "            grid_size=(1.0, 1.0),\n",
    "            obstacles=obstacles,\n",
    "            render_mode='human'\n",
    "        )\n",
    "\n",
    "        env = Monitor(env)  # record stats such as returns\n",
    "        return env\n",
    "\n",
    "    env = DummyVecEnv([make_env])\n",
    "\n",
    "\n",
    "    net_arch=[128, 128, 128]\n",
    "    model = DQN(\"MultiInputPolicy\",env,batch_size=batch_size,gamma=gamma, \n",
    "                target_update_interval=target_update, policy_kwargs=dict(net_arch=net_arch), verbose=1,\n",
    "                tensorboard_log=f\"runs/{run.id}\", device=\"cpu\", learning_rate=lr,\n",
    "                train_freq=(1,\"episode\"), gradient_steps=1)\n",
    "    model.learn(total_timesteps,progress_bar=True, callback=callbacks, log_interval=1)\n",
    "    model.save(f\"agents/pretrained/MDP/DQNsb3_{run.id}\")\n",
    "    env.close()\n",
    "    run.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=0.001)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--total_timesteps\", type=int, default=1000000) # env steps\n",
    "    parser.add_argument(\"--target_update\", type=int, default=555) # in env steps\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
    "    parser.add_argument(\"--f\", type=str, default=None)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    train_dqn(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (108027871.py, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[84], line 33\u001b[0;36m\u001b[0m\n\u001b[0;31m    plt.plot(point[], 'ro')\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
