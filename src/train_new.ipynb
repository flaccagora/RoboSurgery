{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import itertools\n",
    "\n",
    "from cartpole_tests.chat_dqn import DoubleDQNAgent\n",
    "from env import GridEnvDeform, POMDPWrapper_v0, create_maze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: 144, Actions: 4, Observations 32, Thetas [(1, 1), (2, 1)]\n",
      "\n",
      "setting reward function\n",
      "\n",
      "setting transition function\n",
      "\n",
      "setting observation function\n",
      "\n",
      "transition probability shape:  torch.Size([144, 4, 144])\n",
      "reward shape:  torch.Size([144, 4, 144])\n",
      "observation shape:  torch.Size([144, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "# maze size\n",
    "N = 2\n",
    "\n",
    "# thetas deformations (range(a,b),range(c,d))\n",
    "l0 = 1\n",
    "h0 = 3\n",
    "l1 = 1\n",
    "h1 = 2\n",
    "\n",
    "maze = create_maze(N)\n",
    "env = GridEnvDeform(maze,l0,h0,l1,h1)\n",
    "env.render()\n",
    "\n",
    "states = [((x,y,phi),(i,j)) for x in range(1,env.max_shape[0]-1) for y in range(1,env.max_shape[1]-1) for phi in range(4) for i in range(l0,h0) for j in range(l1,h1)] \n",
    "actions = [0,1,2,3]\n",
    "obs = list(itertools.product([0,1], repeat=5))\n",
    "thetas = [(i,j) for i in range(l0,h0) for j in range(l1,h1)]\n",
    "\n",
    "state_dict = {state: i for i, state in enumerate(states)}\n",
    "obs_dict = {obs : i for i, obs in enumerate(obs)}\n",
    "\n",
    "# Actions are: 0-listen, 1-open-left, 2-open-right\n",
    "lenS = len(states)\n",
    "lenA = len(actions)\n",
    "lenO = len(obs)\n",
    "\n",
    "print(f\"States: {lenS}, Actions: {lenA}, Observations {lenO}, Thetas {thetas}\\n\")\n",
    "\n",
    "\n",
    "print(\"setting reward function\\n\")\n",
    "R = torch.zeros(lenS,lenA,lenS)\n",
    "for s in range(lenS):\n",
    "    for a in range(lenA):\n",
    "        r = env.R(states[s],a)\n",
    "        for s_ in range(lenS):\n",
    "            R[s][a][s_] = r\n",
    "\n",
    "print(\"setting transition function\\n\")\n",
    "T = torch.zeros(lenS,lenA,lenS)\n",
    "for s, state in enumerate(states):\n",
    "    for a, action in enumerate(actions):\n",
    "        for s_, state_ in enumerate(states):\n",
    "            T[s,a,s_] = env.T(state,action,state_)\n",
    "\n",
    "print(\"setting observation function\\n\")\n",
    "O = torch.zeros(lenS,lenA,lenO)\n",
    "for s, state in enumerate(states):\n",
    "    for o, observation in enumerate(obs):\n",
    "        prob = env.O(state,action,observation)\n",
    "        for a, action in enumerate(actions):\n",
    "            O[s,a,o] = prob \n",
    "\n",
    "\n",
    "\n",
    "print(\"transition probability shape: \", T.shape)\n",
    "print(\"reward shape: \", R.shape)\n",
    "print(\"observation shape: \", O.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, agent, num_episodes=10):\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        c = 25\n",
    "        while c > 0:\n",
    "            # Render the environment\n",
    "            # env.render()\n",
    "\n",
    "            # Agent takes an action using a greedy policy (without exploration)\n",
    "            action = agent.choose_action([state])\n",
    "            obs, reward, done, info = env.step(state, action)\n",
    "            state = info['actual_state']\n",
    "\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done or c == 1:\n",
    "                total_rewards.append(episode_reward)\n",
    "                print(f\"Episode {episode + 1}/{num_episodes}, Reward: {episode_reward}\")\n",
    "\n",
    "            c -= 1\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"Average Reward over {num_episodes} episodes: {avg_reward}\")\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 1\n",
    "action_dim = 4\n",
    "\n",
    "agent = DoubleDQNAgent(state_dim, action_dim)\n",
    "\n",
    "env_wrapper = POMDPWrapper_v0(env, agent, T, O, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/500, Reward: -111.0, Epsilon: 0.99\n",
      "Episode 2/500, Reward: -143.5, Epsilon: 0.99\n",
      "Episode 3/500, Reward: -123.5, Epsilon: 0.99\n",
      "Episode 4/500, Reward: -124.5, Epsilon: 0.98\n",
      "Episode 5/500, Reward: -110.5, Epsilon: 0.98\n",
      "Episode 6/500, Reward: -70.5, Epsilon: 0.97\n",
      "Episode 7/500, Reward: -126.5, Epsilon: 0.97\n",
      "Episode 8/500, Reward: -83.0, Epsilon: 0.96\n",
      "Episode 9/500, Reward: -63.5, Epsilon: 0.96\n",
      "Episode 10/500, Reward: -156.5, Epsilon: 0.95\n",
      "Episode 11/500, Reward: -117.5, Epsilon: 0.95\n",
      "Episode 12/500, Reward: -111.0, Epsilon: 0.94\n",
      "Episode 13/500, Reward: -77.5, Epsilon: 0.94\n",
      "Episode 14/500, Reward: -78.5, Epsilon: 0.93\n",
      "Episode 15/500, Reward: -116.5, Epsilon: 0.93\n",
      "Episode 16/500, Reward: -87.5, Epsilon: 0.92\n",
      "Episode 17/500, Reward: -123.0, Epsilon: 0.92\n",
      "Episode 18/500, Reward: -90.5, Epsilon: 0.91\n",
      "Episode 19/500, Reward: -131.0, Epsilon: 0.91\n",
      "Episode 20/500, Reward: -115.0, Epsilon: 0.90\n",
      "Episode 21/500, Reward: -134.5, Epsilon: 0.90\n",
      "Episode 1/10, Reward: -29.5\n",
      "Episode 2/10, Reward: -25.0\n",
      "Episode 3/10, Reward: -31.0\n",
      "Episode 4/10, Reward: -23.0\n",
      "Episode 5/10, Reward: -17.0\n",
      "Episode 6/10, Reward: -38.5\n",
      "Episode 7/10, Reward: -22.5\n",
      "Episode 8/10, Reward: -38.0\n",
      "Episode 9/10, Reward: -38.0\n",
      "Episode 10/10, Reward: -29.0\n",
      "Average Reward over 10 episodes: -29.15\n",
      "Episode 21/500, Average Reward: -29.15\n",
      "Episode 22/500, Reward: -165.0, Epsilon: 0.90\n",
      "Episode 23/500, Reward: -89.5, Epsilon: 0.89\n",
      "Episode 24/500, Reward: -95.5, Epsilon: 0.89\n",
      "Episode 25/500, Reward: -109.0, Epsilon: 0.88\n",
      "Episode 26/500, Reward: -89.5, Epsilon: 0.88\n",
      "Episode 27/500, Reward: -155.0, Epsilon: 0.87\n",
      "Episode 28/500, Reward: -83.5, Epsilon: 0.87\n",
      "Episode 29/500, Reward: -122.0, Epsilon: 0.86\n",
      "Episode 30/500, Reward: -91.0, Epsilon: 0.86\n",
      "Episode 31/500, Reward: -122.5, Epsilon: 0.86\n",
      "Episode 32/500, Reward: -89.0, Epsilon: 0.85\n",
      "Episode 33/500, Reward: -149.0, Epsilon: 0.85\n",
      "Episode 34/500, Reward: -52.5, Epsilon: 0.84\n",
      "Episode 35/500, Reward: -129.0, Epsilon: 0.84\n",
      "Episode 36/500, Reward: -133.0, Epsilon: 0.83\n",
      "Episode 37/500, Reward: -140.0, Epsilon: 0.83\n",
      "Episode 38/500, Reward: -38.5, Epsilon: 0.83\n",
      "Episode 39/500, Reward: -72.0, Epsilon: 0.82\n",
      "Episode 40/500, Reward: -131.5, Epsilon: 0.82\n",
      "Episode 41/500, Reward: -142.0, Epsilon: 0.81\n",
      "Episode 1/10, Reward: -25.5\n",
      "Episode 2/10, Reward: -13.0\n",
      "Episode 3/10, Reward: -37.0\n",
      "Episode 4/10, Reward: -14.0\n",
      "Episode 5/10, Reward: -18.5\n",
      "Episode 6/10, Reward: -35.0\n",
      "Episode 7/10, Reward: -20.0\n",
      "Episode 8/10, Reward: -21.0\n",
      "Episode 9/10, Reward: -32.5\n",
      "Episode 10/10, Reward: -15.5\n",
      "Average Reward over 10 episodes: -23.2\n",
      "Episode 41/500, Average Reward: -23.2\n",
      "Episode 42/500, Reward: -52.5, Epsilon: 0.81\n",
      "Episode 43/500, Reward: -78.0, Epsilon: 0.81\n",
      "Episode 44/500, Reward: -92.5, Epsilon: 0.80\n",
      "Episode 45/500, Reward: -79.5, Epsilon: 0.80\n",
      "Episode 46/500, Reward: -100.5, Epsilon: 0.79\n",
      "Episode 47/500, Reward: -75.0, Epsilon: 0.79\n",
      "Episode 48/500, Reward: -92.0, Epsilon: 0.79\n",
      "Episode 49/500, Reward: -69.0, Epsilon: 0.78\n",
      "Episode 50/500, Reward: -134.0, Epsilon: 0.78\n",
      "Episode 51/500, Reward: -112.5, Epsilon: 0.77\n",
      "Episode 52/500, Reward: -73.0, Epsilon: 0.77\n",
      "Episode 53/500, Reward: -153.5, Epsilon: 0.77\n",
      "Episode 54/500, Reward: -153.5, Epsilon: 0.76\n",
      "Episode 55/500, Reward: -46.5, Epsilon: 0.76\n",
      "Episode 56/500, Reward: -117.0, Epsilon: 0.76\n",
      "Episode 57/500, Reward: -86.0, Epsilon: 0.75\n",
      "Episode 58/500, Reward: -103.0, Epsilon: 0.75\n",
      "Episode 59/500, Reward: -89.0, Epsilon: 0.74\n",
      "Episode 60/500, Reward: -134.0, Epsilon: 0.74\n",
      "Episode 61/500, Reward: -111.0, Epsilon: 0.74\n",
      "Episode 1/10, Reward: -11.0\n",
      "Episode 2/10, Reward: -26.5\n",
      "Episode 3/10, Reward: -35.5\n",
      "Episode 4/10, Reward: -12.0\n",
      "Episode 5/10, Reward: -27.0\n",
      "Episode 6/10, Reward: -38.0\n",
      "Episode 7/10, Reward: -18.0\n",
      "Episode 8/10, Reward: -24.5\n",
      "Episode 9/10, Reward: -22.5\n",
      "Episode 10/10, Reward: -6.5\n",
      "Average Reward over 10 episodes: -22.15\n",
      "Episode 61/500, Average Reward: -22.15\n",
      "Episode 62/500, Reward: -83.0, Epsilon: 0.73\n",
      "Episode 63/500, Reward: -98.0, Epsilon: 0.73\n",
      "Episode 64/500, Reward: -74.5, Epsilon: 0.73\n",
      "Episode 65/500, Reward: -48.5, Epsilon: 0.72\n",
      "Episode 66/500, Reward: -123.0, Epsilon: 0.72\n",
      "Episode 67/500, Reward: -120.5, Epsilon: 0.71\n",
      "Episode 68/500, Reward: -160.5, Epsilon: 0.71\n",
      "Episode 69/500, Reward: -88.5, Epsilon: 0.71\n",
      "Episode 70/500, Reward: -120.5, Epsilon: 0.70\n",
      "Episode 71/500, Reward: -138.5, Epsilon: 0.70\n",
      "Episode 72/500, Reward: -123.5, Epsilon: 0.70\n",
      "Episode 73/500, Reward: -103.0, Epsilon: 0.69\n",
      "Episode 74/500, Reward: -49.5, Epsilon: 0.69\n",
      "Episode 75/500, Reward: -68.5, Epsilon: 0.69\n",
      "Episode 76/500, Reward: -71.5, Epsilon: 0.68\n",
      "Episode 77/500, Reward: -131.5, Epsilon: 0.68\n",
      "Episode 78/500, Reward: -63.0, Epsilon: 0.68\n",
      "Episode 79/500, Reward: -83.5, Epsilon: 0.67\n",
      "Episode 80/500, Reward: -133.0, Epsilon: 0.67\n",
      "Episode 81/500, Reward: -43.5, Epsilon: 0.67\n",
      "Episode 1/10, Reward: -19.5\n",
      "Episode 2/10, Reward: -17.5\n",
      "Episode 3/10, Reward: -39.0\n",
      "Episode 4/10, Reward: -28.5\n",
      "Episode 5/10, Reward: 3.5\n",
      "Episode 6/10, Reward: -22.0\n",
      "Episode 7/10, Reward: -42.0\n",
      "Episode 8/10, Reward: -24.5\n",
      "Episode 9/10, Reward: 5.5\n",
      "Episode 10/10, Reward: -24.5\n",
      "Average Reward over 10 episodes: -20.85\n",
      "Episode 81/500, Average Reward: -20.85\n",
      "Episode 82/500, Reward: -88.0, Epsilon: 0.66\n",
      "Episode 83/500, Reward: -41.0, Epsilon: 0.66\n",
      "Episode 84/500, Reward: -32.5, Epsilon: 0.66\n",
      "Episode 85/500, Reward: -142.5, Epsilon: 0.65\n",
      "Episode 86/500, Reward: -134.5, Epsilon: 0.65\n",
      "Episode 87/500, Reward: -87.0, Epsilon: 0.65\n",
      "Episode 88/500, Reward: -112.0, Epsilon: 0.64\n",
      "Episode 89/500, Reward: -54.5, Epsilon: 0.64\n",
      "Episode 90/500, Reward: -66.5, Epsilon: 0.64\n",
      "Episode 91/500, Reward: -110.0, Epsilon: 0.63\n",
      "Episode 92/500, Reward: -104.0, Epsilon: 0.63\n",
      "Episode 93/500, Reward: -82.5, Epsilon: 0.63\n",
      "Episode 94/500, Reward: -90.0, Epsilon: 0.62\n",
      "Episode 95/500, Reward: -97.0, Epsilon: 0.62\n",
      "Episode 96/500, Reward: -23.0, Epsilon: 0.62\n",
      "Episode 97/500, Reward: -113.0, Epsilon: 0.61\n",
      "Episode 98/500, Reward: -75.5, Epsilon: 0.61\n",
      "Episode 99/500, Reward: -76.5, Epsilon: 0.61\n",
      "Episode 100/500, Reward: -64.5, Epsilon: 0.61\n",
      "Episode 101/500, Reward: -47.0, Epsilon: 0.60\n",
      "Episode 1/10, Reward: -22.5\n",
      "Episode 2/10, Reward: -34.0\n",
      "Episode 3/10, Reward: -14.5\n",
      "Episode 4/10, Reward: -29.0\n",
      "Episode 5/10, Reward: -23.0\n",
      "Episode 6/10, Reward: -31.0\n",
      "Episode 7/10, Reward: -8.5\n",
      "Episode 8/10, Reward: -9.5\n",
      "Episode 9/10, Reward: -31.5\n",
      "Episode 10/10, Reward: -41.0\n",
      "Average Reward over 10 episodes: -24.45\n",
      "Episode 101/500, Average Reward: -24.45\n",
      "Episode 102/500, Reward: -66.0, Epsilon: 0.60\n",
      "Episode 103/500, Reward: -74.5, Epsilon: 0.60\n",
      "Episode 104/500, Reward: -122.0, Epsilon: 0.59\n",
      "Episode 105/500, Reward: -108.5, Epsilon: 0.59\n",
      "Episode 106/500, Reward: -75.0, Epsilon: 0.59\n",
      "Episode 107/500, Reward: -81.5, Epsilon: 0.58\n",
      "Episode 108/500, Reward: -66.5, Epsilon: 0.58\n",
      "Episode 109/500, Reward: -100.0, Epsilon: 0.58\n",
      "Episode 110/500, Reward: -61.5, Epsilon: 0.58\n",
      "Episode 111/500, Reward: -47.5, Epsilon: 0.57\n",
      "Episode 112/500, Reward: -89.0, Epsilon: 0.57\n",
      "Episode 113/500, Reward: -80.5, Epsilon: 0.57\n",
      "Episode 114/500, Reward: -39.0, Epsilon: 0.56\n",
      "Episode 115/500, Reward: -133.0, Epsilon: 0.56\n",
      "Episode 116/500, Reward: -98.5, Epsilon: 0.56\n",
      "Episode 117/500, Reward: -97.5, Epsilon: 0.56\n",
      "Episode 118/500, Reward: -88.0, Epsilon: 0.55\n",
      "Episode 119/500, Reward: -76.5, Epsilon: 0.55\n",
      "Episode 120/500, Reward: -165.5, Epsilon: 0.55\n",
      "Episode 121/500, Reward: -101.5, Epsilon: 0.55\n",
      "Episode 1/10, Reward: -26.0\n",
      "Episode 2/10, Reward: -40.5\n",
      "Episode 3/10, Reward: -21.5\n",
      "Episode 4/10, Reward: -23.5\n",
      "Episode 5/10, Reward: -17.5\n",
      "Episode 6/10, Reward: -26.5\n",
      "Episode 7/10, Reward: -13.5\n",
      "Episode 8/10, Reward: -24.0\n",
      "Episode 9/10, Reward: -37.0\n",
      "Episode 10/10, Reward: -35.0\n",
      "Average Reward over 10 episodes: -26.5\n",
      "Episode 121/500, Average Reward: -26.5\n",
      "Episode 122/500, Reward: -106.0, Epsilon: 0.54\n",
      "Episode 123/500, Reward: -67.0, Epsilon: 0.54\n",
      "Episode 124/500, Reward: -93.5, Epsilon: 0.54\n",
      "Episode 125/500, Reward: -86.0, Epsilon: 0.53\n",
      "Episode 126/500, Reward: -40.0, Epsilon: 0.53\n",
      "Episode 127/500, Reward: -106.5, Epsilon: 0.53\n",
      "Episode 128/500, Reward: -115.0, Epsilon: 0.53\n",
      "Episode 129/500, Reward: -13.0, Epsilon: 0.52\n",
      "Episode 130/500, Reward: -76.5, Epsilon: 0.52\n",
      "Episode 131/500, Reward: -103.5, Epsilon: 0.52\n",
      "Episode 132/500, Reward: -33.5, Epsilon: 0.52\n",
      "Episode 133/500, Reward: -118.0, Epsilon: 0.51\n",
      "Episode 134/500, Reward: -98.5, Epsilon: 0.51\n",
      "Episode 135/500, Reward: -164.0, Epsilon: 0.51\n",
      "Episode 136/500, Reward: -99.0, Epsilon: 0.51\n",
      "Episode 137/500, Reward: -99.5, Epsilon: 0.50\n",
      "Episode 138/500, Reward: -122.0, Epsilon: 0.50\n",
      "Episode 139/500, Reward: -85.5, Epsilon: 0.50\n",
      "Episode 140/500, Reward: -101.5, Epsilon: 0.50\n",
      "Episode 141/500, Reward: -73.0, Epsilon: 0.49\n",
      "Episode 1/10, Reward: 1.5\n",
      "Episode 2/10, Reward: -8.0\n",
      "Episode 3/10, Reward: -20.0\n",
      "Episode 4/10, Reward: -4.5\n",
      "Episode 5/10, Reward: -13.5\n",
      "Episode 6/10, Reward: -20.5\n",
      "Episode 7/10, Reward: -32.0\n",
      "Episode 8/10, Reward: -21.0\n",
      "Episode 9/10, Reward: -1.0\n",
      "Episode 10/10, Reward: -33.5\n",
      "Average Reward over 10 episodes: -15.25\n",
      "Episode 141/500, Average Reward: -15.25\n",
      "Episode 142/500, Reward: -47.0, Epsilon: 0.49\n",
      "Episode 143/500, Reward: -117.0, Epsilon: 0.49\n",
      "Episode 144/500, Reward: -80.0, Epsilon: 0.49\n",
      "Episode 145/500, Reward: -146.0, Epsilon: 0.48\n",
      "Episode 146/500, Reward: -144.5, Epsilon: 0.48\n",
      "Episode 147/500, Reward: -80.5, Epsilon: 0.48\n",
      "Episode 148/500, Reward: -111.0, Epsilon: 0.48\n",
      "Episode 149/500, Reward: -49.0, Epsilon: 0.47\n",
      "Episode 150/500, Reward: -78.0, Epsilon: 0.47\n",
      "Episode 151/500, Reward: -141.0, Epsilon: 0.47\n",
      "Episode 152/500, Reward: -120.0, Epsilon: 0.47\n",
      "Episode 153/500, Reward: -121.0, Epsilon: 0.46\n",
      "Episode 154/500, Reward: -58.5, Epsilon: 0.46\n",
      "Episode 155/500, Reward: -92.0, Epsilon: 0.46\n",
      "Episode 156/500, Reward: -88.0, Epsilon: 0.46\n",
      "Episode 157/500, Reward: -36.0, Epsilon: 0.46\n",
      "Episode 158/500, Reward: -71.5, Epsilon: 0.45\n",
      "Episode 159/500, Reward: -77.0, Epsilon: 0.45\n",
      "Episode 160/500, Reward: -109.0, Epsilon: 0.45\n",
      "Episode 161/500, Reward: -114.5, Epsilon: 0.45\n",
      "Episode 1/10, Reward: -32.5\n",
      "Episode 2/10, Reward: -28.5\n",
      "Episode 3/10, Reward: -15.5\n",
      "Episode 4/10, Reward: -13.5\n",
      "Episode 5/10, Reward: -16.0\n",
      "Episode 6/10, Reward: -11.5\n",
      "Episode 7/10, Reward: -35.5\n",
      "Episode 8/10, Reward: -36.5\n",
      "Episode 9/10, Reward: -24.5\n",
      "Episode 10/10, Reward: -31.0\n",
      "Average Reward over 10 episodes: -24.5\n",
      "Episode 161/500, Average Reward: -24.5\n",
      "Episode 162/500, Reward: -77.5, Epsilon: 0.44\n",
      "Episode 163/500, Reward: -74.5, Epsilon: 0.44\n",
      "Episode 164/500, Reward: -72.5, Epsilon: 0.44\n",
      "Episode 165/500, Reward: -77.5, Epsilon: 0.44\n",
      "Episode 166/500, Reward: -9.0, Epsilon: 0.44\n",
      "Episode 167/500, Reward: -105.5, Epsilon: 0.43\n",
      "Episode 168/500, Reward: -60.5, Epsilon: 0.43\n",
      "Episode 169/500, Reward: -124.0, Epsilon: 0.43\n",
      "Episode 170/500, Reward: -18.0, Epsilon: 0.43\n",
      "Episode 171/500, Reward: -65.0, Epsilon: 0.42\n",
      "Episode 172/500, Reward: -84.0, Epsilon: 0.42\n",
      "Episode 173/500, Reward: -21.0, Epsilon: 0.42\n",
      "Episode 174/500, Reward: -63.0, Epsilon: 0.42\n",
      "Episode 175/500, Reward: -27.5, Epsilon: 0.42\n",
      "Episode 176/500, Reward: -69.0, Epsilon: 0.41\n",
      "Episode 177/500, Reward: -100.5, Epsilon: 0.41\n",
      "Episode 178/500, Reward: -109.0, Epsilon: 0.41\n",
      "Episode 179/500, Reward: -98.0, Epsilon: 0.41\n",
      "Episode 180/500, Reward: -90.0, Epsilon: 0.41\n",
      "Episode 181/500, Reward: -115.5, Epsilon: 0.40\n",
      "Episode 1/10, Reward: -17.0\n",
      "Episode 2/10, Reward: -23.0\n",
      "Episode 3/10, Reward: -31.5\n",
      "Episode 4/10, Reward: -23.5\n",
      "Episode 5/10, Reward: -5.5\n",
      "Episode 6/10, Reward: -15.5\n",
      "Episode 7/10, Reward: -22.0\n",
      "Episode 8/10, Reward: -18.5\n",
      "Episode 9/10, Reward: -10.0\n",
      "Episode 10/10, Reward: -47.0\n",
      "Average Reward over 10 episodes: -21.35\n",
      "Episode 181/500, Average Reward: -21.35\n",
      "Episode 182/500, Reward: -84.5, Epsilon: 0.40\n",
      "Episode 183/500, Reward: -114.5, Epsilon: 0.40\n",
      "Episode 184/500, Reward: -104.0, Epsilon: 0.40\n",
      "Episode 185/500, Reward: -121.5, Epsilon: 0.40\n",
      "Episode 186/500, Reward: -52.0, Epsilon: 0.39\n",
      "Episode 187/500, Reward: -79.0, Epsilon: 0.39\n",
      "Episode 188/500, Reward: -101.5, Epsilon: 0.39\n",
      "Episode 189/500, Reward: -7.5, Epsilon: 0.39\n",
      "Episode 190/500, Reward: -99.0, Epsilon: 0.39\n",
      "Episode 191/500, Reward: -155.5, Epsilon: 0.38\n",
      "Episode 192/500, Reward: -89.0, Epsilon: 0.38\n",
      "Episode 193/500, Reward: -114.0, Epsilon: 0.38\n",
      "Episode 194/500, Reward: -117.5, Epsilon: 0.38\n",
      "Episode 195/500, Reward: -70.5, Epsilon: 0.38\n",
      "Episode 196/500, Reward: -69.0, Epsilon: 0.37\n",
      "Episode 197/500, Reward: -118.5, Epsilon: 0.37\n",
      "Episode 198/500, Reward: -124.5, Epsilon: 0.37\n",
      "Episode 199/500, Reward: -100.0, Epsilon: 0.37\n",
      "Episode 200/500, Reward: -75.5, Epsilon: 0.37\n",
      "Episode 201/500, Reward: -96.5, Epsilon: 0.37\n",
      "Episode 1/10, Reward: -28.5\n",
      "Episode 2/10, Reward: -28.5\n",
      "Episode 3/10, Reward: -26.5\n",
      "Episode 4/10, Reward: -34.0\n",
      "Episode 5/10, Reward: -28.0\n",
      "Episode 6/10, Reward: -20.0\n",
      "Episode 7/10, Reward: -28.0\n",
      "Episode 8/10, Reward: -26.5\n",
      "Episode 9/10, Reward: -0.5\n",
      "Episode 10/10, Reward: -21.0\n",
      "Average Reward over 10 episodes: -24.15\n",
      "Episode 201/500, Average Reward: -24.15\n",
      "Episode 202/500, Reward: -92.5, Epsilon: 0.36\n",
      "Episode 203/500, Reward: -124.5, Epsilon: 0.36\n",
      "Episode 204/500, Reward: -110.5, Epsilon: 0.36\n",
      "Episode 205/500, Reward: -75.5, Epsilon: 0.36\n",
      "Episode 206/500, Reward: -130.5, Epsilon: 0.36\n",
      "Episode 207/500, Reward: -112.5, Epsilon: 0.35\n",
      "Episode 208/500, Reward: -140.0, Epsilon: 0.35\n",
      "Episode 209/500, Reward: -77.0, Epsilon: 0.35\n",
      "Episode 210/500, Reward: -73.0, Epsilon: 0.35\n",
      "Episode 211/500, Reward: -82.5, Epsilon: 0.35\n",
      "Episode 212/500, Reward: -57.0, Epsilon: 0.35\n",
      "Episode 213/500, Reward: -28.5, Epsilon: 0.34\n",
      "Episode 214/500, Reward: -114.5, Epsilon: 0.34\n",
      "Episode 215/500, Reward: -25.0, Epsilon: 0.34\n",
      "Episode 216/500, Reward: -118.5, Epsilon: 0.34\n",
      "Episode 217/500, Reward: -37.0, Epsilon: 0.34\n",
      "Episode 218/500, Reward: -73.0, Epsilon: 0.34\n",
      "Episode 219/500, Reward: -79.0, Epsilon: 0.33\n",
      "Episode 220/500, Reward: -130.0, Epsilon: 0.33\n",
      "Episode 221/500, Reward: -75.0, Epsilon: 0.33\n",
      "Episode 1/10, Reward: -10.5\n",
      "Episode 2/10, Reward: -20.0\n",
      "Episode 3/10, Reward: -26.5\n",
      "Episode 4/10, Reward: -3.5\n",
      "Episode 5/10, Reward: -23.0\n",
      "Episode 6/10, Reward: 20.0\n",
      "Episode 7/10, Reward: -47.0\n",
      "Episode 8/10, Reward: -32.0\n",
      "Episode 9/10, Reward: -14.0\n",
      "Episode 10/10, Reward: 6.0\n",
      "Average Reward over 10 episodes: -15.05\n",
      "Episode 221/500, Average Reward: -15.05\n",
      "Episode 222/500, Reward: -79.0, Epsilon: 0.33\n",
      "Episode 223/500, Reward: -155.0, Epsilon: 0.33\n",
      "Episode 224/500, Reward: -7.0, Epsilon: 0.33\n",
      "Episode 225/500, Reward: -88.5, Epsilon: 0.32\n",
      "Episode 226/500, Reward: -63.0, Epsilon: 0.32\n",
      "Episode 227/500, Reward: -65.5, Epsilon: 0.32\n",
      "Episode 228/500, Reward: 0.0, Epsilon: 0.32\n",
      "Episode 229/500, Reward: -62.5, Epsilon: 0.32\n",
      "Episode 230/500, Reward: -140.5, Epsilon: 0.32\n",
      "Episode 231/500, Reward: -36.5, Epsilon: 0.31\n",
      "Episode 232/500, Reward: -40.5, Epsilon: 0.31\n",
      "Episode 233/500, Reward: -84.0, Epsilon: 0.31\n",
      "Episode 234/500, Reward: -101.5, Epsilon: 0.31\n",
      "Episode 235/500, Reward: -115.5, Epsilon: 0.31\n",
      "Episode 236/500, Reward: -108.5, Epsilon: 0.31\n",
      "Episode 237/500, Reward: -76.5, Epsilon: 0.30\n",
      "Episode 238/500, Reward: -117.5, Epsilon: 0.30\n",
      "Episode 239/500, Reward: -60.0, Epsilon: 0.30\n",
      "Episode 240/500, Reward: -79.5, Epsilon: 0.30\n",
      "Episode 241/500, Reward: -59.5, Epsilon: 0.30\n",
      "Episode 1/10, Reward: 1.0\n",
      "Episode 2/10, Reward: -14.5\n",
      "Episode 3/10, Reward: -11.0\n",
      "Episode 4/10, Reward: -45.0\n",
      "Episode 5/10, Reward: -22.0\n",
      "Episode 6/10, Reward: -13.0\n",
      "Episode 7/10, Reward: -26.5\n",
      "Episode 8/10, Reward: -3.5\n",
      "Episode 9/10, Reward: -28.0\n",
      "Episode 10/10, Reward: -10.0\n",
      "Average Reward over 10 episodes: -17.25\n",
      "Episode 241/500, Average Reward: -17.25\n",
      "Episode 242/500, Reward: -41.0, Epsilon: 0.30\n",
      "Episode 243/500, Reward: -99.5, Epsilon: 0.30\n",
      "Episode 244/500, Reward: -94.5, Epsilon: 0.29\n",
      "Episode 245/500, Reward: -44.5, Epsilon: 0.29\n",
      "Episode 246/500, Reward: -18.0, Epsilon: 0.29\n",
      "Episode 247/500, Reward: 10.0, Epsilon: 0.29\n",
      "Episode 248/500, Reward: -115.0, Epsilon: 0.29\n",
      "Episode 249/500, Reward: -27.0, Epsilon: 0.29\n",
      "Episode 250/500, Reward: -81.0, Epsilon: 0.29\n",
      "Episode 251/500, Reward: -151.5, Epsilon: 0.28\n",
      "Episode 252/500, Reward: -55.5, Epsilon: 0.28\n",
      "Episode 253/500, Reward: -110.5, Epsilon: 0.28\n",
      "Episode 254/500, Reward: -119.5, Epsilon: 0.28\n",
      "Episode 255/500, Reward: 16.0, Epsilon: 0.28\n",
      "Episode 256/500, Reward: -78.5, Epsilon: 0.28\n",
      "Episode 257/500, Reward: -80.0, Epsilon: 0.28\n",
      "Episode 258/500, Reward: -78.0, Epsilon: 0.27\n",
      "Episode 259/500, Reward: -139.5, Epsilon: 0.27\n",
      "Episode 260/500, Reward: -126.0, Epsilon: 0.27\n",
      "Episode 261/500, Reward: -81.0, Epsilon: 0.27\n",
      "Episode 1/10, Reward: -20.5\n",
      "Episode 2/10, Reward: 12.5\n",
      "Episode 3/10, Reward: -17.0\n",
      "Episode 4/10, Reward: 12.0\n",
      "Episode 5/10, Reward: -47.0\n",
      "Episode 6/10, Reward: -16.0\n",
      "Episode 7/10, Reward: -13.0\n",
      "Episode 8/10, Reward: -29.0\n",
      "Episode 9/10, Reward: 2.5\n",
      "Episode 10/10, Reward: -23.0\n",
      "Average Reward over 10 episodes: -13.85\n",
      "Episode 261/500, Average Reward: -13.85\n",
      "Episode 262/500, Reward: -122.0, Epsilon: 0.27\n",
      "Episode 263/500, Reward: -21.5, Epsilon: 0.27\n",
      "Episode 264/500, Reward: -61.0, Epsilon: 0.27\n",
      "Episode 265/500, Reward: -125.5, Epsilon: 0.26\n",
      "Episode 266/500, Reward: -15.5, Epsilon: 0.26\n",
      "Episode 267/500, Reward: -56.5, Epsilon: 0.26\n",
      "Episode 268/500, Reward: -25.0, Epsilon: 0.26\n",
      "Episode 269/500, Reward: -1.5, Epsilon: 0.26\n",
      "Episode 270/500, Reward: -63.0, Epsilon: 0.26\n",
      "Episode 271/500, Reward: -94.5, Epsilon: 0.26\n",
      "Episode 272/500, Reward: -84.5, Epsilon: 0.26\n",
      "Episode 273/500, Reward: -114.0, Epsilon: 0.25\n",
      "Episode 274/500, Reward: 17.5, Epsilon: 0.25\n",
      "Episode 275/500, Reward: 37.0, Epsilon: 0.25\n",
      "Episode 276/500, Reward: -99.0, Epsilon: 0.25\n",
      "Episode 277/500, Reward: -87.0, Epsilon: 0.25\n",
      "Episode 278/500, Reward: 55.5, Epsilon: 0.25\n",
      "Episode 279/500, Reward: -145.0, Epsilon: 0.25\n",
      "Episode 280/500, Reward: -73.5, Epsilon: 0.25\n",
      "Episode 281/500, Reward: -100.0, Epsilon: 0.24\n",
      "Episode 1/10, Reward: -48.0\n",
      "Episode 2/10, Reward: -30.5\n",
      "Episode 3/10, Reward: -20.5\n",
      "Episode 4/10, Reward: -16.5\n",
      "Episode 5/10, Reward: -33.0\n",
      "Episode 6/10, Reward: -13.0\n",
      "Episode 7/10, Reward: -49.0\n",
      "Episode 8/10, Reward: -13.5\n",
      "Episode 9/10, Reward: -13.5\n",
      "Episode 10/10, Reward: -20.0\n",
      "Average Reward over 10 episodes: -25.75\n",
      "Episode 281/500, Average Reward: -25.75\n",
      "Episode 282/500, Reward: -114.5, Epsilon: 0.24\n",
      "Episode 283/500, Reward: -50.5, Epsilon: 0.24\n",
      "Episode 284/500, Reward: 6.5, Epsilon: 0.24\n",
      "Episode 285/500, Reward: -23.5, Epsilon: 0.24\n",
      "Episode 286/500, Reward: -61.5, Epsilon: 0.24\n",
      "Episode 287/500, Reward: -103.5, Epsilon: 0.24\n",
      "Episode 288/500, Reward: -56.5, Epsilon: 0.24\n",
      "Episode 289/500, Reward: -51.5, Epsilon: 0.23\n",
      "Episode 290/500, Reward: -22.0, Epsilon: 0.23\n",
      "Episode 291/500, Reward: -118.5, Epsilon: 0.23\n",
      "Episode 292/500, Reward: -109.5, Epsilon: 0.23\n",
      "Episode 293/500, Reward: -130.0, Epsilon: 0.23\n",
      "Episode 294/500, Reward: 14.0, Epsilon: 0.23\n",
      "Episode 295/500, Reward: -66.5, Epsilon: 0.23\n",
      "Episode 296/500, Reward: -25.5, Epsilon: 0.23\n",
      "Episode 297/500, Reward: -41.0, Epsilon: 0.23\n",
      "Episode 298/500, Reward: -83.5, Epsilon: 0.22\n",
      "Episode 299/500, Reward: -7.5, Epsilon: 0.22\n",
      "Episode 300/500, Reward: -117.0, Epsilon: 0.22\n",
      "Episode 301/500, Reward: -101.5, Epsilon: 0.22\n",
      "Episode 1/10, Reward: -44.0\n",
      "Episode 2/10, Reward: -29.5\n",
      "Episode 3/10, Reward: -14.5\n",
      "Episode 4/10, Reward: -17.5\n",
      "Episode 5/10, Reward: -49.0\n",
      "Episode 6/10, Reward: -14.0\n",
      "Episode 7/10, Reward: -24.5\n",
      "Episode 8/10, Reward: -13.0\n",
      "Episode 9/10, Reward: -23.5\n",
      "Episode 10/10, Reward: -27.0\n",
      "Average Reward over 10 episodes: -25.65\n",
      "Episode 301/500, Average Reward: -25.65\n",
      "Episode 302/500, Reward: -108.5, Epsilon: 0.22\n",
      "Episode 303/500, Reward: -43.5, Epsilon: 0.22\n",
      "Episode 304/500, Reward: -25.5, Epsilon: 0.22\n",
      "Episode 305/500, Reward: -99.5, Epsilon: 0.22\n",
      "Episode 306/500, Reward: -83.5, Epsilon: 0.22\n",
      "Episode 307/500, Reward: -107.5, Epsilon: 0.21\n",
      "Episode 308/500, Reward: -113.5, Epsilon: 0.21\n",
      "Episode 309/500, Reward: -12.0, Epsilon: 0.21\n",
      "Episode 310/500, Reward: -105.0, Epsilon: 0.21\n",
      "Episode 311/500, Reward: -106.0, Epsilon: 0.21\n",
      "Episode 312/500, Reward: -22.5, Epsilon: 0.21\n",
      "Episode 313/500, Reward: -109.0, Epsilon: 0.21\n",
      "Episode 314/500, Reward: -16.5, Epsilon: 0.21\n",
      "Episode 315/500, Reward: -64.5, Epsilon: 0.21\n",
      "Episode 316/500, Reward: -66.0, Epsilon: 0.21\n",
      "Episode 317/500, Reward: -36.5, Epsilon: 0.20\n",
      "Episode 318/500, Reward: -135.5, Epsilon: 0.20\n",
      "Episode 319/500, Reward: -106.5, Epsilon: 0.20\n",
      "Episode 320/500, Reward: 79.0, Epsilon: 0.20\n",
      "Episode 321/500, Reward: -41.0, Epsilon: 0.20\n",
      "Episode 1/10, Reward: -17.0\n",
      "Episode 2/10, Reward: -30.0\n",
      "Episode 3/10, Reward: -24.5\n",
      "Episode 4/10, Reward: -25.0\n",
      "Episode 5/10, Reward: -21.0\n",
      "Episode 6/10, Reward: -37.5\n",
      "Episode 7/10, Reward: -27.5\n",
      "Episode 8/10, Reward: -14.0\n",
      "Episode 9/10, Reward: -16.5\n",
      "Episode 10/10, Reward: -33.0\n",
      "Average Reward over 10 episodes: -24.6\n",
      "Episode 321/500, Average Reward: -24.6\n",
      "Episode 322/500, Reward: 63.0, Epsilon: 0.20\n",
      "Episode 323/500, Reward: -82.5, Epsilon: 0.20\n",
      "Episode 324/500, Reward: -45.5, Epsilon: 0.20\n",
      "Episode 325/500, Reward: -67.5, Epsilon: 0.20\n",
      "Episode 326/500, Reward: -104.0, Epsilon: 0.20\n",
      "Episode 327/500, Reward: 42.5, Epsilon: 0.19\n",
      "Episode 328/500, Reward: -105.0, Epsilon: 0.19\n",
      "Episode 329/500, Reward: -99.5, Epsilon: 0.19\n",
      "Episode 330/500, Reward: 64.5, Epsilon: 0.19\n",
      "Episode 331/500, Reward: -61.5, Epsilon: 0.19\n",
      "Episode 332/500, Reward: -96.0, Epsilon: 0.19\n",
      "Episode 333/500, Reward: -54.5, Epsilon: 0.19\n",
      "Episode 334/500, Reward: -111.0, Epsilon: 0.19\n",
      "Episode 335/500, Reward: -62.0, Epsilon: 0.19\n",
      "Episode 336/500, Reward: -84.0, Epsilon: 0.19\n",
      "Episode 337/500, Reward: -31.0, Epsilon: 0.18\n",
      "Episode 338/500, Reward: -75.5, Epsilon: 0.18\n",
      "Episode 339/500, Reward: -53.5, Epsilon: 0.18\n",
      "Episode 340/500, Reward: -104.0, Epsilon: 0.18\n",
      "Episode 341/500, Reward: -29.5, Epsilon: 0.18\n",
      "Episode 1/10, Reward: -31.0\n",
      "Episode 2/10, Reward: -16.5\n",
      "Episode 3/10, Reward: -16.5\n",
      "Episode 4/10, Reward: -34.0\n",
      "Episode 5/10, Reward: -33.0\n",
      "Episode 6/10, Reward: -22.0\n",
      "Episode 7/10, Reward: -15.5\n",
      "Episode 8/10, Reward: 3.5\n",
      "Episode 9/10, Reward: 21.5\n",
      "Episode 10/10, Reward: 9.5\n",
      "Average Reward over 10 episodes: -13.4\n",
      "Episode 341/500, Average Reward: -13.4\n",
      "Episode 342/500, Reward: -103.5, Epsilon: 0.18\n",
      "Episode 343/500, Reward: -75.0, Epsilon: 0.18\n",
      "Episode 344/500, Reward: 17.5, Epsilon: 0.18\n",
      "Episode 345/500, Reward: -111.0, Epsilon: 0.18\n",
      "Episode 346/500, Reward: -134.0, Epsilon: 0.18\n",
      "Episode 347/500, Reward: -2.5, Epsilon: 0.18\n",
      "Episode 348/500, Reward: -103.5, Epsilon: 0.17\n",
      "Episode 349/500, Reward: -125.0, Epsilon: 0.17\n",
      "Episode 350/500, Reward: -18.0, Epsilon: 0.17\n",
      "Episode 351/500, Reward: -11.5, Epsilon: 0.17\n",
      "Episode 352/500, Reward: -108.0, Epsilon: 0.17\n",
      "Episode 353/500, Reward: -103.5, Epsilon: 0.17\n",
      "Episode 354/500, Reward: -82.0, Epsilon: 0.17\n",
      "Episode 355/500, Reward: 60.5, Epsilon: 0.17\n",
      "Episode 356/500, Reward: -122.5, Epsilon: 0.17\n",
      "Episode 357/500, Reward: -111.0, Epsilon: 0.17\n",
      "Episode 358/500, Reward: -20.0, Epsilon: 0.17\n",
      "Episode 359/500, Reward: -95.0, Epsilon: 0.17\n",
      "Episode 360/500, Reward: -108.5, Epsilon: 0.16\n",
      "Episode 361/500, Reward: -84.0, Epsilon: 0.16\n",
      "Episode 1/10, Reward: -23.0\n",
      "Episode 2/10, Reward: 17.0\n",
      "Episode 3/10, Reward: -6.5\n",
      "Episode 4/10, Reward: -29.0\n",
      "Episode 5/10, Reward: -16.0\n",
      "Episode 6/10, Reward: 7.5\n",
      "Episode 7/10, Reward: -16.5\n",
      "Episode 8/10, Reward: -23.0\n",
      "Episode 9/10, Reward: -28.0\n",
      "Episode 10/10, Reward: 0.5\n",
      "Average Reward over 10 episodes: -11.7\n",
      "Episode 361/500, Average Reward: -11.7\n",
      "Episode 362/500, Reward: -44.5, Epsilon: 0.16\n",
      "Episode 363/500, Reward: 59.5, Epsilon: 0.16\n",
      "Episode 364/500, Reward: -92.0, Epsilon: 0.16\n",
      "Episode 365/500, Reward: 79.5, Epsilon: 0.16\n",
      "Episode 366/500, Reward: -75.0, Epsilon: 0.16\n",
      "Episode 367/500, Reward: -97.5, Epsilon: 0.16\n",
      "Episode 368/500, Reward: -101.5, Epsilon: 0.16\n",
      "Episode 369/500, Reward: -110.5, Epsilon: 0.16\n",
      "Episode 370/500, Reward: -115.5, Epsilon: 0.16\n",
      "Episode 371/500, Reward: 6.5, Epsilon: 0.16\n",
      "Episode 372/500, Reward: -110.0, Epsilon: 0.15\n",
      "Episode 373/500, Reward: -106.0, Epsilon: 0.15\n",
      "Episode 374/500, Reward: -18.0, Epsilon: 0.15\n",
      "Episode 375/500, Reward: -90.0, Epsilon: 0.15\n",
      "Episode 376/500, Reward: -83.5, Epsilon: 0.15\n",
      "Episode 377/500, Reward: -67.5, Epsilon: 0.15\n",
      "Episode 378/500, Reward: -130.0, Epsilon: 0.15\n",
      "Episode 379/500, Reward: -80.5, Epsilon: 0.15\n",
      "Episode 380/500, Reward: -105.5, Epsilon: 0.15\n",
      "Episode 381/500, Reward: -39.0, Epsilon: 0.15\n",
      "Episode 1/10, Reward: 18.5\n",
      "Episode 2/10, Reward: -29.0\n",
      "Episode 3/10, Reward: -7.5\n",
      "Episode 4/10, Reward: -2.0\n",
      "Episode 5/10, Reward: -28.5\n",
      "Episode 6/10, Reward: -18.5\n",
      "Episode 7/10, Reward: -29.5\n",
      "Episode 8/10, Reward: -40.0\n",
      "Episode 9/10, Reward: -23.5\n",
      "Episode 10/10, Reward: -26.0\n",
      "Average Reward over 10 episodes: -18.6\n",
      "Episode 381/500, Average Reward: -18.6\n",
      "Episode 382/500, Reward: 25.5, Epsilon: 0.15\n",
      "Episode 383/500, Reward: -107.0, Epsilon: 0.15\n",
      "Episode 384/500, Reward: -98.0, Epsilon: 0.15\n",
      "Episode 385/500, Reward: -43.5, Epsilon: 0.15\n",
      "Episode 386/500, Reward: -10.0, Epsilon: 0.14\n",
      "Episode 387/500, Reward: -74.0, Epsilon: 0.14\n",
      "Episode 388/500, Reward: -108.0, Epsilon: 0.14\n",
      "Episode 389/500, Reward: -109.0, Epsilon: 0.14\n",
      "Episode 390/500, Reward: -97.5, Epsilon: 0.14\n",
      "Episode 391/500, Reward: 2.5, Epsilon: 0.14\n",
      "Episode 392/500, Reward: -6.0, Epsilon: 0.14\n",
      "Episode 393/500, Reward: 6.5, Epsilon: 0.14\n",
      "Episode 394/500, Reward: -83.0, Epsilon: 0.14\n",
      "Episode 395/500, Reward: -116.0, Epsilon: 0.14\n",
      "Episode 396/500, Reward: 55.0, Epsilon: 0.14\n",
      "Episode 397/500, Reward: -35.0, Epsilon: 0.14\n",
      "Episode 398/500, Reward: 40.0, Epsilon: 0.14\n",
      "Episode 399/500, Reward: 10.0, Epsilon: 0.14\n",
      "Episode 400/500, Reward: 23.0, Epsilon: 0.13\n",
      "Episode 401/500, Reward: -97.5, Epsilon: 0.13\n",
      "Episode 1/10, Reward: -25.5\n",
      "Episode 2/10, Reward: -48.0\n",
      "Episode 3/10, Reward: -26.5\n",
      "Episode 4/10, Reward: -5.0\n",
      "Episode 5/10, Reward: -46.0\n",
      "Episode 6/10, Reward: -5.0\n",
      "Episode 7/10, Reward: -32.5\n",
      "Episode 8/10, Reward: -14.5\n",
      "Episode 9/10, Reward: -29.0\n",
      "Episode 10/10, Reward: -41.0\n",
      "Average Reward over 10 episodes: -27.3\n",
      "Episode 401/500, Average Reward: -27.3\n",
      "Episode 402/500, Reward: 5.0, Epsilon: 0.13\n",
      "Episode 403/500, Reward: -78.5, Epsilon: 0.13\n",
      "Episode 404/500, Reward: -84.5, Epsilon: 0.13\n",
      "Episode 405/500, Reward: 9.5, Epsilon: 0.13\n",
      "Episode 406/500, Reward: -78.5, Epsilon: 0.13\n",
      "Episode 407/500, Reward: -138.5, Epsilon: 0.13\n",
      "Episode 408/500, Reward: -103.0, Epsilon: 0.13\n",
      "Episode 409/500, Reward: -107.5, Epsilon: 0.13\n",
      "Episode 410/500, Reward: -12.5, Epsilon: 0.13\n",
      "Episode 411/500, Reward: -125.5, Epsilon: 0.13\n",
      "Episode 412/500, Reward: -113.0, Epsilon: 0.13\n",
      "Episode 413/500, Reward: -91.0, Epsilon: 0.13\n",
      "Episode 414/500, Reward: -75.0, Epsilon: 0.13\n",
      "Episode 415/500, Reward: -65.5, Epsilon: 0.12\n",
      "Episode 416/500, Reward: -5.0, Epsilon: 0.12\n",
      "Episode 417/500, Reward: 66.0, Epsilon: 0.12\n",
      "Episode 418/500, Reward: -51.0, Epsilon: 0.12\n",
      "Episode 419/500, Reward: -110.0, Epsilon: 0.12\n",
      "Episode 420/500, Reward: -105.5, Epsilon: 0.12\n",
      "Episode 421/500, Reward: -115.0, Epsilon: 0.12\n",
      "Episode 1/10, Reward: -31.0\n",
      "Episode 2/10, Reward: -26.0\n",
      "Episode 3/10, Reward: -30.5\n",
      "Episode 4/10, Reward: -1.0\n",
      "Episode 5/10, Reward: -49.0\n",
      "Episode 6/10, Reward: -19.5\n",
      "Episode 7/10, Reward: -13.0\n",
      "Episode 8/10, Reward: -25.0\n",
      "Episode 9/10, Reward: -42.0\n",
      "Episode 10/10, Reward: -25.5\n",
      "Average Reward over 10 episodes: -26.25\n",
      "Episode 421/500, Average Reward: -26.25\n",
      "Episode 422/500, Reward: -111.0, Epsilon: 0.12\n",
      "Episode 423/500, Reward: -58.5, Epsilon: 0.12\n",
      "Episode 424/500, Reward: 41.0, Epsilon: 0.12\n",
      "Episode 425/500, Reward: 57.5, Epsilon: 0.12\n",
      "Episode 426/500, Reward: -146.0, Epsilon: 0.12\n",
      "Episode 427/500, Reward: -112.0, Epsilon: 0.12\n",
      "Episode 428/500, Reward: -101.0, Epsilon: 0.12\n",
      "Episode 429/500, Reward: -38.0, Epsilon: 0.12\n",
      "Episode 430/500, Reward: -53.5, Epsilon: 0.12\n",
      "Episode 431/500, Reward: -67.0, Epsilon: 0.12\n",
      "Episode 432/500, Reward: -86.5, Epsilon: 0.11\n",
      "Episode 433/500, Reward: -45.0, Epsilon: 0.11\n",
      "Episode 434/500, Reward: -106.5, Epsilon: 0.11\n",
      "Episode 435/500, Reward: -117.0, Epsilon: 0.11\n",
      "Episode 436/500, Reward: 25.0, Epsilon: 0.11\n",
      "Episode 437/500, Reward: -25.0, Epsilon: 0.11\n",
      "Episode 438/500, Reward: -55.5, Epsilon: 0.11\n",
      "Episode 439/500, Reward: 33.5, Epsilon: 0.11\n",
      "Episode 440/500, Reward: -101.5, Epsilon: 0.11\n",
      "Episode 441/500, Reward: 61.5, Epsilon: 0.11\n",
      "Episode 1/10, Reward: -48.0\n",
      "Episode 2/10, Reward: -13.0\n",
      "Episode 3/10, Reward: -25.0\n",
      "Episode 4/10, Reward: -13.0\n",
      "Episode 5/10, Reward: -12.5\n",
      "Episode 6/10, Reward: -26.5\n",
      "Episode 7/10, Reward: -12.5\n",
      "Episode 8/10, Reward: -30.5\n",
      "Episode 9/10, Reward: -32.0\n",
      "Episode 10/10, Reward: -28.5\n",
      "Average Reward over 10 episodes: -24.15\n",
      "Episode 441/500, Average Reward: -24.15\n",
      "Episode 442/500, Reward: 34.0, Epsilon: 0.11\n",
      "Episode 443/500, Reward: -12.0, Epsilon: 0.11\n",
      "Episode 444/500, Reward: 3.0, Epsilon: 0.11\n",
      "Episode 445/500, Reward: -118.0, Epsilon: 0.11\n",
      "Episode 446/500, Reward: -30.5, Epsilon: 0.11\n",
      "Episode 447/500, Reward: 60.5, Epsilon: 0.11\n",
      "Episode 448/500, Reward: 9.0, Epsilon: 0.11\n",
      "Episode 449/500, Reward: 63.5, Epsilon: 0.11\n",
      "Episode 450/500, Reward: -5.5, Epsilon: 0.10\n",
      "Episode 451/500, Reward: -107.0, Epsilon: 0.10\n",
      "Episode 452/500, Reward: -113.5, Epsilon: 0.10\n",
      "Episode 453/500, Reward: -95.0, Epsilon: 0.10\n",
      "Episode 454/500, Reward: -85.0, Epsilon: 0.10\n",
      "Episode 455/500, Reward: -109.5, Epsilon: 0.10\n",
      "Episode 456/500, Reward: -11.5, Epsilon: 0.10\n",
      "Episode 457/500, Reward: -86.5, Epsilon: 0.10\n",
      "Episode 458/500, Reward: -48.0, Epsilon: 0.10\n",
      "Episode 459/500, Reward: 2.0, Epsilon: 0.10\n",
      "Episode 460/500, Reward: 12.5, Epsilon: 0.10\n",
      "Episode 461/500, Reward: 69.5, Epsilon: 0.10\n",
      "Episode 1/10, Reward: -30.0\n",
      "Episode 2/10, Reward: -25.0\n",
      "Episode 3/10, Reward: -7.0\n",
      "Episode 4/10, Reward: -13.0\n",
      "Episode 5/10, Reward: -20.5\n",
      "Episode 6/10, Reward: -49.0\n",
      "Episode 7/10, Reward: -12.5\n",
      "Episode 8/10, Reward: -47.0\n",
      "Episode 9/10, Reward: -16.5\n",
      "Episode 10/10, Reward: -23.5\n",
      "Average Reward over 10 episodes: -24.4\n",
      "Episode 461/500, Average Reward: -24.4\n",
      "Episode 462/500, Reward: -96.5, Epsilon: 0.10\n",
      "Episode 463/500, Reward: -2.5, Epsilon: 0.10\n",
      "Episode 464/500, Reward: -27.0, Epsilon: 0.10\n",
      "Episode 465/500, Reward: -93.0, Epsilon: 0.10\n",
      "Episode 466/500, Reward: -26.5, Epsilon: 0.10\n",
      "Episode 467/500, Reward: -31.5, Epsilon: 0.10\n",
      "Episode 468/500, Reward: -103.0, Epsilon: 0.10\n",
      "Episode 469/500, Reward: -25.0, Epsilon: 0.10\n",
      "Episode 470/500, Reward: -106.0, Epsilon: 0.09\n",
      "Episode 471/500, Reward: -89.5, Epsilon: 0.09\n",
      "Episode 472/500, Reward: 17.5, Epsilon: 0.09\n",
      "Episode 473/500, Reward: -170.0, Epsilon: 0.09\n",
      "Episode 474/500, Reward: -112.0, Epsilon: 0.09\n",
      "Episode 475/500, Reward: -98.5, Epsilon: 0.09\n",
      "Episode 476/500, Reward: -56.0, Epsilon: 0.09\n",
      "Episode 477/500, Reward: -106.0, Epsilon: 0.09\n",
      "Episode 478/500, Reward: 33.5, Epsilon: 0.09\n",
      "Episode 479/500, Reward: -97.0, Epsilon: 0.09\n",
      "Episode 480/500, Reward: -105.0, Epsilon: 0.09\n",
      "Episode 481/500, Reward: -12.0, Epsilon: 0.09\n",
      "Episode 1/10, Reward: -20.0\n",
      "Episode 2/10, Reward: -12.5\n",
      "Episode 3/10, Reward: -27.0\n",
      "Episode 4/10, Reward: -24.0\n",
      "Episode 5/10, Reward: -34.0\n",
      "Episode 6/10, Reward: -13.0\n",
      "Episode 7/10, Reward: -28.0\n",
      "Episode 8/10, Reward: -25.0\n",
      "Episode 9/10, Reward: -24.0\n",
      "Episode 10/10, Reward: -3.0\n",
      "Average Reward over 10 episodes: -21.05\n",
      "Episode 481/500, Average Reward: -21.05\n",
      "Episode 482/500, Reward: -60.5, Epsilon: 0.09\n",
      "Episode 483/500, Reward: -102.5, Epsilon: 0.09\n",
      "Episode 484/500, Reward: -108.0, Epsilon: 0.09\n",
      "Episode 485/500, Reward: -160.0, Epsilon: 0.09\n",
      "Episode 486/500, Reward: 84.0, Epsilon: 0.09\n",
      "Episode 487/500, Reward: -73.0, Epsilon: 0.09\n",
      "Episode 488/500, Reward: -105.0, Epsilon: 0.09\n",
      "Episode 489/500, Reward: 53.0, Epsilon: 0.09\n",
      "Episode 490/500, Reward: -114.5, Epsilon: 0.09\n",
      "Episode 491/500, Reward: 77.5, Epsilon: 0.09\n",
      "Episode 492/500, Reward: -97.0, Epsilon: 0.08\n",
      "Episode 493/500, Reward: 29.0, Epsilon: 0.08\n",
      "Episode 494/500, Reward: 49.5, Epsilon: 0.08\n",
      "Episode 495/500, Reward: -77.5, Epsilon: 0.08\n",
      "Episode 496/500, Reward: 13.0, Epsilon: 0.08\n",
      "Episode 497/500, Reward: -106.5, Epsilon: 0.08\n",
      "Episode 498/500, Reward: -99.0, Epsilon: 0.08\n",
      "Episode 499/500, Reward: -115.0, Epsilon: 0.08\n",
      "Episode 500/500, Reward: -36.0, Epsilon: 0.08\n",
      "Training complete.\n",
      "evalrewards:  [np.float64(-29.15), np.float64(-23.2), np.float64(-22.15), np.float64(-20.85), np.float64(-24.45), np.float64(-26.5), np.float64(-15.25), np.float64(-24.5), np.float64(-21.35), np.float64(-24.15), np.float64(-15.05), np.float64(-17.25), np.float64(-13.85), np.float64(-25.75), np.float64(-25.65), np.float64(-24.6), np.float64(-13.4), np.float64(-11.7), np.float64(-18.6), np.float64(-27.3), np.float64(-26.25), np.float64(-24.15), np.float64(-24.4), np.float64(-21.05)]\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 500\n",
    "\n",
    "\n",
    "rewards = []\n",
    "evalrewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env_wrapper.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    steps = 0\n",
    "    while not done:\n",
    "        action = agent.choose_action([state])\n",
    "        obs, reward, done, info = env_wrapper.step(state, action)\n",
    "        next_state = info['actual_state']\n",
    "        agent.store_transition([state], action, reward, [next_state], done)\n",
    "\n",
    "        agent.train()\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        steps += 1        \n",
    "        if done or steps > 100:\n",
    "            agent.update_epsilon()\n",
    "            rewards.append(episode_reward)\n",
    "            print(f\"Episode {episode + 1}/{num_episodes}, Reward: {episode_reward}, Epsilon: {agent.epsilon:.2f}\")\n",
    "            break\n",
    "    if episode != 0 and episode % 20 == 0:\n",
    "        avg_reward = evaluate_agent(env_wrapper, agent)\n",
    "        evalrewards.append(avg_reward)\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}, Average Reward: {avg_reward}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "print(\"evalrewards: \", evalrewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env_wrapper.reset()\n",
    "episode_reward = 0\n",
    "\n",
    "action = agent.choose_action([state])\n",
    "obs, reward, done, info = env_wrapper.step(state, action)\n",
    "next_state = info['actual_state']\n",
    "agent.store_transition([state], action, reward, [next_state], done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 3, -0.5, 102, False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, action, reward, next_state, done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
